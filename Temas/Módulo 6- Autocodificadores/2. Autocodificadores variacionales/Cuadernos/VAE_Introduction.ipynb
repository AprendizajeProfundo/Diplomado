{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto codificadores Variacionales (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Inferencia Variacional](#Inferencia-Variacional)\n",
    " * [Planteamiento del problema variacional](#Planteamiento-del-problema-variacional)\n",
    " * [Aproximación Variacional](#Aproximación-Variacional)\n",
    " * [Divergencia Kullback-Leibler](#Divergencia-Kullback-Leibler)\n",
    " * [Optimización](#Optimización)\n",
    "* [Implementación de un Autoencoder Variacional](#Implementación-de-un-Autoencoder-Variacional)\n",
    "* [Ejemplo. Autoencoder Variacional Dense MNIST](VAE_Example_Dense_mnist.ipynb#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autocodificadores variacionales constituyen una herramienta para problemas en los que se supone que existe una variable latente que genera los datos de entrada.\n",
    "\n",
    "Para fijar ideas, supongamos que se tiene el resultado de una prueba académica aplicada a una muestra de personas. Más aún, supongamos que la codificación es binaria, en donde 1 (uno) codifica una respuesta correcta y 0 (cero) una respuesta incorrecta. \n",
    "\n",
    "Observe que la representación vectorial de la base de datos *mnist*, puede considerarse similar si cada pixel se codifica como cero o uno únicamente.\n",
    "\n",
    "Entonces a la entrada se tienen vectores binarios  $\\mathbf{x}$. de tamaño fijo, digamos $d=100$. Se busca entonces una respresentación de estos vectores en un espacio Euclideano de dimensión $m$, en donde $m<<d$. Digamos $m=5$. \n",
    "\n",
    "Los expertos utilizan distintas técnicas como por ejemplo el empleo del análisis de componentes principales (ACP) o la $q$-dimensión, para detectar la dimensión $m$.\n",
    "\n",
    "La siguiente gráfica ilustra la técnica para el ejmeplo de datos de una prueba académica. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/autoencoder_intro.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Autoencoder Variacional</p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el espacio de entrada se tiene vectores de respuestas, Cada uno de ellos denotado por $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "Matemáticamente, estos objetos estan en el espacio $ A= \\{0,1\\}^d$. Es decir, sus elementos son vectores de tamaño $d$ y las componentes de cada vector son unos y ceros. La respuestas en la prueba de una  determinada persona se codifican en un vector $\\mathbf{x}\\in A$. Sobre $A$ se supone definida una medida de probabilidad que genéricamente denotaremos $P_{\\theta}(\\mathbf{x})$. Entonces se asume que  $\\mathbf{x}$ es una muestra generada por dicha medida de probabilidad y se escribe $\\mathbf{x}\\sim P_{\\theta}$. El símbolo $\\theta$ refrefeiereiere a los parámetros (desconocidos) de la medida de probabilidad.\n",
    "\n",
    "Ahora bien, se supone que la capacidad de una persona para resolver la prueba puede ser representada por un vector aleatorio $\\mathbf{z} \\in \\mathbb{R}^m$. En principio, la teoría psicométrica está orientada a predecir un valor para $\\mathbf{z}$, que en una prueba de Estado correponde a una calificación.\n",
    "\n",
    "No haremos eso aquí, pero si generaremos muestras que correspondería a distintos valores que podría representar la capacidad de la persona. Estos valores se llaman muestras de las variables latentes. \n",
    "\n",
    "\n",
    "Si ahora se considera a un individuo en particular que  tiene capacidad $\\mathbf{z}$ para resolver la prueba y obtiene el vector de respuestas $\\mathbf{x}$, claramente existe una relación entre esos dos vectores que desde el punto de vista probabilístico se escribe de la siguiente forma:\n",
    "\n",
    "1. El vector de respuestas $\\mathbf{x}$ es generado a partir de la  distribución condicional $P(\\mathbf{x}|\\mathbf{z})$. En términos estadísticos $P(\\mathbf{x}|\\mathbf{z})$ es conocida excepto por sus parámetros. Estimar los parámetros de esta distribución será el problema central de una **autoencoder variacional** y es la parte que corresponde al decodificador. Aquí por lo general se introduce un modelo estadístico, por ejemplo un modelo de Bernoulli, y se estiman sus parámetros, haciendo uso de la entropía cruzada como función de pérdida.\n",
    "\n",
    "2. La variable latente $\\mathbf{z}$ es generada por la distribución condicional $Q(\\mathbf{z}|\\mathbf{x})$. Esta es siempre desconocida y justamente el problema de la inferencia variacional es encontrar una distribución simple, Gaussiana por ejemplo, que aproxime lo mejor posible a $Q(\\mathbf{z}|\\mathbf{x})$. Desde el punto de vista del autoencoder variacional, esta parte corresponde al codificador.\n",
    "\n",
    "\n",
    "En la siguiente sección se muestra el desarrollo teórico del problema de inferencia variacional. Solamente para lectores con algún conocimiento matemático. Puede omitir sin problema. \n",
    "\n",
    "* [Ir a la implementación](#Implementación-de-un-Autoencoder-Variacional) \n",
    "* [Regresar al inicio](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de la inferencia variacional es aproximar una densidad haciendo una paso intermedio por un espacio de variables latentes.\n",
    "\n",
    "El proceso  puede ser imaginado de una forma análoga a la construcción de un codificador en variables latentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planteamiento del problema variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que $p_{\\theta}(\\mathbf{x})$ es la densidad asociada a un vector aleatorio de respuesta. El problema estadístico en principio es estimar el parámetro $\\theta$ que indexa a la distribución.\n",
    "\n",
    "Si se asume que $\\mathbf{z}$ es el vector latente asociado a $\\mathbf{x}$  se tiene que\n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{x})= \\int P_{\\theta}(\\mathbf{z},\\mathbf{x})d\\mathbf{z} = \\int P_{\\theta}(\\mathbf{z}|\\mathbf{x})P(\\mathbf{x})d\\mathbf{z}.\n",
    "$$\n",
    "\n",
    "Un esquema de muestreo para $\\mathbf{z}$ se deriva de esta ecuación:\n",
    "\n",
    "1. Dada una muestra $\\mathbf{x} \\sim P_{\\theta}(x)$, es decir una muestra en la entrada de la red.\n",
    "2. Se obtiene una muestra $\\mathbf{z}\\sim P_{\\theta}(\\mathbf{z}|\\mathbf{x})$.\n",
    "\n",
    "\n",
    "El problema es que en general $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$ es intratable. Lo que vamos a hacer para obtener la muestras $\\mathbf{z}$ es obtener una densidad aproximada $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ en una familia conocida.\n",
    "\n",
    "En esta lección lo que haremos es proponer una aproximación del tipo $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})= \\mathcal{N}_m(\\boldsymbol{\\mu}(\\mathbf{x}),\\text{diag}(\\boldsymbol{\\sigma}(\\mathbf{x})^2)) $.\n",
    "\n",
    "El problema en la inferencia variacional es jusramente encontrar una buen aproximante $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$, talque $Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\approx P_{\\theta}(\\mathbf{z}|\\mathbf{x})$.\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el propósito de convertir $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ en una función de densidad tratable la solución propuesta desde la inferencia variacional es la introducción de una densidad aproximada $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ de tal manera que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\approx P_{\\theta}(\\mathbf{z}|\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "La densidad $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ se escoge en una familia de distribuciones tratables indexadas por $\\phi$. Es común escoger $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ en la familia normal multivariada. Eso haremos en esta lección. \n",
    "\n",
    "Entonces para cada $\\mathbf{x}$ tendremos que\n",
    "\n",
    "$$\n",
    "Q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}(\\mathbf{x}), \\text{diag}(\\boldsymbol{\\sigma}(\\mathbf{x})^2)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{\\mu}(\\mathbf{x})$ es el vector de medias (condicionadas por $\\mathbf{x}$) y $\\boldsymbol{\\sigma}(\\mathbf{x})$ es un vector de desviaciones estándar. Como la matriz de covarianza es diagonal, se está asumiendo que las componentes del vector $\\mathbf{z}$ son condicionalmente independientes, dado el vector  $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergencia Kullback-Leibler (KL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha definido la familia de distribuciones a partir de la cual se obtendrá la aproximación $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ el siguiente paso es decidir como medir la proximidad o la discrepancia de la densidad aproximante con la densidad original. La solución sugerida desde la inferencia variacional es usar la divergencia KL, la cual se define por\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)) = \\int(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\right)Q_{\\phi}(\\mathbf{z}|\\mathbf{x})dz.\n",
    "$$\n",
    "\n",
    "El símbolo $\\mathbb{E}_{\\phi}$ indica que la esperanza es con respecto a la densidad $Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cota inferior de la evidencia (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo en la inferencia variacional es encontrar una densidad aproximante $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$  para la densidad $p_{\\theta}(\\mathbf{z}|\\mathbf{x})$ utilizando como métrica la divergencia KL, que por cierto no es una distancia, dado que no es simétrica.\n",
    "\n",
    "A partir del teorema de Bayes se obtiene que \n",
    "\n",
    "$$\n",
    "P_{\\theta}(\\mathbf{z}|\\mathbf{x})= \\frac{P_{\\theta}(\\mathbf{x}|\\mathbf{z})P_{\\theta}(\\mathbf{z})}{P_{\\theta}(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Por lo que la divergencia KL se transforma en\n",
    "\n",
    "$$\n",
    "D_{KL}(Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x}))  = \\mathbb{E}_{\\phi}(\\log Q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}) - \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})- \\log P_{\\theta}(\\mathbf{z})\\right)) + \\log P_{\\theta}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "De donde se obtiene que\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] = \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "$$\n",
    "\n",
    "Esta ecuación constituye el núcleo de la inferencia variacional. El lado izquierdo de la ecuación  contiene el término $P_{\\theta}(\\mathbf{x})$ que se busca maximizar menos el error de la aproximación medido por $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})]$ que se espera  sea aproximadamente cero. \n",
    "\n",
    "\n",
    "Se sabe que la divergencia KL siempre es positiva, por lo que la parte izquierda de la ecuación se denomina como la cota inferior de la evidencia (**ELBO**) del inglés *evidence lower bound*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación clave de la inferencia variacional es dada por\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{ELBO}  & = \n",
    "\\log P_{\\theta}(\\mathbf{x}) - \n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z}|\\mathbf{x})] \\\\\n",
    "&= \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "El proceso de optimización se basa en la segunda parte ecuación, por lo que usaremos al definición\n",
    "\n",
    "$$\n",
    "\\text{ELBO}  = \\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]-\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]\n",
    "$$\n",
    "\n",
    "\n",
    "El término $\\mathbb{E}_{\\phi}[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$ corresponde al modelo generativo en el problema. La interpretación estadística de este término es que el modelo generador toma muestras obtenidas de la salida del modelo latente $P_{\\theta}(\\mathbf{z}|\\mathbf{x})$, el cual estamos aproximando con $Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$. Es decir, se genera una muestra $\\mathbf{z} \\sim Q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ y partir de esta se trata de reconstruir la entrada $\\mathbf{x}$.\n",
    "\n",
    "En el ejemplo propuesto, si se considera que se tiene vectores dicotómicos, se asume una distribución de Bernoulli para cada componente. Si las respuestas son condicionalmente independientes dado el vector $\\mathbf{z}$ entonces la función de pérdida es la entropía cruzada binaria $\\mathcal{L}_R$ dada por\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_R = - \\frac{1}{d}\\sum_{j=1}^d x_j \\log p(\\mathbf{w}_j'\\mathbf{z} + b_j) + (1-x_j)\\log(1-p(\\mathbf{w}_j'\\mathbf{z} + b_j))\n",
    "$$\n",
    "\n",
    "\n",
    "El segundo término $D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]$ puede ser evaluado directamente. Como asumimos que $Q_{\\phi}$ es una distribución Gaussiana y si se tiene en cuenta que típicamente $P_{\\theta}(\\mathbf{z})= P(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, se obtiene que \n",
    "\n",
    "$$\n",
    "D_{KL}[Q_{\\phi}(\\mathbf{z}|\\mathbf{x})|| p_{\\theta}(\\mathbf{z})]= \\frac{1}{2} \\sum_{j=1}^{d} (1+\\log(\\sigma_j)^2 - (\\mu_j)^2-(\\sigma_j)^2)\n",
    "$$\n",
    "\n",
    "Tanto $\\mu_j$ como $\\sigma_j$ son funciones de la entrada $\\mathbf{x}$ ue se estiman en el modelo de inferencia.\n",
    "\n",
    "Para minimizar $\\mathcal{L}_{KL} =D_{KL}$, se requiere que $\\mu_j\\to 0$ y $\\sigma_j\\to 1$.\n",
    "\n",
    "En resumen para el problema de inferencia variacional la función de pérdida es dada por\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VAE} = \\mathcal{L}_{R} + \\mathcal{L}_{KL}\n",
    "$$\n",
    "\n",
    "[Regresar al inicio](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de un Autoencoder Variacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Nuestro autocodificador (autoencoder) variacional (VAE) tendrá variables latentes gaussianas y una distribución posterior gaussiana   $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$  con una matriz de covarianza diagonal.\n",
    "\n",
    "Recordemos que  un VAE de cuatro elementos escenciales:\n",
    "\n",
    "1. Una  variable  latente $\\boldsymbol{z}$ con distribución  $p(\\boldsymbol{z})$  que en nuestro casoserá una variable  aleatoria Gaussiana con media cero y varianza 1 y que denotamos   $\\epsilon$.\n",
    "2. Un decodificador(decoder)  $p(\\boldsymbol{x}|\\boldsymbol{z})$  que mapea las  variables latentes  $\\boldsymbol{z}$  a variables observables $\\boldsymbol{x}$. En este ejemplo este codificador implementa un perceptron multicapa (MLP), es decir una red neuronal con una capa oculta.\n",
    "3. Un codificador (encoder)  $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$  que mapea ejemplos de entrada al espacio latente. Como se stá cosntruyendo un autoencoder variacional se tiene que este mapeo se hace generando muestras aleatorias de distribciones Gaussianas con medias y varianzas que dependen de  la entrada:   $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})=N(\\boldsymbol{z},\\boldsymbol{\\mu}(x),\\text{diag}(\\boldsymbol{\\sigma}^2(\\boldsymbol{x})))$. \n",
    "4. Una función de costo que tiene dos términos: el  error de construcción que corresponde al modelo generativo implementado en el decoder y un término adicional de regularización que minimiza la divergencia KL. El error de reconstrucción es medido por el error cuadrático medio y la divergencia por el término\n",
    "$-D_{KL}(q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})|p(\\boldsymbol{z}))=\\tfrac{1}{2}\\sum_{j=1}^{J}(1+\\log \\boldsymbol{\\sigma}^2_j(\\boldsymbol{x})-\\boldsymbol{\\mu}^2_j(\\boldsymbol{x})-\\boldsymbol{\\sigma}^2_j(\\boldsymbol{x}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El truco de la reparametrización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Los gradientes de propagación hacia atrás no pueden pasar por el bloque de muestreo estocástico. Si bien está bien tener entradas estocásticas para redes neuronales, no es posible  pasar los gradientes por una capa estocástica. La solución a este problema es eliminar el proceso de muestreo como entrada, como se muestra en el lado derecho de la siguiente figura. \n",
    "\n",
    "Desde el punto de vista estadístico, esto es bastante simple. Se usa la representación estocástica clasica de una distribución normal multivariada asi:\n",
    "\n",
    "1. Genera una muestra $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}_m(\\mathbf{0}, \\mathbf{I})$.\n",
    "2. Obtiene $\\mathbf{z} =\\boldsymbol{\\mu} + \\boldsymbol{\\epsilon} \\odot  \\boldsymbol{\\sigma}$.\n",
    "\n",
    "En consecuencia $\\mathbf{z}\\sim \\mathcal{N}_m(\\boldsymbol{\\mu},\\text{diag}( \\boldsymbol{\\sigma}^2))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Probabilistico_intro.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Implementación de un  Autoencoder Variacional</p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Regresar al inicio](#Contenido)\n",
    "- [Ejemplo Auto Encoder Variacional para los datos de MNIST](VAE_Example_Dense_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
