{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder aplicado a una prueba de Estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Importa módulos](#Importa-módulos)\n",
    "* [Los datos](#Los-datos)\n",
    "* [Separa áreas](#Separa-áreas)\n",
    "* [Separa datos de entrenamiento y validación](#Separa-datos-de-entrenamiento-y-validación)\n",
    "* [Modelo línea base](#Modelo-línea-base)\n",
    "* [Compila](#Compila)\n",
    "* [Entrena](#Entrena)\n",
    "* [Recupera parámetros de los ítems](#Recupera-parámetros-de-los-ítems)\n",
    "* [Gráficos](#Gráficos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Construimos un autoencoder para una prueba de Estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.losses import  binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.constraints import non_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son 93 items que corresponde a 5 pruebas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee los datos\n",
    "#read csv\n",
    "datos = pd.read_csv(\"../Datos/items_93.csv\")\n",
    "areas = pd.read_csv(\"../Datos/items_areas_93.csv\")\n",
    "# read pickel\n",
    "#datos = pd.read_pickle(\"../Datos/items_93.pkl\")\n",
    "#areas = pd.read_pickle(\"../Datos/items_areas_93.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I12</th>\n",
       "      <th>...</th>\n",
       "      <th>I111</th>\n",
       "      <th>I112</th>\n",
       "      <th>I113</th>\n",
       "      <th>I114</th>\n",
       "      <th>I115</th>\n",
       "      <th>I116</th>\n",
       "      <th>I117</th>\n",
       "      <th>I118</th>\n",
       "      <th>I119</th>\n",
       "      <th>I120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   I1  I2  I3  I4  I5  I6  I7  I9  I10  I12  ...  I111  I112  I113  I114  \\\n",
       "0   1   0   0   0   0   0   0   1    0    1  ...     1     0     0     1   \n",
       "1   0   0   0   0   0   0   1   1    1    1  ...     1     1     1     0   \n",
       "2   0   0   0   0   0   0   1   0    1    0  ...     0     0     1     0   \n",
       "3   1   1   0   1   0   0   1   1    0    1  ...     0     0     0     0   \n",
       "4   1   0   0   0   0   0   0   1    0    0  ...     1     1     0     0   \n",
       "\n",
       "   I115  I116  I117  I118  I119  I120  \n",
       "0     0     0     1     1     1     0  \n",
       "1     1     1     1     1     0     0  \n",
       "2     0     0     1     1     1     0  \n",
       "3     1     0     0     0     1     1  \n",
       "4     0     0     1     1     1     1  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>Id_item</th>\n",
       "      <th>construct_name</th>\n",
       "      <th>construct_nick</th>\n",
       "      <th>Id_construct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I1</td>\n",
       "      <td>1</td>\n",
       "      <td>Textual</td>\n",
       "      <td>\"T\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I2</td>\n",
       "      <td>2</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>\"C\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I3</td>\n",
       "      <td>3</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>\"C\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I4</td>\n",
       "      <td>4</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>\"C\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I5</td>\n",
       "      <td>5</td>\n",
       "      <td>Ciencias</td>\n",
       "      <td>\"C\"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item  Id_item construct_name construct_nick  Id_construct\n",
       "0   I1        1        Textual            \"T\"             1\n",
       "1   I2        2       Ciencias            \"C\"             3\n",
       "2   I3        3       Ciencias            \"C\"             3\n",
       "3   I4        4       Ciencias            \"C\"             3\n",
       "4   I5        5       Ciencias            \"C\"             3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separa áreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# índices de items\n",
    "tex_item = areas[areas.Id_construct==1]['item'].values\n",
    "mat_item = areas[areas.Id_construct==2]['item'].values\n",
    "cie_item = areas[areas.Id_construct==3]['item'].values\n",
    "soc_item = areas[areas.Id_construct==4]['item'].values\n",
    "ima_item = areas[areas.Id_construct==5]['item'].values\n",
    "# Datos por áreas, no requerido de momento\n",
    "tex_data = pd.DataFrame(datos[tex_item])\n",
    "mat_data = pd.DataFrame(datos[mat_item])\n",
    "cie_data = pd.DataFrame(datos[cie_item])\n",
    "soc_data = pd.DataFrame(datos[soc_item])\n",
    "ima_data = pd.DataFrame(datos[ima_item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separa datos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de Datos de Entrenamiento: (32005, 93)\n",
      "Tamaño de Datos de Test         : (3557, 93)\n",
      "Total de Datos                 : 35562\n"
     ]
    }
   ],
   "source": [
    "N = len(datos)# 35562\n",
    "\n",
    "# Porcentaje de entrenamiento\n",
    "\n",
    "pe = 0.9           # Porcentaje de Entrenamiento\n",
    "pt = round(1-pe,2) # Porcentaje de Test\n",
    "\n",
    "# Número de datos de entrenamiento\n",
    "train = int(N*pe)\n",
    "# Número de datos de test\n",
    "test = N-train\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(11)\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "# Elegir los indices de las filas a seleccionar para entrenamiento de manera aleatoria\n",
    "train_choice = np.random.choice(N,train,replace=False)\n",
    "\n",
    "# Extraer datos seleccionados: features = labels\n",
    "x_train=datos.iloc[train_choice,:]\n",
    "print(\"Tamaño de Datos de Entrenamiento:\",x_train.shape)\n",
    "\n",
    "# Elegir los indices de las filas a seleccionar para test \n",
    "test_choice=np.setdiff1d(range(N),train_choice)\n",
    "# Extraer datos seleccionados\n",
    "x_test=datos.iloc[test_choice,:]\n",
    "\n",
    "\n",
    "print(\"Tamaño de Datos de Test         :\",x_test.shape)\n",
    "print(\"Total de Datos                 :\",len(x_train)+len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1] #93\n",
    "hidden_dim = input_size // 2  # 46\n",
    "latent_dim = 5\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "buffer_size = batch_size * 5\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepara dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.cast(x_train, dtype = tf.float32))\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase  VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,dim, dropout=0.2, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        input_size = dim[0]\n",
    "        hidden_dim = dim[1]\n",
    "        latent_dim = dim[2]\n",
    "        \n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_dim,\n",
    "                                                  activation='relu',\n",
    "                                                  name='hidden_layer') # 1\n",
    "        self.mu_parameter = tf.keras.layers.Dense(latent_dim, name='mu_layer') #2\n",
    "        self.sigma_parameter = tf.keras.layers.Dense(latent_dim, name='sigma_layer') #3\n",
    "\n",
    "        #self.fc4 = tf.keras.layers.Dense(hidden_dim) #4   # hidden decoder, no va\n",
    "        self.output_layer = tf.keras.layers.Dense(input_size, \n",
    "                                                  #kernel_constraint=non_neg(),\n",
    "                                                  name='output_layer',\n",
    "                                                 activation='sigmoid') #5   # output\n",
    "        \n",
    "        self.dropout_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.dropout_1(x)\n",
    "        h = self.hidden_layer(h)\n",
    "        h = self.dropout_2(h)  \n",
    "        \n",
    "        return self.mu_parameter(h), self.sigma_parameter(h)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = tf.exp(log_var * 0.5)\n",
    "        eps = tf.random.normal(std.shape)\n",
    "\n",
    "        return mu + eps * std\n",
    "\n",
    "    #def decode_logits(self, z):\n",
    "    #    h = self.fc4(z))\n",
    "    #    return self.fc5(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #return tf.nn.sigmoid(self.decode_logits(z))\n",
    "        return self.output_layer(z)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        mu, log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed_logits = self.decode(z)\n",
    "\n",
    "        return x_reconstructed_logits, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer (Dense)         multiple                  4324      \n",
      "_________________________________________________________________\n",
      "mu_layer (Dense)             multiple                  235       \n",
      "_________________________________________________________________\n",
      "sigma_layer (Dense)          multiple                  235       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         multiple                  558       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 5,352\n",
      "Trainable params: 5,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VAE([input_size, hidden_dim, latent_dim])\n",
    "model.build(input_shape=(3, input_size))\n",
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/10], Step [50/500], Reconst Loss: 69.6418, KL Div: 0.3941, Total Loss: 70.0359\n",
      "Epoch[1/10], Step [100/500], Reconst Loss: 69.1501, KL Div: 0.3187, Total Loss: 69.4688\n",
      "Epoch[1/10], Step [150/500], Reconst Loss: 68.0315, KL Div: 0.3079, Total Loss: 68.3394\n",
      "Epoch[1/10], Step [200/500], Reconst Loss: 66.8964, KL Div: 0.5465, Total Loss: 67.4430\n",
      "Epoch[1/10], Step [250/500], Reconst Loss: 67.3695, KL Div: 0.8511, Total Loss: 68.2206\n",
      "Epoch[1/10], Step [300/500], Reconst Loss: 65.9233, KL Div: 0.9870, Total Loss: 66.9102\n",
      "Epoch[1/10], Step [350/500], Reconst Loss: 65.0956, KL Div: 1.0844, Total Loss: 66.1800\n",
      "Epoch[1/10], Step [400/500], Reconst Loss: 65.2337, KL Div: 1.2330, Total Loss: 66.4667\n",
      "Epoch[1/10], Step [450/500], Reconst Loss: 64.8367, KL Div: 1.1990, Total Loss: 66.0357\n",
      "Epoch[1/10], Step [500/500], Reconst Loss: 64.1623, KL Div: 1.2181, Total Loss: 65.3804\n",
      "Epoch[2/10], Step [50/500], Reconst Loss: 63.4984, KL Div: 1.0924, Total Loss: 64.5908\n",
      "Epoch[2/10], Step [100/500], Reconst Loss: 62.7917, KL Div: 1.2542, Total Loss: 64.0459\n",
      "Epoch[2/10], Step [150/500], Reconst Loss: 63.5304, KL Div: 1.3606, Total Loss: 64.8910\n",
      "Epoch[2/10], Step [200/500], Reconst Loss: 63.2194, KL Div: 1.3413, Total Loss: 64.5607\n",
      "Epoch[2/10], Step [250/500], Reconst Loss: 63.3306, KL Div: 1.2247, Total Loss: 64.5553\n",
      "Epoch[2/10], Step [300/500], Reconst Loss: 62.9033, KL Div: 1.1253, Total Loss: 64.0287\n",
      "Epoch[2/10], Step [350/500], Reconst Loss: 62.7193, KL Div: 1.0952, Total Loss: 63.8144\n",
      "Epoch[2/10], Step [400/500], Reconst Loss: 63.1609, KL Div: 1.1030, Total Loss: 64.2639\n",
      "Epoch[2/10], Step [450/500], Reconst Loss: 62.7782, KL Div: 1.2545, Total Loss: 64.0328\n",
      "Epoch[2/10], Step [500/500], Reconst Loss: 63.0913, KL Div: 1.0298, Total Loss: 64.1212\n",
      "Epoch[3/10], Step [50/500], Reconst Loss: 63.4738, KL Div: 1.1567, Total Loss: 64.6305\n",
      "Epoch[3/10], Step [100/500], Reconst Loss: 61.9707, KL Div: 1.0963, Total Loss: 63.0669\n",
      "Epoch[3/10], Step [150/500], Reconst Loss: 62.3856, KL Div: 1.0271, Total Loss: 63.4127\n",
      "Epoch[3/10], Step [200/500], Reconst Loss: 62.4421, KL Div: 1.1548, Total Loss: 63.5969\n",
      "Epoch[3/10], Step [250/500], Reconst Loss: 62.6039, KL Div: 1.1342, Total Loss: 63.7381\n",
      "Epoch[3/10], Step [300/500], Reconst Loss: 62.7212, KL Div: 1.1522, Total Loss: 63.8734\n",
      "Epoch[3/10], Step [350/500], Reconst Loss: 61.9655, KL Div: 1.0456, Total Loss: 63.0111\n",
      "Epoch[3/10], Step [400/500], Reconst Loss: 63.4681, KL Div: 0.9523, Total Loss: 64.4204\n",
      "Epoch[3/10], Step [450/500], Reconst Loss: 62.6382, KL Div: 0.9909, Total Loss: 63.6291\n",
      "Epoch[3/10], Step [500/500], Reconst Loss: 62.4183, KL Div: 1.0351, Total Loss: 63.4533\n",
      "Epoch[4/10], Step [50/500], Reconst Loss: 63.6727, KL Div: 1.0014, Total Loss: 64.6741\n",
      "Epoch[4/10], Step [100/500], Reconst Loss: 62.9181, KL Div: 0.9022, Total Loss: 63.8203\n",
      "Epoch[4/10], Step [150/500], Reconst Loss: 62.4236, KL Div: 0.8798, Total Loss: 63.3035\n",
      "Epoch[4/10], Step [200/500], Reconst Loss: 62.6538, KL Div: 1.0633, Total Loss: 63.7171\n",
      "Epoch[4/10], Step [250/500], Reconst Loss: 62.3885, KL Div: 0.9900, Total Loss: 63.3784\n",
      "Epoch[4/10], Step [300/500], Reconst Loss: 62.3933, KL Div: 0.9350, Total Loss: 63.3284\n",
      "Epoch[4/10], Step [350/500], Reconst Loss: 62.7624, KL Div: 1.0368, Total Loss: 63.7992\n",
      "Epoch[4/10], Step [400/500], Reconst Loss: 62.5851, KL Div: 0.8500, Total Loss: 63.4351\n",
      "Epoch[4/10], Step [450/500], Reconst Loss: 62.1961, KL Div: 1.0052, Total Loss: 63.2013\n",
      "Epoch[4/10], Step [500/500], Reconst Loss: 62.8630, KL Div: 0.9379, Total Loss: 63.8009\n",
      "Epoch[5/10], Step [50/500], Reconst Loss: 62.7562, KL Div: 0.8948, Total Loss: 63.6510\n",
      "Epoch[5/10], Step [100/500], Reconst Loss: 62.3538, KL Div: 0.8532, Total Loss: 63.2070\n",
      "Epoch[5/10], Step [150/500], Reconst Loss: 62.4923, KL Div: 1.0255, Total Loss: 63.5178\n",
      "Epoch[5/10], Step [200/500], Reconst Loss: 62.5244, KL Div: 0.8864, Total Loss: 63.4108\n",
      "Epoch[5/10], Step [250/500], Reconst Loss: 62.5628, KL Div: 0.8175, Total Loss: 63.3803\n",
      "Epoch[5/10], Step [300/500], Reconst Loss: 62.8735, KL Div: 0.8203, Total Loss: 63.6937\n",
      "Epoch[5/10], Step [350/500], Reconst Loss: 63.2526, KL Div: 0.8835, Total Loss: 64.1361\n",
      "Epoch[5/10], Step [400/500], Reconst Loss: 62.5420, KL Div: 0.8899, Total Loss: 63.4319\n",
      "Epoch[5/10], Step [450/500], Reconst Loss: 62.0008, KL Div: 0.9575, Total Loss: 62.9583\n",
      "Epoch[5/10], Step [500/500], Reconst Loss: 62.2892, KL Div: 0.8037, Total Loss: 63.0930\n",
      "Epoch[6/10], Step [50/500], Reconst Loss: 62.4111, KL Div: 1.0115, Total Loss: 63.4226\n",
      "Epoch[6/10], Step [100/500], Reconst Loss: 62.0521, KL Div: 0.9506, Total Loss: 63.0027\n",
      "Epoch[6/10], Step [150/500], Reconst Loss: 62.3022, KL Div: 0.9224, Total Loss: 63.2246\n",
      "Epoch[6/10], Step [200/500], Reconst Loss: 61.8140, KL Div: 0.6097, Total Loss: 62.4237\n",
      "Epoch[6/10], Step [250/500], Reconst Loss: 62.3654, KL Div: 0.9199, Total Loss: 63.2854\n",
      "Epoch[6/10], Step [300/500], Reconst Loss: 62.5177, KL Div: 0.8793, Total Loss: 63.3970\n",
      "Epoch[6/10], Step [350/500], Reconst Loss: 62.4746, KL Div: 0.8301, Total Loss: 63.3047\n",
      "Epoch[6/10], Step [400/500], Reconst Loss: 62.0236, KL Div: 0.7189, Total Loss: 62.7425\n",
      "Epoch[6/10], Step [450/500], Reconst Loss: 62.8573, KL Div: 0.8227, Total Loss: 63.6800\n",
      "Epoch[6/10], Step [500/500], Reconst Loss: 62.0195, KL Div: 0.8363, Total Loss: 62.8558\n",
      "Epoch[7/10], Step [50/500], Reconst Loss: 62.3931, KL Div: 0.8576, Total Loss: 63.2507\n",
      "Epoch[7/10], Step [100/500], Reconst Loss: 61.9502, KL Div: 0.6703, Total Loss: 62.6205\n",
      "Epoch[7/10], Step [150/500], Reconst Loss: 61.8571, KL Div: 0.8134, Total Loss: 62.6705\n",
      "Epoch[7/10], Step [200/500], Reconst Loss: 61.4962, KL Div: 0.7639, Total Loss: 62.2601\n",
      "Epoch[7/10], Step [250/500], Reconst Loss: 62.0553, KL Div: 0.7573, Total Loss: 62.8125\n",
      "Epoch[7/10], Step [300/500], Reconst Loss: 61.9840, KL Div: 0.7456, Total Loss: 62.7297\n",
      "Epoch[7/10], Step [350/500], Reconst Loss: 62.3082, KL Div: 0.7663, Total Loss: 63.0744\n",
      "Epoch[7/10], Step [400/500], Reconst Loss: 62.5308, KL Div: 0.8524, Total Loss: 63.3832\n",
      "Epoch[7/10], Step [450/500], Reconst Loss: 62.1092, KL Div: 0.7494, Total Loss: 62.8586\n",
      "Epoch[7/10], Step [500/500], Reconst Loss: 62.5115, KL Div: 0.7607, Total Loss: 63.2722\n",
      "Epoch[8/10], Step [50/500], Reconst Loss: 62.4480, KL Div: 0.7208, Total Loss: 63.1688\n",
      "Epoch[8/10], Step [100/500], Reconst Loss: 62.4225, KL Div: 0.7581, Total Loss: 63.1806\n",
      "Epoch[8/10], Step [150/500], Reconst Loss: 61.8820, KL Div: 0.8084, Total Loss: 62.6903\n",
      "Epoch[8/10], Step [200/500], Reconst Loss: 61.2546, KL Div: 0.6939, Total Loss: 61.9485\n",
      "Epoch[8/10], Step [250/500], Reconst Loss: 62.1120, KL Div: 0.8266, Total Loss: 62.9386\n",
      "Epoch[8/10], Step [300/500], Reconst Loss: 61.4891, KL Div: 0.7267, Total Loss: 62.2158\n",
      "Epoch[8/10], Step [350/500], Reconst Loss: 60.8964, KL Div: 0.7548, Total Loss: 61.6512\n",
      "Epoch[8/10], Step [400/500], Reconst Loss: 61.8917, KL Div: 0.7793, Total Loss: 62.6711\n",
      "Epoch[8/10], Step [450/500], Reconst Loss: 61.8125, KL Div: 0.7475, Total Loss: 62.5600\n",
      "Epoch[8/10], Step [500/500], Reconst Loss: 61.3066, KL Div: 0.7067, Total Loss: 62.0133\n",
      "Epoch[9/10], Step [50/500], Reconst Loss: 61.2216, KL Div: 0.7716, Total Loss: 61.9932\n",
      "Epoch[9/10], Step [100/500], Reconst Loss: 61.2080, KL Div: 0.7356, Total Loss: 61.9436\n",
      "Epoch[9/10], Step [150/500], Reconst Loss: 61.9436, KL Div: 0.7203, Total Loss: 62.6639\n",
      "Epoch[9/10], Step [200/500], Reconst Loss: 61.3541, KL Div: 0.6981, Total Loss: 62.0522\n",
      "Epoch[9/10], Step [250/500], Reconst Loss: 62.4477, KL Div: 0.7863, Total Loss: 63.2340\n",
      "Epoch[9/10], Step [300/500], Reconst Loss: 61.7425, KL Div: 0.7672, Total Loss: 62.5096\n",
      "Epoch[9/10], Step [350/500], Reconst Loss: 61.6751, KL Div: 0.5921, Total Loss: 62.2672\n",
      "Epoch[9/10], Step [400/500], Reconst Loss: 61.5283, KL Div: 0.7469, Total Loss: 62.2752\n",
      "Epoch[9/10], Step [450/500], Reconst Loss: 60.5841, KL Div: 0.5923, Total Loss: 61.1764\n",
      "Epoch[9/10], Step [500/500], Reconst Loss: 62.3207, KL Div: 0.6805, Total Loss: 63.0012\n",
      "Epoch[10/10], Step [50/500], Reconst Loss: 61.2653, KL Div: 0.6791, Total Loss: 61.9444\n",
      "Epoch[10/10], Step [100/500], Reconst Loss: 62.0391, KL Div: 0.5055, Total Loss: 62.5446\n",
      "Epoch[10/10], Step [150/500], Reconst Loss: 61.5413, KL Div: 0.6491, Total Loss: 62.1904\n",
      "Epoch[10/10], Step [200/500], Reconst Loss: 60.9414, KL Div: 0.5734, Total Loss: 61.5149\n",
      "Epoch[10/10], Step [250/500], Reconst Loss: 62.2046, KL Div: 0.6646, Total Loss: 62.8692\n",
      "Epoch[10/10], Step [300/500], Reconst Loss: 61.5774, KL Div: 0.5768, Total Loss: 62.1542\n",
      "Epoch[10/10], Step [350/500], Reconst Loss: 61.8321, KL Div: 0.5499, Total Loss: 62.3820\n",
      "Epoch[10/10], Step [400/500], Reconst Loss: 62.0703, KL Div: 0.5945, Total Loss: 62.6648\n",
      "Epoch[10/10], Step [450/500], Reconst Loss: 62.0952, KL Div: 0.5858, Total Loss: 62.6810\n",
      "Epoch[10/10], Step [500/500], Reconst Loss: 61.6164, KL Div: 0.6326, Total Loss: 62.2490\n"
     ]
    }
   ],
   "source": [
    "num_batches = x_train.shape[0] // batch_size\n",
    "\n",
    "kl_div_list = []\n",
    "reconstruction_list = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for step, x in enumerate(dataset):\n",
    "\n",
    "        #x = tf.reshape(x, [-1, input_size])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Forward pass\n",
    "            x_reconstruction_logits, mu, log_var = model(x)\n",
    "            #print (x_reconstruction_logits, mu, log_var)\n",
    "            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_reconstruction_logits)\n",
    "            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size\n",
    "            kl_div = - 0.5 * tf.reduce_sum(1. + log_var - tf.square(mu) - tf.exp(log_var), axis=-1)\n",
    "            kl_div = tf.reduce_mean(kl_div)\n",
    "\n",
    "            # Backprop and optimize\n",
    "            loss = tf.reduce_mean(reconstruction_loss) + kl_div\n",
    "            # save losses\n",
    "            kl_div_list.append(kl_div)\n",
    "            reconstruction_list.append(reconstruction_loss)\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        for g in gradients:\n",
    "            tf.clip_by_norm(g, 15)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}, Total Loss: {:.4f}\"\n",
    "                  .format(epoch + 1, num_epochs, step + 1, num_batches, float(reconstruction_loss), float(kl_div), float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupera parámetros de los ítems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_layer= model.get_layer( name='output_layer')\n",
    "item_params = tri_layer.get_weights()\n",
    "item_params = np.vstack([np.array(item_params[0]),np.array(item_params[1])])\n",
    "item_params = pd.DataFrame(np .transpose(item_params))\n",
    "\n",
    "colnames =['dim_1', 'dim_2','dim_3','dim_4','dim_5','intercepto']\n",
    "\n",
    "item_params.columns = colnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>intercepto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912029</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>-0.444610</td>\n",
       "      <td>0.767766</td>\n",
       "      <td>0.627911</td>\n",
       "      <td>0.414429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.207998</td>\n",
       "      <td>-0.015507</td>\n",
       "      <td>-0.565330</td>\n",
       "      <td>0.928492</td>\n",
       "      <td>0.916048</td>\n",
       "      <td>1.205181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.201893</td>\n",
       "      <td>0.603874</td>\n",
       "      <td>-0.372167</td>\n",
       "      <td>-0.649868</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>-2.482927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933209</td>\n",
       "      <td>0.545310</td>\n",
       "      <td>-0.180608</td>\n",
       "      <td>-0.861281</td>\n",
       "      <td>0.498072</td>\n",
       "      <td>-2.872647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925277</td>\n",
       "      <td>0.174909</td>\n",
       "      <td>-0.419643</td>\n",
       "      <td>0.176235</td>\n",
       "      <td>0.681475</td>\n",
       "      <td>-0.198225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.763248</td>\n",
       "      <td>0.501723</td>\n",
       "      <td>-0.113317</td>\n",
       "      <td>-0.815546</td>\n",
       "      <td>0.410663</td>\n",
       "      <td>-3.002936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.406333</td>\n",
       "      <td>0.533686</td>\n",
       "      <td>-0.496779</td>\n",
       "      <td>-0.391025</td>\n",
       "      <td>0.964651</td>\n",
       "      <td>-2.226226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.425187</td>\n",
       "      <td>0.693219</td>\n",
       "      <td>-0.455791</td>\n",
       "      <td>-0.718865</td>\n",
       "      <td>0.834543</td>\n",
       "      <td>-2.572235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.294202</td>\n",
       "      <td>0.660563</td>\n",
       "      <td>-0.371222</td>\n",
       "      <td>-0.729657</td>\n",
       "      <td>0.844527</td>\n",
       "      <td>-2.640185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.636204</td>\n",
       "      <td>0.026264</td>\n",
       "      <td>-0.246002</td>\n",
       "      <td>0.336767</td>\n",
       "      <td>0.399698</td>\n",
       "      <td>-0.273801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.567253</td>\n",
       "      <td>-0.097432</td>\n",
       "      <td>-0.359000</td>\n",
       "      <td>0.752377</td>\n",
       "      <td>0.500440</td>\n",
       "      <td>0.874114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.187661</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>-0.490048</td>\n",
       "      <td>0.399671</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>-0.516763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.202053</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>-0.544469</td>\n",
       "      <td>0.897724</td>\n",
       "      <td>0.850119</td>\n",
       "      <td>0.896663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.757617</td>\n",
       "      <td>-0.178040</td>\n",
       "      <td>-0.477118</td>\n",
       "      <td>1.226619</td>\n",
       "      <td>0.553375</td>\n",
       "      <td>1.764856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.112225</td>\n",
       "      <td>0.042242</td>\n",
       "      <td>-0.527249</td>\n",
       "      <td>0.772792</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.624717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.078646</td>\n",
       "      <td>0.592912</td>\n",
       "      <td>-0.203974</td>\n",
       "      <td>-0.785854</td>\n",
       "      <td>0.581140</td>\n",
       "      <td>-2.922275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.452471</td>\n",
       "      <td>0.671076</td>\n",
       "      <td>-0.410176</td>\n",
       "      <td>-0.666844</td>\n",
       "      <td>0.846520</td>\n",
       "      <td>-2.742507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.391236</td>\n",
       "      <td>0.656286</td>\n",
       "      <td>-0.471381</td>\n",
       "      <td>-0.627494</td>\n",
       "      <td>0.857722</td>\n",
       "      <td>-2.534696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.159670</td>\n",
       "      <td>0.634531</td>\n",
       "      <td>-0.271960</td>\n",
       "      <td>-0.829231</td>\n",
       "      <td>0.685311</td>\n",
       "      <td>-2.849706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.376435</td>\n",
       "      <td>-0.386119</td>\n",
       "      <td>-0.367629</td>\n",
       "      <td>1.308196</td>\n",
       "      <td>0.324325</td>\n",
       "      <td>2.365697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.892781</td>\n",
       "      <td>0.533616</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>-0.736274</td>\n",
       "      <td>0.580573</td>\n",
       "      <td>-2.680988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.363449</td>\n",
       "      <td>-0.435426</td>\n",
       "      <td>-0.042800</td>\n",
       "      <td>0.913107</td>\n",
       "      <td>-0.099340</td>\n",
       "      <td>2.983779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.277616</td>\n",
       "      <td>-0.436097</td>\n",
       "      <td>-0.049081</td>\n",
       "      <td>0.973354</td>\n",
       "      <td>-0.058170</td>\n",
       "      <td>2.764486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.431220</td>\n",
       "      <td>0.519772</td>\n",
       "      <td>-0.519662</td>\n",
       "      <td>-0.289696</td>\n",
       "      <td>1.068577</td>\n",
       "      <td>-2.089291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.358049</td>\n",
       "      <td>-0.440163</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.843827</td>\n",
       "      <td>-0.159064</td>\n",
       "      <td>2.501656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.417007</td>\n",
       "      <td>-0.122623</td>\n",
       "      <td>-0.301415</td>\n",
       "      <td>0.840312</td>\n",
       "      <td>0.397480</td>\n",
       "      <td>1.088177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.402455</td>\n",
       "      <td>0.333537</td>\n",
       "      <td>-0.483661</td>\n",
       "      <td>-0.012463</td>\n",
       "      <td>0.992860</td>\n",
       "      <td>-1.070866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.183518</td>\n",
       "      <td>0.552735</td>\n",
       "      <td>-0.380212</td>\n",
       "      <td>-0.530085</td>\n",
       "      <td>0.756064</td>\n",
       "      <td>-2.176149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.236835</td>\n",
       "      <td>-0.419884</td>\n",
       "      <td>-0.099820</td>\n",
       "      <td>1.000735</td>\n",
       "      <td>-0.048232</td>\n",
       "      <td>2.932859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.620095</td>\n",
       "      <td>-0.038086</td>\n",
       "      <td>-0.340588</td>\n",
       "      <td>0.597592</td>\n",
       "      <td>0.458886</td>\n",
       "      <td>0.487196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.128998</td>\n",
       "      <td>0.563237</td>\n",
       "      <td>-0.285069</td>\n",
       "      <td>-0.695782</td>\n",
       "      <td>0.649975</td>\n",
       "      <td>-2.612824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.449170</td>\n",
       "      <td>-0.439734</td>\n",
       "      <td>-0.021822</td>\n",
       "      <td>0.915496</td>\n",
       "      <td>-0.164164</td>\n",
       "      <td>2.823318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.217071</td>\n",
       "      <td>-0.415963</td>\n",
       "      <td>-0.100646</td>\n",
       "      <td>0.995592</td>\n",
       "      <td>-0.077352</td>\n",
       "      <td>2.740908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.332311</td>\n",
       "      <td>0.602195</td>\n",
       "      <td>-0.403723</td>\n",
       "      <td>-0.608348</td>\n",
       "      <td>0.879112</td>\n",
       "      <td>-2.615805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.318384</td>\n",
       "      <td>-0.389730</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.818224</td>\n",
       "      <td>-0.074857</td>\n",
       "      <td>2.501266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.565584</td>\n",
       "      <td>-0.178568</td>\n",
       "      <td>-0.380769</td>\n",
       "      <td>1.053636</td>\n",
       "      <td>0.512334</td>\n",
       "      <td>1.727545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.395984</td>\n",
       "      <td>-0.326174</td>\n",
       "      <td>-0.346343</td>\n",
       "      <td>1.298752</td>\n",
       "      <td>0.310132</td>\n",
       "      <td>2.316451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.469376</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>-0.561290</td>\n",
       "      <td>0.474668</td>\n",
       "      <td>0.876626</td>\n",
       "      <td>-0.477839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.793290</td>\n",
       "      <td>0.669615</td>\n",
       "      <td>-0.601376</td>\n",
       "      <td>-0.372957</td>\n",
       "      <td>1.171662</td>\n",
       "      <td>-2.207957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.640410</td>\n",
       "      <td>0.719188</td>\n",
       "      <td>-0.439557</td>\n",
       "      <td>-0.700879</td>\n",
       "      <td>0.913655</td>\n",
       "      <td>-2.729257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.159342</td>\n",
       "      <td>0.602193</td>\n",
       "      <td>-0.260430</td>\n",
       "      <td>-0.819193</td>\n",
       "      <td>0.687334</td>\n",
       "      <td>-2.940360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.986245</td>\n",
       "      <td>0.564304</td>\n",
       "      <td>-0.224199</td>\n",
       "      <td>-0.829000</td>\n",
       "      <td>0.573857</td>\n",
       "      <td>-2.919255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.158881</td>\n",
       "      <td>0.598493</td>\n",
       "      <td>-0.353991</td>\n",
       "      <td>-0.753651</td>\n",
       "      <td>0.770116</td>\n",
       "      <td>-2.458699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.982966</td>\n",
       "      <td>0.556155</td>\n",
       "      <td>-0.197785</td>\n",
       "      <td>-0.804571</td>\n",
       "      <td>0.562448</td>\n",
       "      <td>-2.975564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.966732</td>\n",
       "      <td>0.515983</td>\n",
       "      <td>-0.148440</td>\n",
       "      <td>-0.801364</td>\n",
       "      <td>0.507061</td>\n",
       "      <td>-3.073876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.203172</td>\n",
       "      <td>0.600043</td>\n",
       "      <td>-0.271737</td>\n",
       "      <td>-0.786203</td>\n",
       "      <td>0.712395</td>\n",
       "      <td>-2.851248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.974709</td>\n",
       "      <td>0.542059</td>\n",
       "      <td>-0.193541</td>\n",
       "      <td>-0.804099</td>\n",
       "      <td>0.536037</td>\n",
       "      <td>-2.987114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.486068</td>\n",
       "      <td>0.477339</td>\n",
       "      <td>-0.597822</td>\n",
       "      <td>-0.248633</td>\n",
       "      <td>1.122904</td>\n",
       "      <td>-1.720759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.571759</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>-0.289077</td>\n",
       "      <td>0.201842</td>\n",
       "      <td>0.480072</td>\n",
       "      <td>-0.393078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.878029</td>\n",
       "      <td>0.499513</td>\n",
       "      <td>-0.135398</td>\n",
       "      <td>-0.795562</td>\n",
       "      <td>0.439692</td>\n",
       "      <td>-3.154299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim_1     dim_2     dim_3     dim_4     dim_5  intercepto\n",
       "0   0.912029 -0.017002 -0.444610  0.767766  0.627911    0.414429\n",
       "1   1.207998 -0.015507 -0.565330  0.928492  0.916048    1.205181\n",
       "2   1.201893  0.603874 -0.372167 -0.649868  0.756885   -2.482927\n",
       "3   0.933209  0.545310 -0.180608 -0.861281  0.498072   -2.872647\n",
       "4   0.925277  0.174909 -0.419643  0.176235  0.681475   -0.198225\n",
       "5   0.763248  0.501723 -0.113317 -0.815546  0.410663   -3.002936\n",
       "6   1.406333  0.533686 -0.496779 -0.391025  0.964651   -2.226226\n",
       "7   1.425187  0.693219 -0.455791 -0.718865  0.834543   -2.572235\n",
       "8   1.294202  0.660563 -0.371222 -0.729657  0.844527   -2.640185\n",
       "9   0.636204  0.026264 -0.246002  0.336767  0.399698   -0.273801\n",
       "10  0.567253 -0.097432 -0.359000  0.752377  0.500440    0.874114\n",
       "11  1.187661  0.170883 -0.490048  0.399671  0.774336   -0.516763\n",
       "12  1.202053 -0.000457 -0.544469  0.897724  0.850119    0.896663\n",
       "13  0.757617 -0.178040 -0.477118  1.226619  0.553375    1.764856\n",
       "14  1.112225  0.042242 -0.527249  0.772792  0.772414    0.624717\n",
       "15  1.078646  0.592912 -0.203974 -0.785854  0.581140   -2.922275\n",
       "16  1.452471  0.671076 -0.410176 -0.666844  0.846520   -2.742507\n",
       "17  1.391236  0.656286 -0.471381 -0.627494  0.857722   -2.534696\n",
       "18  1.159670  0.634531 -0.271960 -0.829231  0.685311   -2.849706\n",
       "19  0.376435 -0.386119 -0.367629  1.308196  0.324325    2.365697\n",
       "20  0.892781  0.533616 -0.286500 -0.736274  0.580573   -2.680988\n",
       "21 -0.363449 -0.435426 -0.042800  0.913107 -0.099340    2.983779\n",
       "22 -0.277616 -0.436097 -0.049081  0.973354 -0.058170    2.764486\n",
       "23  1.431220  0.519772 -0.519662 -0.289696  1.068577   -2.089291\n",
       "24 -0.358049 -0.440163  0.027000  0.843827 -0.159064    2.501656\n",
       "25  0.417007 -0.122623 -0.301415  0.840312  0.397480    1.088177\n",
       "26  1.402455  0.333537 -0.483661 -0.012463  0.992860   -1.070866\n",
       "27  1.183518  0.552735 -0.380212 -0.530085  0.756064   -2.176149\n",
       "28 -0.236835 -0.419884 -0.099820  1.000735 -0.048232    2.932859\n",
       "29  0.620095 -0.038086 -0.340588  0.597592  0.458886    0.487196\n",
       "30  1.128998  0.563237 -0.285069 -0.695782  0.649975   -2.612824\n",
       "31 -0.449170 -0.439734 -0.021822  0.915496 -0.164164    2.823318\n",
       "32 -0.217071 -0.415963 -0.100646  0.995592 -0.077352    2.740908\n",
       "33  1.332311  0.602195 -0.403723 -0.608348  0.879112   -2.615805\n",
       "34 -0.318384 -0.389730 -0.031064  0.818224 -0.074857    2.501266\n",
       "35  0.565584 -0.178568 -0.380769  1.053636  0.512334    1.727545\n",
       "36  0.395984 -0.326174 -0.346343  1.298752  0.310132    2.316451\n",
       "37  1.469376  0.104762 -0.561290  0.474668  0.876626   -0.477839\n",
       "38  1.793290  0.669615 -0.601376 -0.372957  1.171662   -2.207957\n",
       "39  1.640410  0.719188 -0.439557 -0.700879  0.913655   -2.729257\n",
       "40  1.159342  0.602193 -0.260430 -0.819193  0.687334   -2.940360\n",
       "41  0.986245  0.564304 -0.224199 -0.829000  0.573857   -2.919255\n",
       "42  1.158881  0.598493 -0.353991 -0.753651  0.770116   -2.458699\n",
       "43  0.982966  0.556155 -0.197785 -0.804571  0.562448   -2.975564\n",
       "44  0.966732  0.515983 -0.148440 -0.801364  0.507061   -3.073876\n",
       "45  1.203172  0.600043 -0.271737 -0.786203  0.712395   -2.851248\n",
       "46  0.974709  0.542059 -0.193541 -0.804099  0.536037   -2.987114\n",
       "47  1.486068  0.477339 -0.597822 -0.248633  1.122904   -1.720759\n",
       "48  0.571759  0.033564 -0.289077  0.201842  0.480072   -0.393078\n",
       "49  0.878029  0.499513 -0.135398 -0.795562  0.439692   -3.154299"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_params.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
