{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Procesamiento de textos - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Blei et al.,[Latent Dirichlet Allocation, 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "2. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python, 2018](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24), en Toward data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Análisis superficial de textos](#Análisis-superficial-de-textos)\n",
    "* [Terminología general](#Terminología-general)\n",
    "* [Preprocesamiento de datos textuales](#Preprocesamiento-de-datos-textuales)\n",
    "* [TF-IDF](#TF-IDF)\n",
    "* [Semántica latente](#Semántica-latente)\n",
    "* [Modelos generativos: Latent Dirichlet Allocation](#Modelos-generativos:-Latent-Dirichlet-Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección hacemos el primer acercamiento la tratamiento de datos de tipo textual.\n",
    "\n",
    "Desarrollaremos un primer ejemplo acerca de análisis de sentimiento basado en textos.\n",
    "\n",
    "\n",
    "Los humanos nos  comunicamos utilizando lenguajes naturales. Los lenguajes naturales difierens de los lenguajes de programación en que éstos últimos siguen reglas sintácticas y semánticas extrictas, mientras que los primeros por su complejidad dependen del contexto.\n",
    "\n",
    "\n",
    "En general el análisis de textos tiene dos grandes subáreas: el análisis superficial de textos y el procesamiento del lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis superficial de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta subárea se desarrolló primero, debido a que los problemas asociados al lengaje natural en este caso son mas simples. Se trata de técnicas en la cuales se busca encontar los tópicos subayacentes en los texto. En este sentido, son modelos de tipo no supervisado y consecuencia basados en técnicas de clasificación automática. \n",
    "\n",
    "Estas técnicas están orientadas a detectar clusters de palabras y documentos en grandes corpus de datos.\n",
    "\n",
    "Un documento es en esta caso una unidad distinguible de otras en el corpus. Por ejemplo una respuesta abierta en una encuesta, un comentario en una revisión, un abstract de un documento, etc. \n",
    "\n",
    "Luego de omitidos términos que se considera que no aportan a la detección de tópicos (temáticas), usualmente conocidos como **palabras vacías (stop words)** y de otros procesos de preprocesamiento como lematización, recorte (steeming),  es común construir una matriz denominada documento-témino (dtm).\n",
    "\n",
    "Esta matriz dtm representa por las filas a cada uno de los documentos individuales del corpus y por las columnas a cada uno de los términos conservados en el análisis. Cada posición de la matriz contien el número de veces que un término aparece en el documento. En lagunos casos esta es una matriz binaria, en cuyo caso la dtm indica cunado un término aparece en un documento.\n",
    "\n",
    "La  dtm es la base de las técnicas conocidas genéricamwnte como **bolsa de palabras**. El nombre deriva del hechp de que al organizar la dtm, el contexto de las palabras en cada documento se pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminología general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras o términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La palabra es la unidad mínima de información  en el trabajo con lenguaje natural. \n",
    "\n",
    "Desde una perspectiva muy moderna las palabras son objetos que puede pensarse como puntos que están en un espacio de alta dimensión, de tal manera que puntos cercanos en algún sentido de distancia corresponde a palabras que tienen una cercanía dentro de un universo de palabras considerado.\n",
    "\n",
    "La siguiente imagen corresponde a uno conjunto de palabras de la astrofísica, consideradas en un estudio de resumenes de artículos científicos. Este es un gráfico obtenido luego de un procesamiento como lo que mostramos hoy, desarrollado por Montenegro y Montenegro usando una técnica de análisis basada en la teoría de respuesta al ítem multidimensional (TRIM).\n",
    "\n",
    "En este documeneto las palabaras se denotarán como $w_i, i =1,2,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cluster_kmeans_10.png\" width=\"700\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Areas de conocimiento Astrofísica, a partir de artículos científicos</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los documentos son los sujetos en los análisis textual superficial. Suponemos que se tiene un conjunto de documentos individualaes, cada uno de los cuales se denotará por $\\mathbf{w}$. Se considera que un documento es una sucesión de palabras de $N$ palabras. Así se tiene que un documento se denota como $\\mathbf{w} = \\{w_1,\\ldots,w_N \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un corpus es una colección de documentos en un problema particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tópicos son áreas latentes a las cuales están asociados tanto las palabras como los documentos. Uno de los propósitos principales del análisis de textos es descubrir o poner en evidencia tales tópicos.\n",
    "\n",
    "La figura anterior muestra por ejemplo la presencia de 10 tópicos en el conjunto de documentos de astrofísica analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos textuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que sigue, vamos utilizar los términos token y tokenizar, que aún no son adoptados por la Real Academia de la Lengua, pero que creemos pronto lo serán como tantos otros debidos a su enorme utlización actual. \n",
    "\n",
    "Realizaremos los siguientes pasos:\n",
    "    \n",
    "- **Tokenización**: divide el texto en oraciones y las oraciones en palabras. Ponga las palabras en minúsculas y elimine la puntuación.\n",
    "- Se **eliminan las palabras que tienen menos de 3 caracteres**.\n",
    "- Se eliminan todas las **palabras vacías**.\n",
    "- Las palabras se **lematizan**: las palabras en tercera persona se cambian a primera persona y los verbos en tiempo pasado y futuro se cambian a presente.\n",
    "- Las palabras se recortan (**stemming**): las palabras se reducen a su forma raíz.\n",
    "- Cargaremos las bibliotecas *gensim* y *nltk*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos términos que se utilizarán con frecuencia son:\n",
    "\n",
    "- *Corpus*: cuerpo del texto, singular. Corpora es el plural de esto.\n",
    "- *Léxico*: palabras y sus significados.\n",
    "- *Token*: cada \"entidad\" que es parte de lo que sea que se dividió según las reglas. Por ejemplo, cada palabra es un token cuando una oración se tokeniza en palabras. Cada oración también puede ser un token, si ha convertido las oraciones en un párrafo.\n",
    "\n",
    "Básicamente, tokenizar implica dividir oraciones y palabras del cuerpo del texto.\n",
    "\n",
    "Veá el siguiente ejemplo tomado de [Geek for Geeks](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/?ref=rp). Usamos la librería *nltk*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.', 'There are 365 days usually.', 'This year is 2020.']\n",
      "\n",
      "\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'There', 'are', '365', 'days', 'usually', '.', 'This', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing  \n",
    "# libraries \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "  \n",
    "text = \"Natural language processing (NLP) is a field \" \\\n",
    "       + \"of computer science, artificial intelligence \" \\\n",
    "       + \"and computational linguistics concerned with \" \\\n",
    "       +\"the interactions between computers and human \" \\\n",
    "       + \"(natural) languages, and, in particular, \" \\\n",
    "       + \"concerned with programming computers to \" \\\n",
    "       + \"fruitfully process large natural language \" \\\n",
    "       + \"corpora. Challenges in natural language \" \\\n",
    "       + \"processing frequently involve natural \" \\\n",
    "       + \"language understanding, natural language \" \\\n",
    "       + \"generation frequently from formal, machine\" \\\n",
    "       + \"-readable logical forms), connecting language \" \\\n",
    "       + \"and machine perception, managing human-\" \\\n",
    "       + \"computer dialog systems, or some combination \" \\\n",
    "       + \"thereof. There are 365 days usually. \" \\\n",
    "       + \"This year is 2020.\"\n",
    "  \n",
    "print(sent_tokenize(text)) \n",
    "print('\\n')\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convierte el texto en unidades (tokeniza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "#\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cambiar texto a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '365', 'days', 'usually', '.', 'this', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve carateres especiales - expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones regulares son objetos matemáticos que permiten interpretar trozos de texto. Son claves en la cosntrucción del lenguajes de programación. Aquí vamos a usar la librería *re* de Python creada para el manejo de expresiones regulares.\n",
    "\n",
    "Usaremos aquí para eliminar algunos símbolos: los números y los paréntesis por ejemplo. No siempre es el caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '', 'nlp', '', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '', 'natural', '', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', '', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '', 'days', 'usually', '.', 'this', 'year', 'is', '', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# digitos\n",
    "tokens = [re.sub(r'\\d+', '',token) for token in tokens]\n",
    "# paréntesis\n",
    "tokens = [re.sub(r'[()]', '',token) for token in tokens]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remueve palabras de longitud menor o igual a tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_4 = []\n",
    "for token in tokens:\n",
    "    if len(token) > 3:\n",
    "        tokens_4.append(token)\n",
    "tokens = tokens_4\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Palabras vacias (stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras varias o stop words son palabras que en el lenguaje común se considera que no aportan al contenido semántico de los textos. En la técnica de bolsa de palabras son omitidos, debido a que causan clasificaciones confusas. En realidad el concepto de palabras vacía depende del contesto de utlización de las técnicas. \n",
    "\n",
    "El siguiente ejemplo muestra el diccionario de plabatras vacías del inglés contenidas en la librería *gensim*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['should', 'does', 'whether', 'a', 'un', 'thru', 'itself', 'using', 'everyone', 'front', 'am', 'say', 'so', 'last', 'whose', 'mine', 'ie', 'always', 'seems', 'yourself', 'amount', 'that', 'make', 'being', 'again', 'my', 'twelve', 'myself', 'show', 'computer', 'put', 'although', 'above', 'fifteen', 'seemed', 'with', 'moreover', 'me', 'yours', 'bill', 'also', 'whole', 'amoungst', 'now', 'other', 'behind', 'couldnt', 'never', 'fill', 'thence', 'first', 'hasnt', 'latterly', 'however', 'is', 'along', 'through', 'three', 'kg', 'who', 'anyone', 'either', 'eleven', 'without', 'do', 'alone', 'call', 'around', 'upon', 'least', 'those', 'give', 'done', 'within', 'six', 'out', 'bottom', 'your', 'to', 'beforehand', 'cry', 'for', 'throughout', 'several', 'thereafter', 'became', 'below', 'via', 'once', 'after', 'well', 'already', 'toward', 'some', 'will', 'where', 'may', 'whom', 'quite', 'sometime', 'into', 'go', 'only', 'they', 'beside', 'two', 'less', 'have', 'none', 'becoming', 'him', 'seem', 'ours', 'ourselves', 'it', 'someone', 'whenever', 'elsewhere', 'towards', 'de', 'serious', 'thereupon', 'much', 'somehow', 'detail', 'many', 'onto', 'really', 'were', 'few', 'among', 'doing', 'its', 'would', 'latter', 'if', 'about', 'co', 'across', 'various', 'per', 'describe', 'yet', 'unless', 'beyond', 'sometimes', 'found', 'even', 'due', 'must', 'herself', 'hereupon', 'noone', 'seeming', 'between', 'whither', 'neither', 'becomes', 'whereafter', 'this', 'formerly', 'third', 'since', 're', 'sixty', 'same', 'regarding', 'further', 'don', 'her', 'thick', 'are', 'most', 'i', 'top', 'though', 'rather', 'she', 'from', 'together', 'whoever', 'before', 'else', 'see', 'whatever', 'themselves', 'part', 'cant', 'didn', 'thus', 'while', 'become', 'down', 'anything', 'eg', 'twenty', 'one', 'often', 'be', 'nine', 'in', 'indeed', 'our', 'very', 'ten', 'anyhow', 'whereas', 'former', 'almost', 'but', 'everywhere', 'except', 'off', 'empty', 'under', 'which', 'others', 'anyway', 'all', 'until', 'back', 'please', 'doesn', 'nor', 'made', 'how', 'fire', 'can', 'somewhere', 'everything', 'every', 'just', 'had', 'on', 'sincere', 'why', 'interest', 'next', 'name', 'meanwhile', 'these', 'therein', 'forty', 'might', 'than', 'he', 'four', 'them', 'afterwards', 'still', 'something', 'km', 'keep', 'nobody', 'wherein', 'move', 'full', 'each', 'mostly', 'you', 'could', 'we', 'then', 'used', 'own', 'take', 'their', 'thin', 'more', 'such', 'there', 'us', 'as', 'eight', 'nothing', 'find', 'ltd', 'otherwise', 'or', 'hers', 'his', 'whereupon', 'no', 'of', 'when', 'system', 'herein', 'ever', 'get', 'been', 'yourselves', 'up', 'inc', 'con', 'perhaps', 'against', 'side', 'during', 'himself', 'an', 'did', 'any', 'wherever', 'amongst', 'and', 'enough', 'therefore', 'hereby', 'by', 'nevertheless', 'over', 'because', 'namely', 'anywhere', 'another', 'the', 'at', 'not', 'hundred', 'fifty', 'besides', 'mill', 'too', 'whence', 'hereafter', 'whereby', 'thereby', 'has', 'nowhere', 'five', 'cannot', 'was', 'etc', 'both', 'hence', 'here', 'what']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "#\n",
    "stop_words_g = []\n",
    "for token  in gensim.parsing.preprocessing.STOPWORDS:\n",
    "    stop_words_g.append(token)\n",
    "print(stop_words_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la librería *nltk* el diciconario de palabras vacías del inglés es actualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', 'should', 'hadn', 's', 'i', 'for', 'each', \"don't\", 'does', 'she', \"doesn't\", 'a', \"didn't\", 'you', 'itself', 'from', 'we', 'then', \"weren't\", 'am', 'below', 'own', \"haven't\", 'their', 'once', 'so', 'before', \"should've\", \"hadn't\", 'after', 'ma', 'more', \"shouldn't\", \"mightn't\", 'haven', 'such', 'themselves', 'there', 'some', 'will', 'didn', 'where', 'while', 'aren', 'as', 'whom', 'yourself', 'down', \"won't\", 've', 'into', 'that', 'only', 'being', 'again', 'be', \"you've\", 'they', 'in', 'or', 'my', 'our', 'hers', 'very', 'll', 'myself', \"wasn't\", 'have', 'him', 'his', 'm', 'but', 'y', 'ours', 'ourselves', 'above', 'off', \"hasn't\", 'it', 'no', 'of', 'when', 'isn', 'mightn', 'with', 'under', 'which', 'all', 'been', 'me', 'yourselves', 'until', 'weren', 'yours', 'up', 'won', 'against', 'were', \"it's\", 'few', 'doesn', 'now', 'its', 'doing', 'during', 'himself', 'an', 'other', 'nor', 'did', 'how', 'having', 'if', 'about', 'any', 'ain', 'shan', 'can', \"needn't\", \"isn't\", 'wasn', \"that'll\", \"shan't\", 'and', 'just', 'hasn', 'by', 'had', 'to', 'on', 'over', 'why', 'is', 'because', 'through', \"aren't\", 'the', 'couldn', 'herself', 'at', 'who', 'not', 'theirs', \"mustn't\", \"she's\", 'these', 'too', 'between', 'do', 'this', 'has', 'shouldn', 'mustn', \"you'll\", 'he', 'than', \"you'd\", 're', \"couldn't\", 'was', 'those', \"you're\", \"wouldn't\", 'same', 'them', 'further', 'don', 'd', 'both', 'needn', 'here', 'wouldn', 'o', 'her', 'out', 'your', 'what', 'are', 't'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#\n",
    "print(stopWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamosa quitar las palabras vacías del ejemplo usando nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_n_e = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in stopWords:\n",
    "        tokens_n_e.append(token)\n",
    "#\n",
    "tokens = tokens_n_e\n",
    "print(tokens)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lematización es el proceso de agrupar las diferentes formas flexionadas de una palabra para que puedan analizarse como un solo elemento. La lematización es similar a la derivación, pero aporta contexto a las palabras. Por lo tanto, vincula palabras con un significado similar a una palabra.\n",
    "\n",
    "El preprocesamiento de texto incluye tanto Stemming como Lemmatization. \n",
    "\n",
    "Muchas veces las personas encuentran confusos estos dos términos. Algunos tratan a estos dos como iguales. \n",
    "\n",
    "En realidad, se prefiere la lematización a la derivación porque la lematización realiza un análisis morfológico de las palabras.\n",
    "\n",
    "Las aplicaciones de la lematización son:\n",
    "\n",
    "- Se utiliza en sistemas de recuperación integrales como motores de búsqueda.\n",
    "- Utilizado en indexación compacta\n",
    "- Ejemplos de lematización:\n",
    "\n",
    "-> rocas: roca\n",
    "-> corpora: corpus\n",
    "-> mejor: bueno\n",
    "\n",
    "Una diferencia importante con la derivación es que lematizar toma una parte del parámetro de voz, \"pos\". Si no se proporciona, el valor predeterminado es \"sustantivo\". En el siguiente ejemplo vamos colocar *pos='a'* que significa adjetivo. Si se coloca *pos ='v'* significa verbo. Por defecto es *pos ='n'*, es decir sustantivo.\n",
    "\n",
    "A continuación se muestra la implementación de lematización de algunas palabras en inglés usando la librería *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que los dos conjuntos de palabara vacías son distintos.\n",
    "\n",
    "Como ejemplo vamos quitar la palabras vacias del objeto text tokenizado definido arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y ahora vamos lematizar el texto de ejemplo, primero con verbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n",
      "\n",
      "\n",
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concern', 'program', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "# verbs\n",
    "lemma_text =[]\n",
    "for token in tokens:\n",
    "    lemma_text.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "\n",
    "print(tokens)\n",
    "print('\\n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'process', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concern', 'interaction', 'computer', 'human', 'natural', 'language', 'particular', 'concern', 'program', 'computer', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpus', 'challenge', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'formal', 'machine-readable', 'logical', 'form', 'connect', 'language', 'machine', 'perception', 'manage', 'human-computer', 'dialog', 'system', 'combination', 'thereof', 'day', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "# nouns\n",
    "for i in range(len(lemma_text )):\n",
    "    lemma_text[i] = WordNetLemmatizer().lemmatize(lemma_text[i], pos='n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steeming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La derivación (steeming) es el proceso de producir variantes morfológicas de una palabra raíz / base. Los programas de derivación se conocen comúnmente como algoritmos de steeming o derivaciones. Un algoritmo de stemming reduce las palabras \"chocolates\", \"chocolates\", \"choco\" a la raíz de la palabra, \"chocolate\" y \"recuperación\", \"recuperado\", \"recupera\" se reduce a la raíz \"recuperar\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errores en la derivación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay principalmente dos errores en la derivación: la derivación excesiva y la derivación insuficiente. \n",
    "\n",
    "El sobre-recorte excesivo ocurre cuando dos palabras se derivan de la misma raíz que tienen raíces diferentes. \n",
    "\n",
    "El  sub-recorte ocurre cuando dos palabras se derivan de la misma raíz pero tienen raíces diferentes.\n",
    "\n",
    "Como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'field', 'comput', 'scienc', 'artifici', 'intellig', 'comput', 'linguist', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpu', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'gener', 'frequent', 'formal', 'machine-read', 'logic', 'form', 'connect', 'languag', 'machin', 'percept', 'manag', 'human-comput', 'dialog', 'system', 'combin', 'thereof', 'day', 'usual', 'year']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "# crea una instancia de PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(lemma_text)):\n",
    "    lemma_text[i] = ps.stem(lemma_text[i])\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomado de [Wikipedia](https://es.wikipedia.org/wiki/Tf-idf).\n",
    "\n",
    "Tf-idf (del inglés Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en el corpus de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en un corpus. Esta medida se utiliza a menudo como un factor de ponderación en la recuperación de información y la minería de textos. \n",
    "\n",
    "\n",
    "El valor tf-idf aumenta proporcionalmente al número de veces que una palabra aparece en el documento, pero es compensada por la frecuencia de la palabra en el corpus de documentos, lo que permite manejar el hecho de que algunas palabras son generalmente más comunes que otras.\n",
    "\n",
    "Variaciones del esquema de peso tf-idf son empleadas frecuentemente por los motores de búsqueda como herramienta fundamental para medir la relevancia de un documento dada una consulta del usuario, estableciendo así una ordenación o ranking de los mismos. \n",
    "\n",
    "\n",
    "Tf-idf puede utilizarse exitosamente para el filtrado de las palabras vacías (stop-words), en diferentes campos del pre-procesamiento de textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalles matemáticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf es el producto de dos medidas, *frecuencia de término* y *frecuencia inversa de documento*. Existen varias maneras de determinar el valor de ambas. \n",
    "\n",
    "En el caso de la frecuencia de término $\\text{tf}(t, d)$, la opción más sencilla es usar la frecuencia bruta del término $t$ en el documento $d$, o sea, el número de veces que el término $t$ ocurre en el documento $d$. Si denotamos la frecuencia bruta de $t$ por $f(t,d)$, entonces el esquema $\\text{tf}$ simple es $\\text{tf}(t, d) = f(t,d)$. \n",
    "\n",
    "\n",
    "Otras posibilidades son:\n",
    "\n",
    "- *frecuencias\" booleanas*: tf(t,d) = 1 si t ocurre en d, y 0 si no;\n",
    "- *frecuencia escalada logarítmicamente*: tf(t,d) = 1 + log f(t,d) (y 0 si f(t,d)=0);\n",
    "- *frecuencia normalizada*, para evitar una predisposición hacia los documentos largos. Por ejemplo, se divide la frecuencia bruta por la frecuencia máxima de algún término en el documento:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {\\mathrm {f} (t,d)}{\\max\\{\\mathrm {f} (t,d):t\\in d\\}}}}\n",
    "$$\n",
    "\n",
    "La frecuencia inversa de documento es una medida de si el término es común o no, en el corpus de documentos. Se obtiene dividiendo el número total de documentos por el número de documentos que contienen el término, y se toma el logaritmo de ese cociente:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "- ${\\displaystyle |D|}$: cardinalidad de $D$, o número de documentos en el corpus.\n",
    "- ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$ : número de documentos donde aparece el término $t$. Si el término no está en la colección se producirá una división-por-cero. Por lo tanto, es común ajustar esta fórmula a ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$.\n",
    "\n",
    "Matemáticamente, la base de la función logaritmo no es importante y constituye un factor constante en el resultado final.\n",
    "\n",
    "Luego, *tf-idf* se calcula como:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf-idf} (t,d,D)=\\mathrm {tf} (t,d)\\times \\mathrm {idf} (t,D)}\n",
    "$$\n",
    "\n",
    "Un peso alto en *tf-idf* se alcanza con una elevada frecuencia de término (en el documento dado) y una pequeña frecuencia de ocurrencia del término en corpus de documentos. \n",
    "\n",
    "Como el cociente dentro de la función logaritmo del idf es siempre mayor o igual que 1, el valor del *idf* (y del *tf-idf*) es mayor o igual que 0. \n",
    "\n",
    "Cuando un término aparece en muchos documentos, el cociente dentro del logaritmo se acerca a 1, ofreciendo un valor de *idf* y de *tf-idf* cercano a 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semántica latente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta técnica es quizas de las primeras aparecidas en el anális de textos. La idea central es la construcción de análisis de componentes principales (ACP) seguida de un proceso de clasificación automática.\n",
    "\n",
    "Las componentes principales del ACP, que son construidas a partir de combinaciones lineales de las columnas de los términos se denominan las **componentes léxicas del corpus de datos**. \n",
    "\n",
    "Las herramientas habituales para la interpretación del ACP permiten determinar o mejor asignar un contenido semántico a cada componente. \n",
    "\n",
    "En consecuencia, es posible determinar las temáticas presentes en el corpus de textos, a partir de los ejes semánticos.\n",
    "\n",
    "\n",
    "Como es habitual en el ACP, una clasificación automática puede ser obtenida a partir de la representación factorial de la dtm.\n",
    "\n",
    "El siguente gráfico ilustra la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/pca.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Arquitectura del modelo Semática Latente</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente lo que se hace es una proyección lineal desde el espacio vectores dispersos al espacio Euclideano. Modernamente se ha encontrado que tiene bastante error, básicamente por el tratamiento lineal. \n",
    "\n",
    "En general, se ha encontrado que estas técnicas permiten un primer acercamiento al descubrimieontos de las tématicas (tópicos), pero que en general se quedan cortas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos generativos: Latent Dirichlet Allocation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica Latent Dirichlet Allocation (LDA) es la más utilizada actualmente para la extracción de toṕicos de corpus de documentos y se debe a [Blei et al](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las ideas centrales detrás de LDA, Blei et al.(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ideas centrales detrás de LDA son las siguientes. El modelo generativo supone que los documentos son gnerados como sigue:\n",
    "\n",
    "1. El tamaño $N$ del documento es generado por una distribución de Poisson $\\text{Poi}(\\xi)$.\n",
    "2. Los tópicos son generados a partir de una distribución multinomial con vector de probabilidades $\\mathbf{\\theta}$. \n",
    "3. A priori se asume que que el vector $\\mathbf{\\theta}$ es generado por una distribución de Dirichlet con vector de parámetros $\\boldsymbol{\\alpha}$. De aquí deriva el nombre de la técnica.\n",
    "4. Cada una de las $N$ palabras en un documentos es generada según el siguiente algoritmo.\n",
    "     - Se escoge un tópico $z_n \\sim \\text{Multinomial}(\\mathbf{\\theta})$.\n",
    "     - Se escoge la palabra $w_n \\sim \\text{P}(w_n|z_n,\\mathbf{\\beta})$. En donde $\\mathbf{\\beta}$ es una matriz de probabilidades de pertenencia de las palabras a los tópicos. $P$ es una probabilidad multinomial condicionada al tópico $z_n$ y al vector de parámetros $\\mathbf{\\beta}$.\n",
    "\n",
    "\n",
    "Al lector interesado en los detalles, lo remitimos al paper original de [Blei et al.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente imagen intenta mostrar las ideas centrales detras  de la técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Diagram_Blei.png\" width=\"800\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Intuición detrás de LDA</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "[Intuition behind LDA](http://www.cs.cornell.edu/courses/cs6784/2010sp/lecture/30-BleiEtAl03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelado de temas (topic modeling) es un tipo de modelado estadístico para descubrir los “temas” abstractos que ocurren en una colección de documentos. La asignación de Dirichlet latente (LDA) es un ejemplo de modelo de tema y se utiliza para clasificar el texto de un documento en un tema en particular. \n",
    "\n",
    "Construye un modelo de tema por documento y palabras por modelo de tema, modelado como distribuciones de Dirichlet.\n",
    "\n",
    "Aquí vamos a aplicar LDA a un conjunto de documentos y dividirlos en temas. ¡Empecemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alvaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a escribir una función que lematiza y hace el preprocesamintos del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokeniza el texto\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos que usaremos es una lista de más de un millón de titulares de noticias publicados durante un período de 15 años y se puede descargar de [Kaggle](https://www.kaggle.com/therohk/million-headlines/metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo adaptado de [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../Datos/abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos a procesar previamente los textos, guardando los resultados en el objeto *processed_docs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bolsa de palabras del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un diccionario a partir de *procesados_docs* que contenga la cantidad de veces que aparece una palabra en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra los tokens que aparecen en\n",
    "menos de 15 documentos (número absoluto) o\n",
    "más de 0,5 documentos (fracción del tamaño total del corpus, no número absoluto).\n",
    "después de los dos pasos anteriores, conserve solo los primeros 100000 tokens más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada documento creamos un diccionario que informa cuántos\n",
    "palabras y cuántas veces aparecen esas palabras. \n",
    "\n",
    "Colocamos esto en el objeto *bow_corpus*, luego verifique nuestro documento seleccionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76, 1), (112, 1), (483, 1), (4015, 1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta es una vista preliminar de la bolsa de palabras del documento preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 76 (\"bushfir\") appears 1 time.\n",
      "Word 112 (\"help\") appears 1 time.\n",
      "Word 483 (\"rain\") appears 1 time.\n",
      "Word 4015 (\"dampen\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.\n",
    "\n",
    "Creamos un objeto modelo *tf-idf* usando *models.TfidfModel* a partir de  \"bow_corpus\" y lo colocamos en *tfidf*, luego aplicamos la transformación a todo el corpus y lo llámamos *corpus_tfidf*. Finalmente, obtenemos una vista previa de las puntuaciones *TF-IDF* para nuestro primer documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5913144510648105),\n",
      " (1, 0.38522584492986706),\n",
      " (2, 0.49651004561935946),\n",
      " (3, 0.5053969162540006)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo LDA usando la bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrener anuestro modelo LDA usando *gensim.models.LdaMulticore* and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para cada tópico, exploraremos las palabras que ocurren en ese tema y su peso relativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.031*\"queensland\" + 0.024*\"perth\" + 0.024*\"market\" + 0.020*\"jail\" + 0.020*\"hospit\" + 0.016*\"share\" + 0.016*\"power\" + 0.015*\"year\" + 0.012*\"bank\" + 0.012*\"talk\"\n",
      "Topic: 1 \n",
      "Words: 0.054*\"australia\" + 0.021*\"countri\" + 0.020*\"rural\" + 0.018*\"hour\" + 0.017*\"live\" + 0.015*\"west\" + 0.015*\"nation\" + 0.012*\"busi\" + 0.012*\"peopl\" + 0.011*\"farmer\"\n",
      "Topic: 2 \n",
      "Words: 0.058*\"polic\" + 0.026*\"death\" + 0.024*\"sydney\" + 0.022*\"attack\" + 0.021*\"crash\" + 0.021*\"woman\" + 0.015*\"shoot\" + 0.014*\"arrest\" + 0.014*\"investig\" + 0.013*\"die\"\n",
      "Topic: 3 \n",
      "Words: 0.030*\"south\" + 0.022*\"north\" + 0.019*\"tasmania\" + 0.014*\"kill\" + 0.014*\"park\" + 0.014*\"protest\" + 0.013*\"close\" + 0.011*\"leav\" + 0.011*\"build\" + 0.010*\"program\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"govern\" + 0.018*\"council\" + 0.018*\"say\" + 0.016*\"plan\" + 0.013*\"rise\" + 0.013*\"water\" + 0.012*\"concern\" + 0.011*\"industri\" + 0.011*\"need\" + 0.009*\"resid\"\n",
      "Topic: 5 \n",
      "Words: 0.021*\"adelaid\" + 0.019*\"open\" + 0.017*\"world\" + 0.016*\"melbourn\" + 0.016*\"final\" + 0.016*\"women\" + 0.015*\"brisban\" + 0.013*\"australian\" + 0.012*\"gold\" + 0.011*\"coast\"\n",
      "Topic: 6 \n",
      "Words: 0.020*\"school\" + 0.018*\"interview\" + 0.014*\"labor\" + 0.014*\"indigen\" + 0.013*\"fund\" + 0.013*\"children\" + 0.013*\"life\" + 0.012*\"student\" + 0.012*\"help\" + 0.012*\"miss\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"court\" + 0.026*\"charg\" + 0.024*\"murder\" + 0.023*\"face\" + 0.017*\"accus\" + 0.016*\"child\" + 0.016*\"trial\" + 0.014*\"high\" + 0.013*\"abus\" + 0.013*\"donald\"\n",
      "Topic: 8 \n",
      "Words: 0.027*\"trump\" + 0.013*\"record\" + 0.012*\"time\" + 0.012*\"break\" + 0.012*\"leagu\" + 0.012*\"fall\" + 0.010*\"news\" + 0.009*\"show\" + 0.009*\"australian\" + 0.009*\"season\"\n",
      "Topic: 9 \n",
      "Words: 0.030*\"elect\" + 0.024*\"hous\" + 0.023*\"canberra\" + 0.016*\"chang\" + 0.016*\"price\" + 0.014*\"lose\" + 0.014*\"health\" + 0.013*\"farm\" + 0.012*\"group\" + 0.011*\"public\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es posible distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corriendo  LDA usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"john\" + 0.007*\"hunter\" + 0.007*\"friday\" + 0.006*\"septemb\" + 0.006*\"project\" + 0.005*\"coal\" + 0.005*\"stori\" + 0.005*\"outback\" + 0.005*\"jam\" + 0.005*\"council\"\n",
      "Topic: 1 Word: 0.022*\"countri\" + 0.021*\"hour\" + 0.017*\"interview\" + 0.012*\"podcast\" + 0.010*\"gold\" + 0.010*\"australia\" + 0.009*\"coast\" + 0.008*\"sport\" + 0.006*\"monday\" + 0.006*\"juli\"\n",
      "Topic: 2 Word: 0.013*\"kill\" + 0.011*\"crash\" + 0.009*\"dead\" + 0.008*\"attack\" + 0.008*\"donald\" + 0.006*\"bomb\" + 0.006*\"wall\" + 0.006*\"street\" + 0.006*\"toni\" + 0.006*\"sexual\"\n",
      "Topic: 3 Word: 0.013*\"south\" + 0.012*\"leagu\" + 0.011*\"west\" + 0.008*\"climat\" + 0.008*\"june\" + 0.008*\"rugbi\" + 0.008*\"liber\" + 0.007*\"peter\" + 0.007*\"syria\" + 0.006*\"australia\"\n",
      "Topic: 4 Word: 0.020*\"charg\" + 0.020*\"polic\" + 0.017*\"murder\" + 0.013*\"woman\" + 0.013*\"court\" + 0.011*\"jail\" + 0.010*\"death\" + 0.009*\"assault\" + 0.009*\"arrest\" + 0.009*\"drug\"\n",
      "Topic: 5 Word: 0.012*\"govern\" + 0.009*\"health\" + 0.007*\"elect\" + 0.006*\"say\" + 0.006*\"labor\" + 0.006*\"indigen\" + 0.006*\"fund\" + 0.006*\"school\" + 0.005*\"chang\" + 0.005*\"violenc\"\n",
      "Topic: 6 Word: 0.018*\"trump\" + 0.013*\"market\" + 0.011*\"share\" + 0.011*\"drum\" + 0.007*\"david\" + 0.007*\"australian\" + 0.007*\"dollar\" + 0.006*\"tuesday\" + 0.006*\"thursday\" + 0.005*\"season\"\n",
      "Topic: 7 Word: 0.026*\"rural\" + 0.018*\"news\" + 0.011*\"turnbul\" + 0.010*\"royal\" + 0.009*\"plead\" + 0.009*\"nation\" + 0.008*\"busi\" + 0.008*\"octob\" + 0.007*\"victorian\" + 0.007*\"commiss\"\n",
      "Topic: 8 Word: 0.009*\"weather\" + 0.008*\"flood\" + 0.007*\"hill\" + 0.007*\"farmer\" + 0.006*\"rain\" + 0.006*\"michael\" + 0.006*\"dairi\" + 0.006*\"price\" + 0.006*\"queensland\" + 0.006*\"wednesday\"\n",
      "Topic: 9 Word: 0.007*\"retir\" + 0.007*\"energi\" + 0.007*\"abus\" + 0.005*\"spring\" + 0.005*\"hors\" + 0.005*\"marriag\" + 0.005*\"child\" + 0.005*\"renew\" + 0.005*\"alic\" + 0.005*\"product\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, ¿podemos distinguir diferentes temas usando las palabras en cada tema y sus pesos correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA de bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobaremos dónde se clasificaría nuestro documento de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.22015565633773804\t \n",
      "Topic: 0.020*\"school\" + 0.018*\"interview\" + 0.014*\"labor\" + 0.014*\"indigen\" + 0.013*\"fund\" + 0.013*\"children\" + 0.013*\"life\" + 0.012*\"student\" + 0.012*\"help\" + 0.012*\"miss\"\n",
      "\n",
      "Score: 0.22014997899532318\t \n",
      "Topic: 0.021*\"adelaid\" + 0.019*\"open\" + 0.017*\"world\" + 0.016*\"melbourn\" + 0.016*\"final\" + 0.016*\"women\" + 0.015*\"brisban\" + 0.013*\"australian\" + 0.012*\"gold\" + 0.011*\"coast\"\n",
      "\n",
      "Score: 0.22013133764266968\t \n",
      "Topic: 0.030*\"south\" + 0.022*\"north\" + 0.019*\"tasmania\" + 0.014*\"kill\" + 0.014*\"park\" + 0.014*\"protest\" + 0.013*\"close\" + 0.011*\"leav\" + 0.011*\"build\" + 0.010*\"program\"\n",
      "\n",
      "Score: 0.2194562405347824\t \n",
      "Topic: 0.035*\"court\" + 0.026*\"charg\" + 0.024*\"murder\" + 0.023*\"face\" + 0.017*\"accus\" + 0.016*\"child\" + 0.016*\"trial\" + 0.014*\"high\" + 0.013*\"abus\" + 0.013*\"donald\"\n",
      "\n",
      "Score: 0.02002192847430706\t \n",
      "Topic: 0.054*\"australia\" + 0.021*\"countri\" + 0.020*\"rural\" + 0.018*\"hour\" + 0.017*\"live\" + 0.015*\"west\" + 0.015*\"nation\" + 0.012*\"busi\" + 0.012*\"peopl\" + 0.011*\"farmer\"\n",
      "\n",
      "Score: 0.020018991082906723\t \n",
      "Topic: 0.030*\"elect\" + 0.024*\"hous\" + 0.023*\"canberra\" + 0.016*\"chang\" + 0.016*\"price\" + 0.014*\"lose\" + 0.014*\"health\" + 0.013*\"farm\" + 0.012*\"group\" + 0.011*\"public\"\n",
      "\n",
      "Score: 0.020018139854073524\t \n",
      "Topic: 0.027*\"trump\" + 0.013*\"record\" + 0.012*\"time\" + 0.012*\"break\" + 0.012*\"leagu\" + 0.012*\"fall\" + 0.010*\"news\" + 0.009*\"show\" + 0.009*\"australian\" + 0.009*\"season\"\n",
      "\n",
      "Score: 0.020015902817249298\t \n",
      "Topic: 0.031*\"queensland\" + 0.024*\"perth\" + 0.024*\"market\" + 0.020*\"jail\" + 0.020*\"hospit\" + 0.016*\"share\" + 0.016*\"power\" + 0.015*\"year\" + 0.012*\"bank\" + 0.012*\"talk\"\n",
      "\n",
      "Score: 0.020015902817249298\t \n",
      "Topic: 0.058*\"polic\" + 0.026*\"death\" + 0.024*\"sydney\" + 0.022*\"attack\" + 0.021*\"crash\" + 0.021*\"woman\" + 0.015*\"shoot\" + 0.014*\"arrest\" + 0.014*\"investig\" + 0.013*\"die\"\n",
      "\n",
      "Score: 0.020015902817249298\t \n",
      "Topic: 0.030*\"govern\" + 0.018*\"council\" + 0.018*\"say\" + 0.016*\"plan\" + 0.013*\"rise\" + 0.013*\"water\" + 0.012*\"concern\" + 0.011*\"industri\" + 0.011*\"need\" + 0.009*\"resid\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del desempeño clasificando el documento de muestra usando el modelo LDA TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5712891817092896\t \n",
      "Topic: 0.009*\"weather\" + 0.008*\"flood\" + 0.007*\"hill\" + 0.007*\"farmer\" + 0.006*\"rain\" + 0.006*\"michael\" + 0.006*\"dairi\" + 0.006*\"price\" + 0.006*\"queensland\" + 0.006*\"wednesday\"\n",
      "\n",
      "Score: 0.2686343789100647\t \n",
      "Topic: 0.020*\"charg\" + 0.020*\"polic\" + 0.017*\"murder\" + 0.013*\"woman\" + 0.013*\"court\" + 0.011*\"jail\" + 0.010*\"death\" + 0.009*\"assault\" + 0.009*\"arrest\" + 0.009*\"drug\"\n",
      "\n",
      "Score: 0.020011145621538162\t \n",
      "Topic: 0.012*\"govern\" + 0.009*\"health\" + 0.007*\"elect\" + 0.006*\"say\" + 0.006*\"labor\" + 0.006*\"indigen\" + 0.006*\"fund\" + 0.006*\"school\" + 0.005*\"chang\" + 0.005*\"violenc\"\n",
      "\n",
      "Score: 0.02001083269715309\t \n",
      "Topic: 0.008*\"john\" + 0.007*\"hunter\" + 0.007*\"friday\" + 0.006*\"septemb\" + 0.006*\"project\" + 0.005*\"coal\" + 0.005*\"stori\" + 0.005*\"outback\" + 0.005*\"jam\" + 0.005*\"council\"\n",
      "\n",
      "Score: 0.020009344443678856\t \n",
      "Topic: 0.026*\"rural\" + 0.018*\"news\" + 0.011*\"turnbul\" + 0.010*\"royal\" + 0.009*\"plead\" + 0.009*\"nation\" + 0.008*\"busi\" + 0.008*\"octob\" + 0.007*\"victorian\" + 0.007*\"commiss\"\n",
      "\n",
      "Score: 0.02000928483903408\t \n",
      "Topic: 0.007*\"retir\" + 0.007*\"energi\" + 0.007*\"abus\" + 0.005*\"spring\" + 0.005*\"hors\" + 0.005*\"marriag\" + 0.005*\"child\" + 0.005*\"renew\" + 0.005*\"alic\" + 0.005*\"product\"\n",
      "\n",
      "Score: 0.020009148865938187\t \n",
      "Topic: 0.018*\"trump\" + 0.013*\"market\" + 0.011*\"share\" + 0.011*\"drum\" + 0.007*\"david\" + 0.007*\"australian\" + 0.007*\"dollar\" + 0.006*\"tuesday\" + 0.006*\"thursday\" + 0.005*\"season\"\n",
      "\n",
      "Score: 0.02000902220606804\t \n",
      "Topic: 0.022*\"countri\" + 0.021*\"hour\" + 0.017*\"interview\" + 0.012*\"podcast\" + 0.010*\"gold\" + 0.010*\"australia\" + 0.009*\"coast\" + 0.008*\"sport\" + 0.006*\"monday\" + 0.006*\"juli\"\n",
      "\n",
      "Score: 0.020008884370326996\t \n",
      "Topic: 0.013*\"south\" + 0.012*\"leagu\" + 0.011*\"west\" + 0.008*\"climat\" + 0.008*\"june\" + 0.008*\"rugbi\" + 0.008*\"liber\" + 0.007*\"peter\" + 0.007*\"syria\" + 0.006*\"australia\"\n",
      "\n",
      "Score: 0.02000882849097252\t \n",
      "Topic: 0.013*\"kill\" + 0.011*\"crash\" + 0.009*\"dead\" + 0.008*\"attack\" + 0.008*\"donald\" + 0.006*\"bomb\" + 0.006*\"wall\" + 0.006*\"street\" + 0.006*\"toni\" + 0.006*\"sexual\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo con un documento no visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8499734401702881\t Topic: 0.031*\"queensland\" + 0.024*\"perth\" + 0.024*\"market\" + 0.020*\"jail\" + 0.020*\"hospit\"\n",
      "Score: 0.01667023077607155\t Topic: 0.030*\"south\" + 0.022*\"north\" + 0.019*\"tasmania\" + 0.014*\"kill\" + 0.014*\"park\"\n",
      "Score: 0.01667018234729767\t Topic: 0.054*\"australia\" + 0.021*\"countri\" + 0.020*\"rural\" + 0.018*\"hour\" + 0.017*\"live\"\n",
      "Score: 0.01667017489671707\t Topic: 0.030*\"elect\" + 0.024*\"hous\" + 0.023*\"canberra\" + 0.016*\"chang\" + 0.016*\"price\"\n",
      "Score: 0.01667015254497528\t Topic: 0.030*\"govern\" + 0.018*\"council\" + 0.018*\"say\" + 0.016*\"plan\" + 0.013*\"rise\"\n",
      "Score: 0.016669869422912598\t Topic: 0.027*\"trump\" + 0.013*\"record\" + 0.012*\"time\" + 0.012*\"break\" + 0.012*\"leagu\"\n",
      "Score: 0.016669215634465218\t Topic: 0.020*\"school\" + 0.018*\"interview\" + 0.014*\"labor\" + 0.014*\"indigen\" + 0.013*\"fund\"\n",
      "Score: 0.016668912023305893\t Topic: 0.058*\"polic\" + 0.026*\"death\" + 0.024*\"sydney\" + 0.022*\"attack\" + 0.021*\"crash\"\n",
      "Score: 0.016668912023305893\t Topic: 0.021*\"adelaid\" + 0.019*\"open\" + 0.017*\"world\" + 0.016*\"melbourn\" + 0.016*\"final\"\n",
      "Score: 0.016668912023305893\t Topic: 0.035*\"court\" + 0.026*\"charg\" + 0.024*\"murder\" + 0.023*\"face\" + 0.017*\"accus\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Regtresar al inicio](Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
