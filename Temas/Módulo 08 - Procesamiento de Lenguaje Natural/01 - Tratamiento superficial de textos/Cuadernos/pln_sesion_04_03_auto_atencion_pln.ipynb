{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Mecanismos de auto-atención</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/child_engaging_in_joint_attention.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: <a href=\"https://commons.wikimedia.org/wiki/File:Illustration_of_caregiver_and_child_engaging_in_joint_attention.jpg\">Rita.obeid6</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Illustrated Auto-attención](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "1. [Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)\n",
    "1. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "1. [Effective Approaches to Attention-based Neural Machine Translation (Luong et. al, 2015)](https://arxiv.org/pdf/1508.04025.pdf)\n",
    "1. [Attention Is All You Need (Vaswani et. al, 2017)](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "1. [Self-Attention GAN (Zhang et. al, 2018)](https://arxiv.org/pdf/1805.08318.pdf)\n",
    "1. [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "1. [TensorFlow’s seq2seq Tutorial with Attention (Tutorial on seq2seq+attention)](https://github.com/tensorflow/nmt)\n",
    "1. [Lilian Weng’s Blog on Attention (Great start to attention)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)\n",
    "1. [Jay Alammar’s Blog on Seq2Seq with Attention (Great illustrations and worked example on seq2seq+attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "1. [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Modelo seq2seq](#Modelo-seq2seq)\n",
    "* [Auto-atención](#Auto-atención)\n",
    "    * [Paso 1. Preparar las entradas](#Paso-1.-Preparar-las-entradas)\n",
    "    * [Paso 2. Inicializar los pesos](#Paso-2.-Inicializar-los-pesos)\n",
    "    * [Paso 3. Obtener claves, consultas y valores](#Paso-3.-Obtener-claves,-consultas-y-valores)\n",
    "    * [Paso 4. Cálculo de los puntajes de auto-atención para las entradas](#Paso-4.-Cálculo-de-los-puntajes-de-auto-atención-para-las-entradas)\n",
    "    * [Paso 5. Cálculo del puntaje softmax](#Paso-5.-Cálculo-del-puntaje-softmax)\n",
    "    * [Paso 6. Multiplica los puntajes softmax con los valores para cada consulta](#Paso-6.-Multiplica-los-puntajes-softmax-con-los-valores-para-cada-consulta)\n",
    "    * [Paso 7. Suma pesada de valores para conseguir los vectores de contexto](#Paso-7.-Suma-pesada-de-valores-para-conseguir-los-vectores-de-contexto)\n",
    "    * [Ejemplo de implementación en Python](#Ejemplo-de-implementación-en-Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tienen en común BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT y CamemBERT?. NO es propiamente BERT.\n",
    "\n",
    "Respuesta: auto-atención (`self-attention`)🤗. No solo estamos hablando de arquitecturas que llevan el nombre \"BERT\", sino más correctamente de arquitecturas basadas en transformers. \n",
    "\n",
    "Las arquitecturas basadas en transformers que se utilizan principalmente en el modelado de tareas de comprensión del lenguaje, evitan el uso de la recurrencia en la red neuronal y, en cambio, confían por completo en los mecanismos de auto-atención para generar dependencias globales entre entradas y salidas. \n",
    "\n",
    "\n",
    "La auto-atención es similar a la atención. Comparten fundamentalmente el mismo concepto y muchas operaciones matemáticas comunes.  Un módulo de auto-atención toma $n$ entradas y devuelve $n$ salidas. ¿Qué pasa en este módulo? En términos sencillos, el mecanismo de auto atención permite que las entradas interactúen entre sí (`auto`) y descubran a quién deben prestar más atención (`atención`). Los resultados son agregados de estas interacciones y puntuaciones de atención.\n",
    "\n",
    "\n",
    "En esta lección  revisamos los detalles de construcción de los mecanismos de atención.\n",
    "\n",
    "Supongamos que tenemos un secuencia con tres entradas $e_1, e_2$ y $e_3$. Un mecanismo de atención siguen el siguiente algoritmo.\n",
    "\n",
    "\n",
    "\n",
    "1. Prepara las entradas\n",
    "1. Inicializa los pesos\n",
    "1. Obtiene `clave` (key), `consulta` (query) y `valor` (value) a aprtir de las entradas.\n",
    "1. Calcula las puntuaciones de atención para la entrada $e_1$.\n",
    "1. Calcula el softmax de las puntuaciones.\n",
    "1. Multiplica las puntuaciones softmax con los `valores`\n",
    "1. Sumar `valores ponderados` para obtener la Salida 1\n",
    "1. Repite los pasos 4 a 7 para la entrada $e_2$ y la entrada $e_3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Auto-atención</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 1. Preparar las entradas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas corresponden a una secuencia de palabras sumergidas (embebidas) en un sumergimiento (embbeding) el cual vamos a suponer de tamaño 4. Para el ejemplo supondremos que la secuencia consta de 3 palabras. Entonces tenemos 3 entradas de tamaño 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_1.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Entradas.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo tenemos tres entradas.\n",
    "\n",
    "1. entrada 1: $e_1=[1, 0, 1, 0]$\n",
    "1. entrada 2: $e_2=[0, 2, 0, 2]$\n",
    "1. entrada 3: $e_3=[1, 1, 1, 1]$\n",
    "\n",
    "En notación matricial escribimos\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "1 & 0 & 1 & 0\\\\\n",
    "0 & 2 & 0 & 2\\\\\n",
    "1 & 1 & 1 &1\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "e_1\\\\\n",
    "e_2\\\\\n",
    "e_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 2. Inicializar los pesos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada una de las entradas se transforman en tres diferentes vectores de algún tamaño. Para ello se construyen tres transformaciones lineales (o afines), digamos $W_k$, $W_q$ y $W_v$. Esta matrices son parámetros de la red neuronal que son inicializados aleatoriamente, o con algún procedimiento estándar  para  inicializar  los  pesos.\n",
    "\n",
    "Para esta ilustración, supondremos que las matrices de pesos son dadas por\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "W_k = \n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_v = \n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 3. Obtener claves, consultas y valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso obtenemos los tres tipos de objetos derivados de la entrada: claves, consultas y valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claves (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las claves se obtienen de la siguiente manera:\n",
    "\n",
    "$$\n",
    "K = E \\times W_k = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} \n",
    "0 & 1 &1\\\\\n",
    "4 & 4 &0 \\\\\n",
    "2 & 3 &1 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "k_1\\\\\n",
    "k_2\\\\\n",
    "k_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores se obtiene como\n",
    "\n",
    "$$\n",
    "V = E \\times W_v = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 2 &3\\\\\n",
    "2 & 8 &0 \\\\\n",
    "2 & 6 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "v_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consultas (queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente las consultas resultantes son\n",
    "\n",
    "$$\n",
    "Q = E \\times W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "q_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_2.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Obtención de claves consultas y valores.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 4. Cálculo de los puntajes de auto-atención para las entradas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustramos aquí como se calculan los puntajes de auto-atención para la entrada 1. Los puntajes de auto-atención para las demás entradas se calculan de la misma manera con el cambio obvio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_3.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Obtención de puntajes de auto-atención para la primera consulta.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se muestra en la anterior ilustración, se toma la consulta obtenida para la primera entrada, que en el ejemplo es $e_1 =[1, 0, 2]$. Los pesos de auto-atención son una medida de similaridad, entre la consulta y cada una de las claves, como se ha estudiado en las lecciones atención. La función de auto-atención se denotará $a$ . En este caso el resultado se obtiene haciendo el producto escalar (producto punto) entre la consulta asociada  a $e_1$, es decir, $q_1$ y cada una de las claves, es decir $a(q_1, c_i)= <q_1, c_i>, i =1,2,3$.\n",
    "\n",
    "Escrito en forma matricial, los pesos de auto-atención para la entrada 1 se calculan como\n",
    "\n",
    "\n",
    "$$\n",
    "p_1 = q_1 \\times K^T = [1, 0, 2]\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix} =[2, 4, 4].\n",
    "$$\n",
    "\n",
    "Los pesos de auto-atención completos, es decir para todas las  consultas  se calculan mediante\n",
    "\n",
    "$$\n",
    "P = Q \\times  K^T =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "2 & 4 & 4 \\\\\n",
    "4 & 16 & 12 \\\\\n",
    "4 & 12 & 10 \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "p_1\\\\\n",
    "p_2\\\\\n",
    "p_3\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 5. Cálculo del puntaje softmax</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos los puntajes a la escala softmax. Por ejemplo softmax([2, 4, 4]) = [0.06, 0.47, 0.47]. Observe que softmax define una distribución discreta de probabilidad. Ele cálculo completo para el ejemplo es\n",
    "\n",
    "\n",
    "$$\n",
    "S = \\begin{pmatrix}\n",
    "\\text{softmax}(p_1)\\\\\n",
    "\\text{softmax}(p_2)\\\\\n",
    "\\text{softmax}(p_3)\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "0.06 & 0.47 &0.47\\\\\n",
    "0.00 & 0.98 &0.02 \\\\\n",
    "0.00 & 0.88 &0.12 \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "s_1\\\\\n",
    "s_2\\\\\n",
    "s_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 6. Multiplica los puntajes softmax con los valores para cada consulta</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la consulta 1 se tiene\n",
    "* 0.06 * [1, 2, 3] = [0.06, 0.13, 0.19]\n",
    "* 0.47 * [2, 8, 0] = [0.94, 3.75, 0.00]\n",
    "* 0.47 * [2, 6, 3] = [0.94, 2.81, 1.40]\n",
    "\n",
    "Para la consulta 2\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.98 * [2, 8, 0] = [1.96, 7.86, 0.00]\n",
    "* 0.02 * [2, 6, 3] = [0.04, 0.11, 0.05]\n",
    "\n",
    "Para la consulta 3\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.88 * [2, 8, 0] = [1.76, 7.04, 0.00]\n",
    "* 0.12 * [2, 6, 3] = [0.24, 0.72, 0.36]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 7. Suma pesada de valores para conseguir los vectores de contexto</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida (vector de contexto) se obtiene como la suma de los vectores pesados asociados a cada consulta así:\n",
    "\n",
    "* salida 1: [0.06, 0.13, 0.19] + [0.94, 3.75, 0.00] + [0.94, 2.81, 1.40] = [1.94, 6.68, 1.60]\n",
    "* salida 2: [0.00, 0.00, 0.00] + [1.96, 7.86, 0.00] + [0.04, 0.11, 0.05] = [2.00, 7.96, 0.05]\n",
    "* salida 3: [0.00, 0.00, 0.00] + [1.76, 7.04, 0.00] + [0.24, 0.72, 0.36] = [2.00, 7.76, 0.36]\n",
    "\n",
    "La siguiente imagen muestra el modelo completo de auto-atención. Los valores se redondearon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_4.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atención: Cálculo completo de las últimas dos salidas</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de implementación en Python</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E=\n",
      "  [[1 0 1 0]\n",
      " [0 2 0 2]\n",
      " [1 1 1 1]]\n",
      "W_k\n",
      " [[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "W_v\n",
      " [[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n",
      "W_q\n",
      " [[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n",
      "K\n",
      " [[0 1 1]\n",
      " [4 4 0]\n",
      " [2 3 1]]\n",
      "V\n",
      " [[1 2 3]\n",
      " [2 8 0]\n",
      " [2 6 3]]\n",
      "Q\n",
      " [[1 0 2]\n",
      " [2 2 2]\n",
      " [2 1 3]]\n",
      "P\n",
      " [[ 2  4  4]\n",
      " [ 4 16 12]\n",
      " [ 4 12 10]]\n",
      "S\n",
      " [[0.06 0.47 0.47]\n",
      " [0.   0.98 0.02]\n",
      " [0.   0.88 0.12]]\n",
      "wV\n",
      " [[[0.06 0.13 0.19]\n",
      "  [0.   0.   0.  ]\n",
      "  [0.   0.   0.  ]]\n",
      "\n",
      " [[0.94 3.75 0.  ]\n",
      "  [1.96 7.86 0.  ]\n",
      "  [1.76 7.04 0.  ]]\n",
      "\n",
      " [[0.94 2.81 1.4 ]\n",
      "  [0.04 0.11 0.05]\n",
      "  [0.24 0.72 0.36]]]\n",
      "O\n",
      " [[1.94 6.68 1.6 ]\n",
      " [2.   7.96 0.05]\n",
      " [2.   7.76 0.36]]\n"
     ]
    }
   ],
   "source": [
    "#@Auto-atención\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Entradas\n",
    "# E\n",
    "E = np.array([[1, 0, 1, 0],\n",
    "              [0, 2, 0, 2],\n",
    "              [1, 1, 1, 1]])\n",
    "\n",
    "# Matrices de pesos W_k, W_v y W_q\n",
    "# W_k\n",
    "W_k = np.array([[0, 0, 1],\n",
    "                [1, 1, 0],\n",
    "                [0, 1, 0],\n",
    "                [1, 1, 0]])\n",
    "\n",
    "# W_v\n",
    "W_v = np.array([[0, 2, 0],\n",
    "                [0, 3, 0],\n",
    "                [1 ,0, 3],\n",
    "                [1, 1, 0]])\n",
    "\n",
    "# W_q\n",
    "W_q = np.array([[1, 0, 1],\n",
    "                [1, 0, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 1, 1]])\n",
    "\n",
    "# Claves (K), Valores (V), Consultas (Q)\n",
    "# K\n",
    "K = E @ W_k\n",
    "\n",
    "# V\n",
    "V = E @ W_v\n",
    "\n",
    "# Q\n",
    "Q = E @ W_q\n",
    "\n",
    "# Puntajes\n",
    "# P\n",
    "P = Q @ np.transpose(K)\n",
    "\n",
    "# Puntajes softmax\n",
    "# S\n",
    "S = softmax(P, axis=1)\n",
    "\n",
    "# Valores pesados con los puntajes softmax\n",
    "# valor 1\n",
    "#v0 = (np.transpose(np.tile(np.transpose(S)[:,0], (3,1))))*V\n",
    "# valor 2\n",
    "#v1 = (np.transpose(np.tile(np.transpose(S)[:,1], (3,1))))*V\n",
    "# valor 3\n",
    "#v2 = (np.transpose(np.tile(np.transpose(S)[:,2], (3,1))))*V\n",
    "\n",
    "wV = V[:,None] * S.T[:,:,None]\n",
    "\n",
    "# Salidas:  sumas de los valores pesados\n",
    "#o0=  np.sum(v0, axis=0)\n",
    "#o1=  np.sum(v1, axis=0)\n",
    "#o2=  np.sum(v2, axis=0)\n",
    "\n",
    "O = wV.sum( axis=0)\n",
    "\n",
    "print('E=\\n ', E)\n",
    "\n",
    "print('W_k\\n', W_k)\n",
    "print('W_v\\n', W_v)\n",
    "print('W_q\\n', W_q)\n",
    "\n",
    "print('K\\n', K)\n",
    "print('V\\n', V)\n",
    "print('Q\\n', Q)\n",
    "print('P\\n', P)\n",
    "print('S\\n', np.round(S,2))\n",
    "\n",
    "#print('v0:', np.round(v0,2))\n",
    "#print('v1:', np.round(v1,2))\n",
    "#print('v2:', np.round(v2,2))\n",
    "\n",
    "#print('o0:', np.round(o0,2))\n",
    "#print('o1:', np.round(o1,2))\n",
    "#print('o2:', np.round(o2,2))\n",
    "\n",
    "print('wV\\n',np.round(wV,2))\n",
    "print('O\\n',np.round(O,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
