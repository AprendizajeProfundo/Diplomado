{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Mecanismos de auto-atenci贸n</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/child_engaging_in_joint_attention.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: <a href=\"https://commons.wikimedia.org/wiki/File:Illustration_of_caregiver_and_child_engaging_in_joint_attention.jpg\">Rita.obeid6</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Illustrated Auto-attenci贸n](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "1. [Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)\n",
    "1. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "1. [Effective Approaches to Attention-based Neural Machine Translation (Luong et. al, 2015)](https://arxiv.org/pdf/1508.04025.pdf)\n",
    "1. [Attention Is All You Need (Vaswani et. al, 2017)](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "1. [Self-Attention GAN (Zhang et. al, 2018)](https://arxiv.org/pdf/1805.08318.pdf)\n",
    "1. [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "1. [TensorFlows seq2seq Tutorial with Attention (Tutorial on seq2seq+attention)](https://github.com/tensorflow/nmt)\n",
    "1. [Lilian Wengs Blog on Attention (Great start to attention)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)\n",
    "1. [Jay Alammars Blog on Seq2Seq with Attention (Great illustrations and worked example on seq2seq+attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "1. [Googles Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducci贸n](#Introducci贸n)\n",
    "* [Modelo seq2seq](#Modelo-seq2seq)\n",
    "* [Auto-atenci贸n](#Auto-atenci贸n)\n",
    "    * [Paso 1. Preparar las entradas](#Paso-1.-Preparar-las-entradas)\n",
    "    * [Paso 2. Inicializar los pesos](#Paso-2.-Inicializar-los-pesos)\n",
    "    * [Paso 3. Obtener claves, consultas y valores](#Paso-3.-Obtener-claves,-consultas-y-valores)\n",
    "    * [Paso 4. C谩lculo de los puntajes de auto-atenci贸n para las entradas](#Paso-4.-C谩lculo-de-los-puntajes-de-auto-atenci贸n-para-las-entradas)\n",
    "    * [Paso 5. C谩lculo del puntaje softmax](#Paso-5.-C谩lculo-del-puntaje-softmax)\n",
    "    * [Paso 6. Multiplica los puntajes softmax con los valores para cada consulta](#Paso-6.-Multiplica-los-puntajes-softmax-con-los-valores-para-cada-consulta)\n",
    "    * [Paso 7. Suma pesada de valores para conseguir los vectores de contexto](#Paso-7.-Suma-pesada-de-valores-para-conseguir-los-vectores-de-contexto)\n",
    "    * [Ejemplo de implementaci贸n en Python](#Ejemplo-de-implementaci贸n-en-Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驴Qu茅 tienen en com煤n BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, SciBERT, BioBERT, MobileBERT, TinyBERT y CamemBERT?. NO es propiamente BERT.\n",
    "\n",
    "Respuesta: auto-atenci贸n (`self-attention`). No solo estamos hablando de arquitecturas que llevan el nombre \"BERT\", sino m谩s correctamente de arquitecturas basadas en transformers. \n",
    "\n",
    "Las arquitecturas basadas en transformers que se utilizan principalmente en el modelado de tareas de comprensi贸n del lenguaje, evitan el uso de la recurrencia en la red neuronal y, en cambio, conf铆an por completo en los mecanismos de auto-atenci贸n para generar dependencias globales entre entradas y salidas. \n",
    "\n",
    "\n",
    "La auto-atenci贸n es similar a la atenci贸n. Comparten fundamentalmente el mismo concepto y muchas operaciones matem谩ticas comunes.  Un m贸dulo de auto-atenci贸n toma $n$ entradas y devuelve $n$ salidas. 驴Qu茅 pasa en este m贸dulo? En t茅rminos sencillos, el mecanismo de auto atenci贸n permite que las entradas interact煤en entre s铆 (`auto`) y descubran a qui茅n deben prestar m谩s atenci贸n (`atenci贸n`). Los resultados son agregados de estas interacciones y puntuaciones de atenci贸n.\n",
    "\n",
    "\n",
    "En esta lecci贸n  revisamos los detalles de construcci贸n de los mecanismos de atenci贸n.\n",
    "\n",
    "Supongamos que tenemos un secuencia con tres entradas $e_1, e_2$ y $e_3$. Un mecanismo de atenci贸n siguen el siguiente algoritmo.\n",
    "\n",
    "\n",
    "\n",
    "1. Prepara las entradas\n",
    "1. Inicializa los pesos\n",
    "1. Obtiene `clave` (key), `consulta` (query) y `valor` (value) a aprtir de las entradas.\n",
    "1. Calcula las puntuaciones de atenci贸n para la entrada $e_1$.\n",
    "1. Calcula el softmax de las puntuaciones.\n",
    "1. Multiplica las puntuaciones softmax con los `valores`\n",
    "1. Sumar `valores ponderados` para obtener la Salida 1\n",
    "1. Repite los pasos 4 a 7 para la entrada $e_2$ y la entrada $e_3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Auto-atenci贸n</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 1. Preparar las entradas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas corresponden a una secuencia de palabras sumergidas (embebidas) en un sumergimiento (embbeding) el cual vamos a suponer de tama帽o 4. Para el ejemplo supondremos que la secuencia consta de 3 palabras. Entonces tenemos 3 entradas de tama帽o 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_1.gif\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Entradas.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo tenemos tres entradas.\n",
    "\n",
    "1. entrada 1: $e_1=[1, 0, 1, 0]$\n",
    "1. entrada 2: $e_2=[0, 2, 0, 2]$\n",
    "1. entrada 3: $e_3=[1, 1, 1, 1]$\n",
    "\n",
    "En notaci贸n matricial escribimos\n",
    "\n",
    "$$\n",
    "E = \\begin{pmatrix}\n",
    "1 & 0 & 1 & 0\\\\\n",
    "0 & 2 & 0 & 2\\\\\n",
    "1 & 1 & 1 &1\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "e_1\\\\\n",
    "e_2\\\\\n",
    "e_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 2. Inicializar los pesos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada una de las entradas se transforman en tres diferentes vectores de alg煤n tama帽o. Para ello se construyen tres transformaciones lineales (o afines), digamos $W_k$, $W_q$ y $W_v$. Esta matrices son par谩metros de la red neuronal que son inicializados aleatoriamente, o con alg煤n procedimiento est谩ndar  para  inicializar  los  pesos.\n",
    "\n",
    "Para esta ilustraci贸n, supondremos que las matrices de pesos son dadas por\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "W_k = \n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_v = \n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}, \\quad\n",
    "W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 3. Obtener claves, consultas y valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso obtenemos los tres tipos de objetos derivados de la entrada: claves, consultas y valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claves (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las claves se obtienen de la siguiente manera:\n",
    "\n",
    "$$\n",
    "K = E \\times W_k = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 0 &1\\\\\n",
    "1 & 1 &0\\\\\n",
    "0 & 1 &0\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}  =\n",
    "\\begin{pmatrix} \n",
    "0 & 1 &1\\\\\n",
    "4 & 4 &0 \\\\\n",
    "2 & 3 &1 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "k_1\\\\\n",
    "k_2\\\\\n",
    "k_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores se obtiene como\n",
    "\n",
    "$$\n",
    "V = E \\times W_v = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 2 &0\\\\\n",
    "0 & 3 &0\\\\\n",
    "1 & 0 &3\\\\\n",
    "1 & 1 &0\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 2 &3\\\\\n",
    "2 & 8 &0 \\\\\n",
    "2 & 6 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "v_1\\\\\n",
    "v_2\\\\\n",
    "v_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consultas (queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente las consultas resultantes son\n",
    "\n",
    "$$\n",
    "Q = E \\times W_q = \n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1 &0\\\\\n",
    "0 & 2 &0 &2\\\\\n",
    "1 & 1 &1 &1\\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &1\\\\\n",
    "1 & 0 &0\\\\\n",
    "0 & 0 &1\\\\\n",
    "0 & 1 &1\n",
    "\\end{pmatrix}   =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "q_1\\\\\n",
    "q_2\\\\\n",
    "q_3\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_2.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Obtenci贸n de claves consultas y valores.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 4. C谩lculo de los puntajes de auto-atenci贸n para las entradas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilustramos aqu铆 como se calculan los puntajes de auto-atenci贸n para la entrada 1. Los puntajes de auto-atenci贸n para las dem谩s entradas se calculan de la misma manera con el cambio obvio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_3.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: Obtenci贸n de puntajes de auto-atenci贸n para la primera consulta.</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se muestra en la anterior ilustraci贸n, se toma la consulta obtenida para la primera entrada, que en el ejemplo es $e_1 =[1, 0, 2]$. Los pesos de auto-atenci贸n son una medida de similaridad, entre la consulta y cada una de las claves, como se ha estudiado en las lecciones atenci贸n. La funci贸n de auto-atenci贸n se denotar谩 $a$ . En este caso el resultado se obtiene haciendo el producto escalar (producto punto) entre la consulta asociada  a $e_1$, es decir, $q_1$ y cada una de las claves, es decir $a(q_1, c_i)= <q_1, c_i>, i =1,2,3$.\n",
    "\n",
    "Escrito en forma matricial, los pesos de auto-atenci贸n para la entrada 1 se calculan como\n",
    "\n",
    "\n",
    "$$\n",
    "p_1 = q_1 \\times K^T = [1, 0, 2]\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix} =[2, 4, 4].\n",
    "$$\n",
    "\n",
    "Los pesos de auto-atenci贸n completos, es decir para todas las  consultas  se calculan mediante\n",
    "\n",
    "$$\n",
    "P = Q \\times  K^T =\n",
    "\\begin{pmatrix} \n",
    "1 & 0 &2\\\\\n",
    "2 & 2 &2 \\\\\n",
    "2 & 1 &3 \\\\\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix} \n",
    "0 & 4 & 2 \\\\\n",
    "1 & 4 & 3 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{pmatrix}= \\begin{pmatrix} \n",
    "2 & 4 & 4 \\\\\n",
    "4 & 16 & 12 \\\\\n",
    "4 & 12 & 10 \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "p_1\\\\\n",
    "p_2\\\\\n",
    "p_3\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 5. C谩lculo del puntaje softmax</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos los puntajes a la escala softmax. Por ejemplo softmax([2, 4, 4]) = [0.06, 0.47, 0.47]. Observe que softmax define una distribuci贸n discreta de probabilidad. Ele c谩lculo completo para el ejemplo es\n",
    "\n",
    "\n",
    "$$\n",
    "S = \\begin{pmatrix}\n",
    "\\text{softmax}(p_1)\\\\\n",
    "\\text{softmax}(p_2)\\\\\n",
    "\\text{softmax}(p_3)\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "0.06 & 0.47 &0.47\\\\\n",
    "0.00 & 0.98 &0.02 \\\\\n",
    "0.00 & 0.88 &0.12 \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "s_1\\\\\n",
    "s_2\\\\\n",
    "s_3\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 6. Multiplica los puntajes softmax con los valores para cada consulta</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la consulta 1 se tiene\n",
    "* 0.06 * [1, 2, 3] = [0.06, 0.13, 0.19]\n",
    "* 0.47 * [2, 8, 0] = [0.94, 3.75, 0.00]\n",
    "* 0.47 * [2, 6, 3] = [0.94, 2.81, 1.40]\n",
    "\n",
    "Para la consulta 2\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.98 * [2, 8, 0] = [1.96, 7.86, 0.00]\n",
    "* 0.02 * [2, 6, 3] = [0.04, 0.11, 0.05]\n",
    "\n",
    "Para la consulta 3\n",
    "* 0.00 * [1, 2, 3] = [0.00, 0.00, 0.00]\n",
    "* 0.88 * [2, 8, 0] = [1.76, 7.04, 0.00]\n",
    "* 0.12 * [2, 6, 3] = [0.24, 0.72, 0.36]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paso 7. Suma pesada de valores para conseguir los vectores de contexto</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida (vector de contexto) se obtiene como la suma de los vectores pesados asociados a cada consulta as铆:\n",
    "\n",
    "* salida 1: [0.06, 0.13, 0.19] + [0.94, 3.75, 0.00] + [0.94, 2.81, 1.40] = [1.94, 6.68, 1.60]\n",
    "* salida 2: [0.00, 0.00, 0.00] + [1.96, 7.86, 0.00] + [0.04, 0.11, 0.05] = [2.00, 7.96, 0.05]\n",
    "* salida 3: [0.00, 0.00, 0.00] + [1.76, 7.04, 0.00] + [0.24, 0.72, 0.36] = [2.00, 7.76, 0.36]\n",
    "\n",
    "La siguiente imagen muestra el modelo completo de auto-atenci贸n. Los valores se redondearon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention_4.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Auto-atenci贸n: C谩lculo completo de las 煤ltimas dos salidas</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Illustrated self attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de implementaci贸n en Python</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E=\n",
      "  [[1 0 1 0]\n",
      " [0 2 0 2]\n",
      " [1 1 1 1]]\n",
      "W_k\n",
      " [[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "W_v\n",
      " [[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n",
      "W_q\n",
      " [[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n",
      "K\n",
      " [[0 1 1]\n",
      " [4 4 0]\n",
      " [2 3 1]]\n",
      "V\n",
      " [[1 2 3]\n",
      " [2 8 0]\n",
      " [2 6 3]]\n",
      "Q\n",
      " [[1 0 2]\n",
      " [2 2 2]\n",
      " [2 1 3]]\n",
      "P\n",
      " [[ 2  4  4]\n",
      " [ 4 16 12]\n",
      " [ 4 12 10]]\n",
      "S\n",
      " [[0.06 0.47 0.47]\n",
      " [0.   0.98 0.02]\n",
      " [0.   0.88 0.12]]\n",
      "wV\n",
      " [[[0.06 0.13 0.19]\n",
      "  [0.   0.   0.  ]\n",
      "  [0.   0.   0.  ]]\n",
      "\n",
      " [[0.94 3.75 0.  ]\n",
      "  [1.96 7.86 0.  ]\n",
      "  [1.76 7.04 0.  ]]\n",
      "\n",
      " [[0.94 2.81 1.4 ]\n",
      "  [0.04 0.11 0.05]\n",
      "  [0.24 0.72 0.36]]]\n",
      "O\n",
      " [[1.94 6.68 1.6 ]\n",
      " [2.   7.96 0.05]\n",
      " [2.   7.76 0.36]]\n"
     ]
    }
   ],
   "source": [
    "#@Auto-atenci贸n\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Entradas\n",
    "# E\n",
    "E = np.array([[1, 0, 1, 0],\n",
    "              [0, 2, 0, 2],\n",
    "              [1, 1, 1, 1]])\n",
    "\n",
    "# Matrices de pesos W_k, W_v y W_q\n",
    "# W_k\n",
    "W_k = np.array([[0, 0, 1],\n",
    "                [1, 1, 0],\n",
    "                [0, 1, 0],\n",
    "                [1, 1, 0]])\n",
    "\n",
    "# W_v\n",
    "W_v = np.array([[0, 2, 0],\n",
    "                [0, 3, 0],\n",
    "                [1 ,0, 3],\n",
    "                [1, 1, 0]])\n",
    "\n",
    "# W_q\n",
    "W_q = np.array([[1, 0, 1],\n",
    "                [1, 0, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 1, 1]])\n",
    "\n",
    "# Claves (K), Valores (V), Consultas (Q)\n",
    "# K\n",
    "K = E @ W_k\n",
    "\n",
    "# V\n",
    "V = E @ W_v\n",
    "\n",
    "# Q\n",
    "Q = E @ W_q\n",
    "\n",
    "# Puntajes\n",
    "# P\n",
    "P = Q @ np.transpose(K)\n",
    "\n",
    "# Puntajes softmax\n",
    "# S\n",
    "S = softmax(P, axis=1)\n",
    "\n",
    "# Valores pesados con los puntajes softmax\n",
    "# valor 1\n",
    "#v0 = (np.transpose(np.tile(np.transpose(S)[:,0], (3,1))))*V\n",
    "# valor 2\n",
    "#v1 = (np.transpose(np.tile(np.transpose(S)[:,1], (3,1))))*V\n",
    "# valor 3\n",
    "#v2 = (np.transpose(np.tile(np.transpose(S)[:,2], (3,1))))*V\n",
    "\n",
    "wV = V[:,None] * S.T[:,:,None]\n",
    "\n",
    "# Salidas:  sumas de los valores pesados\n",
    "#o0=  np.sum(v0, axis=0)\n",
    "#o1=  np.sum(v1, axis=0)\n",
    "#o2=  np.sum(v2, axis=0)\n",
    "\n",
    "O = wV.sum( axis=0)\n",
    "\n",
    "print('E=\\n ', E)\n",
    "\n",
    "print('W_k\\n', W_k)\n",
    "print('W_v\\n', W_v)\n",
    "print('W_q\\n', W_q)\n",
    "\n",
    "print('K\\n', K)\n",
    "print('V\\n', V)\n",
    "print('Q\\n', Q)\n",
    "print('P\\n', P)\n",
    "print('S\\n', np.round(S,2))\n",
    "\n",
    "#print('v0:', np.round(v0,2))\n",
    "#print('v1:', np.round(v1,2))\n",
    "#print('v2:', np.round(v2,2))\n",
    "\n",
    "#print('o0:', np.round(o0,2))\n",
    "#print('o1:', np.round(o1,2))\n",
    "#print('o2:', np.round(o2,2))\n",
    "\n",
    "print('wV\\n',np.round(wV,2))\n",
    "print('O\\n',np.round(O,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
