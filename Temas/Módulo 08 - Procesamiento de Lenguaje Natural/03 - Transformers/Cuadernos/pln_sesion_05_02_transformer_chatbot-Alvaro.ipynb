{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Transformer Chatbot</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Chatbot conversacional </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/robot_chat.jpg\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Pixabay](https://www.istockphoto.com/es/search/2/image?mediatype=&phrase=robot+chat&utm_source=pixabay&utm_medium=affiliate&utm_campaign=SRP_image_sponsored&utm_content=http%3A%2F%2Fpixabay.com%2Fes%2Fimages%2Fsearch%2Frobot+chat%2F&utm_term=robot+chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Alvaro Montenegro, Daniel Montenegro y Oleg Jarma, Inteligencia Artificial y Aprendizaje Profundo Avanzado, 2022](https://github.com/AprendizajeProfundo/Diplomado-Avanzado)\n",
    "1. [Transformer Chatbot with TensorFlow 2](https://github.com/bryanlimy/tf2-transformer-chatbot)\n",
    "1. [Marvin Lanhenke, NLP-Day 22: How To Create A Chatbot With Transformers, 2022](https://medium.com/mlearning-ai/nlp-day-22-how-to-create-a-chatbot-with-transformers-fbb194608217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Importa-librerías](#Importa-librerías)\n",
    "* [Hiperparámetros](Hiperparámetros)\n",
    "* [Prepara el Dataset](Prepara-el-Dataset)\n",
    "* [Atención](#Atención)\n",
    "* [Transformer](#Transformer)\n",
    "* [Entrenamiento del modelo](#Entrenamiento-del-modelo)\n",
    "* [Evaluación y predicción](#Evaluación-y-predicción)\n",
    "* [Resumen](#Resumen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHMPkA2eQrpT"
   },
   "source": [
    "En este tutorial entrenamos <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" class=\"external\"> modelo Transformer </a> Pra que actúe como un chatbot. Este es un ejemplo avanzado que asume el conocimiento de [generación de texto](https://tensorflow.org/alpha/tutorials/text/text_generation), [atención](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) and [Transformer](https://www.tensorflow.org/alpha/tutorials/text/transformer).\n",
    "\n",
    "La idea central detrás del modelo Transformer es la *autoatención*: la capacidad de prestar atención a diferentes posiciones de la secuencia de entrada para calcular una representación de esa secuencia. Transformer crea pilas de capas de autoatención y se explica a continuación en las secciones *Atención de producto de punto escalado* y *Atención de varios cabezales*.\n",
    "\n",
    "Nota: La arquitectura del modelo es idéntica al ejemplo en [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer), y demostramos cómo implementar el mismo modelo en el enfoque Funcional en lugar de subclases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHMPkA2eQrpT"
   },
   "source": [
    "## <span style=\"color:blue\">Importa librerías</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "mb_5bl7G_n30",
    "outputId": "55ab009c-7f3c-4b47-9c77-529ea00146f6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "!pip install tensorflow-datasets==4.1.0\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YXFrSnNE_2j"
   },
   "source": [
    "## <span style=\"color:blue\">Inicialización de GPU / TPU </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YXFrSnNE_2j"
   },
   "source": [
    "En Google Colab, seleccione el hardware acelrador `TPU` o `GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "Rz2ZbpAREmRA",
    "outputId": "3095d2e2-c783-4af5-9b77-b1e40ad1c82e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMfzRV5cEnb"
   },
   "source": [
    "## <span style=\"color:blue\">Hiperparámetros</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMfzRV5cEnb"
   },
   "source": [
    "Para que este ejemplo sea pequeño y relativamente rápido, se han reducido los valores de *num_layers, d_model y units*. Consulte el [artículo](https://arxiv.org/abs/1706.03762) para conocer todas las demás versiones del transformador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alnYEatYcHp7"
   },
   "outputs": [],
   "source": [
    "# configura hiper params\n",
    "# Longitud máxima de sentencia\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# Número máximo de muestras para preprocesar\n",
    "MAX_SAMPLES = 50000\n",
    "\n",
    "# Para tf.data.Dataset\n",
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# Para el Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0AqALdZCbCW"
   },
   "source": [
    "## <span style=\"color:blue\">Prepara el Dataset</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awb4RH3XCobf"
   },
   "source": [
    "Usaremos las conversaciones en películas y programas de televisión proporcionados por [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), que contiene más de 220 mil intercambios conversacionales. entre más de 10k pares de personajes de películas, como nuestro conjunto de datos.\n",
    "\n",
    "`movie_conversations.txt` contiene una lista de ID de conversación y `movie_lines.text` contiene el texto asociado con cada ID de conversación. Para obtener más información sobre el conjunto de datos, consulte el archivo README en el archivo zip.\n",
    "\n",
    "Bajaremos los datos directamente de Cornell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "S17Nfn6W_vhd",
    "outputId": "92553a0e-78b3-4504-bdc1-ae79ae84c64c"
   },
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZMuzj0cVr3E"
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Cargar y preprocesar datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZMuzj0cVr3E"
   },
   "source": [
    "Para mantener este ejemplo simple y rápido, estamos limitando el número máximo de muestras de entrenamiento a `MAX_SAMPLES=25000` y la longitud máxima de la oración a `MAX_LENGTH=40`.\n",
    "\n",
    "Preprocesamos nuestro conjunto de datos en el siguiente orden:\n",
    "* Extraiga los pares de conversación `MAX_SAMPLES` en la lista de `preguntas` y `respuestas.\n",
    "* Preprocesar cada oración eliminando caracteres especiales en cada oración.\n",
    "* Crear tokenizador (asignar texto a ID e ID a texto) usando [TensorFlow Datasets SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder).\n",
    "* Tokenice cada oración y agregue `START_TOKEN` y `END_TOKEN` para indicar el comienzo y el final de cada oración.\n",
    "* Filtre la oración que tenga más de tokens `MAX_LENGTH`.\n",
    "* Rellene las oraciones tokenizadas a `MAX_LENGTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_B147qKb_0ks"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    # creando un espacio entre una palabra y la puntuación que le sigue\n",
    "    # ejemplo: \"he is a boy.\" => \"he is a boy .\"\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # remueve contracciones\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "    # reemplazando todo con espacio excepto (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "    # diccionario de id de línea a texto\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        # obtener conversación en una lista de ID de línea\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "            outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "        return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304713\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "\n",
      "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
      "\n",
      "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
      "\n",
      "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n",
      "\n",
      "L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
      "\n",
      "L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Revisión del archivo de líneas\n",
    "id2line = {}\n",
    "with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "print(len(lines))\n",
    "for i in range(10):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!']\n",
      "{'L1045': 'They do not!'}\n"
     ]
    }
   ],
   "source": [
    "parts = lines[0].replace('\\n', '').split(' +++$+++ ')\n",
    "id2line[parts[0]] = parts[4]\n",
    "print(parts)\n",
    "print(id2line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Revisión archivo de conversaciones\n",
    "with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for i in range(10):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
    "\n",
    "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mfOOK5f7Wm6c",
    "outputId": "77711477-381e-4043-f7bd-68a737ac769f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra de una pregunta: i really , really , really wanna go , but i cannot . not unless my sister goes .\n",
      "Muestra de una respuesta: i am working on it . but she does not seem to be going for him .\n"
     ]
    }
   ],
   "source": [
    "print('Muestra de una pregunta: {}'.format(questions[20]))\n",
    "print('Muestra de una respuesta: {}'.format(answers[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6XX2udMTCQt"
   },
   "outputs": [],
   "source": [
    "# Crea un tokenizador usando tfds para preguntas y respuestas\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# Definir token de inicio y final para indicar el inicio y el final de una oración\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Tamaño del vocabulario más token de inicio y finalización\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h5h8pvRUTFt5",
    "outputId": "c79a6f34-6f92-4f13-b241-01ac4dfb04ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta de muestra tokenizada: [4, 271, 3, 271, 3, 141, 385, 173, 3, 40, 4, 611, 2, 11, 864, 30, 2021, 3086, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Pregunta de muestra tokenizada: {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YESTPgeg_XgT"
   },
   "outputs": [],
   "source": [
    "# Tokenizar, filtrar and rellenar sentencias\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokeniza sentencia\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    # verificar la longitud máxima de la oración tokenizada\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # rellenar oraciones tokenizadas\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "pohHm8IRWlIH",
    "outputId": "75276d1d-1cf5-41c7-b337-bade514ea195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 8279\n",
      "Numbero de muestras: 44131\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño del vocabulario: {}'.format(VOCAB_SIZE))\n",
    "print('Numbero de muestras: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S50jT4upWh5c",
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Crea el  `tf.data.Dataset`s</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S50jT4upWh5c"
   },
   "source": [
    "Vamos a utilizar [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data) para construir nuestra línea de entrada con el fin de utilizar funciones como el almacenamiento en caché y la captación previa para acelerar el proceso de formación.\n",
    "\n",
    "El transformador es un modelo autorregresivo: hace predicciones una parte a la vez y usa su salida hasta el momento para decidir qué hacer a continuación.\n",
    "\n",
    "Durante la capacitación, este ejemplo utiliza la fuerza del maestro. La fuerza del profesor pasa la salida real al siguiente paso de tiempo, independientemente de lo que prediga el modelo en el paso de tiempo actual.\n",
    "\n",
    "A medida que el transformador predice cada palabra, la autoatención le permite mirar las palabras anteriores en la secuencia de entrada para predecir mejor la siguiente palabra.\n",
    "\n",
    "Para evitar que el modelo alcance el resultado esperado, el modelo utiliza una máscara de anticipación.\n",
    "\n",
    "El objetivo se divide en `decoder_inputs` que se rellenan como una entrada al decodificador y `cropped_targets` para calcular nuestra pérdida y precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pttC3XxgAXWQ"
   },
   "outputs": [],
   "source": [
    "# decoder:   las entradas usan el  target previo as entrada\n",
    "# remover START_TOKEN de los target\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9eeMPjGXmI1"
   },
   "source": [
    "## <span style=\"color:blue\">Atención</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uctkwvPZVSzu"
   },
   "source": [
    "### Producto punto escalado Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uctkwvPZVSzu"
   },
   "source": [
    "La función de atención de producto punto escalado utilizada por el transformador toma tres entradas: Q (consulta), K (clave), V (valor). La ecuación utilizada para calcular los pesos de atención es:\n",
    "\n",
    "$$\\Large{Atención(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "Como la normalización softmax se realiza en la `clave`, sus valores deciden la cantidad de importancia que se le da a la `consulta`.\n",
    "\n",
    "La salida representa la multiplicación de los pesos de atención y el vector `valor`. Esto asegura que las palabras en las que queremos centrarnos se mantengan como están y las palabras irrelevantes se eliminen.\n",
    "\n",
    "La atención del producto escalar se escala por un factor de raíz cuadrada de la profundidad. Esto se hace porque para valores grandes de profundidad, el producto escalar crece en magnitud empujando la función softmax donde tiene pequeños gradientes que dan como resultado un softmax muy duro.\n",
    "\n",
    "Por ejemplo, considere que `query` y `key` tienen una media de 0 y una varianza de 1. Su multiplicación de matriz tendrá una media de 0 y una varianza de `dk`. Por lo tanto, *la raíz cuadrada de `dk`* se usa para escalar (y no cualquier otro número) porque el matmul de `query` y `key` debe tener una media de 0 y una varianza de 1, de modo que obtengamos un softmax más suave. .\n",
    "\n",
    "La máscara se multiplica con *-1e9 (cerca del infinito negativo).* Esto se hace porque la máscara se suma con la multiplicación de matriz escalada de `query` y `key` y se aplica inmediatamente antes de un softmax. El objetivo es poner a cero estas celdas, y las entradas negativas grandes a softmax están cerca de cero en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENfqAFna_50H"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwmOB9HvVbyh"
   },
   "source": [
    "### Cabeza Multi-atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwmOB9HvVbyh"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "La atención de múltiples cabezas consta de cuatro partes:\n",
    "* Capas lineales y divididas en cabezas.\n",
    "* Atención de producto punto escalado.\n",
    "* Concatenación de cabezas.\n",
    "* Capa lineal final.\n",
    "\n",
    "Cada bloque de atención de varios cabezales recibe tres entradas; Q (consulta), K (clave), V (valor). Estos se colocan a través de capas lineales (Densas) y se dividen en múltiples cabezas.\n",
    "\n",
    "El `scaled_dot_product_attention` definido anteriormente se aplica a cada cabeza (transmitido por eficiencia). Se debe utilizar una máscara adecuada en el paso de atención. Luego, la salida de atención para cada cabeza se concatena (usando `tf.transpose` y `tf.reshape`) y se pasa por una capa final `Dense`.\n",
    "\n",
    "En lugar de un solo encabezado de atención, `consulta`, `clave` y `valor` se dividen en múltiples encabezados porque permite que el modelo atienda conjuntamente la información en diferentes posiciones desde diferentes espacios de representación. Después de la división, cada cabeza tiene una dimensionalidad reducida, por lo que el costo total de cómputo es el mismo que el de la atención de una sola cabeza con dimensionalidad completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9eYssGIAG4h"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "  \n",
    "  def get_config(self):\n",
    "        config = super(MultiHeadAttention,self).get_config()\n",
    "        config.update({\n",
    "            'num_heads':self.num_heads,\n",
    "            'd_model':self.d_model,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.keras.layers.Lambda(lambda inputs:tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)\n",
    "    return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # Producto escalr atencional\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "    scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(\n",
    "        scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)\n",
    "\n",
    "    # concatenación de las cabezas de atención\n",
    "    concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model)))(scaled_attention)\n",
    "\n",
    "    # Capa lineal final\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDUX7Oa8Xudj"
   },
   "source": [
    "## <span style=\"color:blue\"> Transformer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5QlgXsxYirg"
   },
   "source": [
    "### Enmascaramiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CX5H8A-Wybj"
   },
   "source": [
    "`create_padding_mask` y `create_look_ahead` son funciones auxiliares para crear máscaras para enmascarar tokens acolchados, vamos a utilizar estas funciones auxiliares como capas `tf.keras.layers.Lambda`.\n",
    "\n",
    "Enmascare todos los tokens de relleno (valor `0`) en el lote para asegurarse de que el modelo no trate el relleno como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imCQ0jrvWhC7"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "IrwtsqrfWd-3",
    "outputId": "ed6f7b55-fb83-4f63-b0d8-797156e83977"
   },
   "outputs": [],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJAicy1zW1QT"
   },
   "source": [
    "Máscara de anticipación para enmascarar los tokens futuros en una secuencia.\n",
    "También enmascaramos tokens de pad.\n",
    "\n",
    "es decir, para predecir la tercera palabra, solo se usarán la primera y la segunda palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSVdD2zKWaXx"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "tf.linalg.band_part(tf.ones((seq_len, seq_len)), 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "xhwz9xzxWcod",
    "outputId": "a84b68d2-d31f-4096-9734-8a44ee6f92b6"
   },
   "outputs": [],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpR7kz4jFkPJ"
   },
   "source": [
    "### Codificación posicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpR7kz4jFkPJ"
   },
   "source": [
    "Dado que este modelo no contiene recurrencia ni convolución, se agrega codificación posicional para brindarle al modelo información sobre la posición relativa de las palabras en la oración.\n",
    "\n",
    "El vector de codificación posicional se agrega al vector de incrustación. Las incrustaciones representan un token en un espacio d-dimensional donde los tokens con un significado similar estarán más cerca unos de otros. Pero las incrustaciones no codifican la posición relativa de las palabras en una oración. Entonces, después de agregar la codificación posicional, las palabras estarán más cerca entre sí en función de la *similitud de su significado y su posición en la oración*, en el espacio d-dimensional.\n",
    "\n",
    "Consulte el cuaderno sobre [codificación posicional](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) para obtener más información al respecto. La fórmula para calcular la codificación posicional es la siguiente:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{modelo}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{modelo}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9Oibz2es-qW"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "  \n",
    "  def get_config(self):\n",
    "\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model,\n",
    "            \n",
    "        })\n",
    "        return config\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, de_model)\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self_position,\n",
    "            'd_model': self.de_model\n",
    "        })\n",
    "        return config\n",
    "    def get_angles(self, position, d_model):\n",
    "        angles = 1/tf.pow(10000), (2*(i//2)) / tf.cast(d_model, tf.float21)\n",
    "        return position * angles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "UC_fQehi3_Yh",
    "outputId": "c6aeef77-4076-43d5-97b0-48f7558b998d"
   },
   "outputs": [],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVazCemoW2Ye"
   },
   "source": [
    "### Capa del codificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVazCemoW2Ye"
   },
   "source": [
    "Cada capa de codificador consta de subcapas:\n",
    "\n",
    "1. Atención multicabezal (con máscara de relleno)\n",
    "1. 2 capas densas seguidas de abandono\n",
    "\n",
    "Cada una de estas subcapas tiene una conexión residual a su alrededor seguida de una normalización de capa. Las conexiones residuales ayudan a evitar el problema del gradiente de fuga en las redes profundas.\n",
    "\n",
    "La salida de cada subcapa es `LayerNorm(x + Sublayer(x))`. La normalización se realiza en el eje `d_model` (último)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5guJOLJmfcuX"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  add_attention = tf.keras.layers.add([inputs,attention])\n",
    "  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  add_attention = tf.keras.layers.add([attention,outputs])\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K16BIGSKfkve",
    "outputId": "79328971-65e1-4bc1-f71f-bb498b762026"
   },
   "outputs": [],
   "source": [
    "sample_encoder_layer = encoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_encoder_layer, to_file='encoder_layer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9r8lWGClfi_1"
   },
   "source": [
    "### Codificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9r8lWGClfi_1"
   },
   "source": [
    "El codificador consta de:\n",
    "1. Incorporación de entrada\n",
    "1. Codificación posicional\n",
    "1. Capas de codificador `num_layers`\n",
    "\n",
    "La entrada se somete a una incrustación que se suma con la codificación posicional. La salida de esta suma es la entrada a las capas del codificador. La salida del codificador es la entrada al decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRfugon5Wy-Y"
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "          inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "          padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "          embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "          embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "          embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)\n",
    "\n",
    "          outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "          for i in range(num_layers):\n",
    "            outputs = encoder_layer(\n",
    "                units=units,\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                name=\"encoder_layer_{}\".format(i),\n",
    "            )([outputs, padding_mask])\n",
    "\n",
    "          return tf.keras.Model(\n",
    "              inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "colab_type": "code",
    "id": "bNxCnjrvglnx",
    "outputId": "fba96682-4659-4e93-effb-fcaa841ee13c"
   },
   "outputs": [],
   "source": [
    "sample_encoder = encoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "   sample_encoder, to_file='encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af66azvgW9P-"
   },
   "source": [
    "### Capa decodificadora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af66azvgW9P-"
   },
   "source": [
    "Cada capa del decodificador consta de subcapas:\n",
    "\n",
    "1. Atención multicabezal enmascarada (con máscara de anticipación y máscara de relleno)\n",
    "1. Atención multicabezal (con máscara de relleno). `value` y `key` reciben la *salida del codificador* como entradas. `query` recibe el *resultado de la subcapa de atención de múltiples cabezas enmascarada.*\n",
    "1. 2 capas densas seguidas de abandono\n",
    "\n",
    "Cada una de estas subcapas tiene una conexión residual a su alrededor seguida de una normalización de capa. La salida de cada subcapa es `LayerNorm(x + Sublayer(x))`. La normalización se realiza en el eje `d_model` (último).\n",
    "\n",
    "Como `query` recibe la salida del primer bloque de atención del decodificador y `key` recibe la salida del codificador, los pesos de atención representan la importancia otorgada a la entrada del decodificador en función de la salida del codificador. En otras palabras, el decodificador predice la siguiente palabra mirando la salida del codificador y atendiendo automáticamente a su propia salida. Vea la demostración anterior en la sección de atención del producto escalar escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mLvvNMWgDnf"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "  add_attention = tf.keras.layers.add([attention1,inputs])    \n",
    "  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  add_attention = tf.keras.layers.add([attention2,attention1])\n",
    "  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  add_attention = tf.keras.layers.add([outputs,attention2])\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8M1NrQ_NgEaM",
    "outputId": "12e3fb73-d13e-4c0a-f96a-bcb1d1691204"
   },
   "outputs": [],
   "source": [
    "sample_decoder_layer = decoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder_layer, to_file='decoder_layer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPSKnjS-gE_q"
   },
   "source": [
    "### Decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPSKnjS-gE_q"
   },
   "source": [
    "El decodificador consta de:\n",
    "1. Incrustación de salida\n",
    "1. Codificación posicional\n",
    "1. N capas decodificadoras\n",
    "\n",
    "El objetivo se somete a una incrustación que se suma con la codificación posicional. La salida de esta suma es la entrada a las capas del decodificador. La salida del decodificador es la entrada a la capa lineal final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYRx7YzCW4bu"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "colab_type": "code",
    "id": "tUdK8jb9hlTZ",
    "outputId": "50aa4972-1374-4e49-88af-326e0443f0ca"
   },
   "outputs": [],
   "source": [
    "sample_decoder = decoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder, to_file='decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl0o97RJXAqw"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yl0o97RJXAqw"
   },
   "source": [
    "El Transformer consiste en el codificador, decodificador y una capa lineal final. La salida del decodificador es la entrada a la capa lineal y se devuelve su salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW-v7Fz6XAfC"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "  # mask the future tokens for decoder inputs at the 1st attention block\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "  # mask the encoder outputs for the 2nd attention block\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "aihJLVq_iJ_T",
    "outputId": "682b0d2b-23f2-4a55-fe6a-238f2d3ac00a"
   },
   "outputs": [],
   "source": [
    "sample_transformer = transformer(\n",
    "    vocab_size=8192,\n",
    "    num_layers=4,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_transformer, to_file='transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9HD7GK-nh_KT"
   },
   "source": [
    "## <span style=\"color:blue\"> Entrenamiento del modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_GCb0LaV1tI"
   },
   "source": [
    "### Función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_GCb0LaV1tI"
   },
   "source": [
    "Dado que las secuencias de destino se rellenan, es importante aplicar una máscara de relleno al calcular la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UInVM9iGAMv1"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvFM9ajSVybP"
   },
   "source": [
    "### Ajuste de la rata de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvFM9ajSVybP"
   },
   "source": [
    "Use el optimizador de Adam con un programador de tasa de aprendizaje personalizado de acuerdo con la fórmula en el [artículo](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WW3SeLDhAMJd"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = tf.constant(d_model,dtype=tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def get_config(self):\n",
    "        return {\"d_model\": self.d_model,\"warmup_steps\":self.warmup_steps}\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "67BoG_UeaHHw",
    "outputId": "5da1c5f3-cd54-48ab-b7c8-f34b9b1cb3ab"
   },
   "outputs": [],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Rata de aprendizaje\")\n",
    "plt.xlabel(\"Paso de entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCqve3kwWCxd"
   },
   "source": [
    "### Inicializar y compilar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCqve3kwWCxd"
   },
   "source": [
    "Inicialice y compile el modelo con nuestra tasa de aprendizaje personalizada predefinida y el optimizador de Adam en el ámbito de la estrategia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "id": "1QqojIa5WEQq",
    "outputId": "24382659-95ca-4332-eff2-6fb9b7a1ea70"
   },
   "outputs": [],
   "source": [
    "# clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# initialize and compile model within strategy scope\n",
    "with strategy.scope():\n",
    "  model = transformer(\n",
    "      vocab_size=VOCAB_SIZE,\n",
    "      num_layers=NUM_LAYERS,\n",
    "      units=UNITS,\n",
    "      d_model=D_MODEL,\n",
    "      num_heads=NUM_HEADS,\n",
    "      dropout=DROPOUT)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDMd69urLNuc"
   },
   "source": [
    "### Ajusta el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDMd69urLNuc"
   },
   "source": [
    "Entrene a nuestro transformador simplemente llamando a `model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d7iahRzlLNG2",
    "outputId": "944b0a77-d6bf-497b-9b50-0dfcbbe3426e"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1DUXog6WqV-"
   },
   "source": [
    "## <span style=\"color:blue\">Evaluación y predicción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1DUXog6WqV-"
   },
   "source": [
    "Los siguientes pasos se utilizan para la evaluación:\n",
    "\n",
    "* Aplique el mismo método de preprocesamiento que usamos para crear nuestro conjunto de datos para la oración de entrada.\n",
    "* Tokenice la oración de entrada y agregue `START_TOKEN` y `END_TOKEN`.\n",
    "* Calcule las máscaras de relleno y las máscaras anticipadas.\n",
    "* El decodificador luego emite las predicciones mirando la salida del codificador y su propia salida.\n",
    "* Seleccione la última palabra y calcule el argmax de eso.\n",
    "* Concatenar la palabra predicha a la entrada del decodificador y pasarla al decodificador.\n",
    "* En este enfoque, el decodificador predice la siguiente palabra basándose en las palabras anteriores que predijo.\n",
    "\n",
    "Nota: El modelo utilizado aquí tiene menos capacidad y se entrenó en un subconjunto del conjunto de datos completo, por lo que su rendimiento se puede mejorar aún más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NjsS3zuAbRn"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # concatenated the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J3Jdtk2P-RT"
   },
   "source": [
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6IeMSGEgRTvC",
    "outputId": "68955e61-5c7f-4ae0-bb82-2ebfd5fdad3d"
   },
   "outputs": [],
   "source": [
    "output = predict('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ivVgU6ydRV8R",
    "outputId": "493ac595-e25a-4bb8-d112-45aa8d1f3169"
   },
   "outputs": [],
   "source": [
    "output = predict(\"It's a trap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "s5zG7i8KAtRU",
    "outputId": "5d2c4927-2435-4646-a2e6-27edc5ffe0a7"
   },
   "outputs": [],
   "source": [
    "# feed the model with its previous output\n",
    "sentence = 'I am not crazy, my mother had me tested.'\n",
    "for _ in range(5):\n",
    "  sentence = predict(sentence)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-dGPc14QnAP"
   },
   "source": [
    "## <span style=\"color:blue\"> Resumen</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-dGPc14QnAP"
   },
   "source": [
    "Aquí estamos, hemos implementado un Transformador en TensorFlow 2.0 en alrededor de 500 líneas de código.\n",
    "\n",
    "En este tutorial, nos enfocamos en los dos enfoques diferentes para implementar modelos complejos con API funcional y subclases de modelos, y cómo incorporarlos.\n",
    "\n",
    "¡Intente usar un conjunto de datos diferente o hiperparámetros para entrenar el Transformador! Gracias por leer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tf2_tpu_transformer_chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
