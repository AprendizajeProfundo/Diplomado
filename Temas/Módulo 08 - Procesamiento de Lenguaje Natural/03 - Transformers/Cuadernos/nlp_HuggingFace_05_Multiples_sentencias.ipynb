{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fdf1c6-20c1-4695-92c3-1a30d118da9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2d584-a384-4517-96c0-b085f6adfe24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Transformers- Múltiples secuencias</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d5c0f-edf4-4e0a-8260-4f6a251eedd3",
   "metadata": {},
   "source": [
    "<center>Manejando múltiples secuencias HuggingFace</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814b837-8220-4f6c-8d62-4f76d8652736",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2eeab-4acc-4a4f-bb79-9e2d45a810d0",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002dc78-cb0a-47d5-a17f-c6df384b96f8",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Diseño gráfico y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05789c43-41b3-494d-afaa-a370002d9ccf",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro Reyes, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda43ed-5f98-4ffa-b3d6-dd268a5fdc0e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a21b86-13b2-4c1b-824f-96dce45f3e4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4447d4d-a448-40d6-98cd-cbaebf381d7c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23caca7-9f44-4ca3-a8c9-2c4cfd44ee39",
   "metadata": {},
   "source": [
    "1. [HuggingFace. Transformers ](https://huggingface.co/transformers/)\n",
    "1. [HuggingFace. Models](https://huggingface.co/course/chapter2/3?fw=tf)\n",
    "1. [HuggingFace. Tokenizers](https://huggingface.co/course/chapter2/3?fw=tf)\n",
    "1. [Tutorial Transformer de Google](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "1. [Transformer-chatbot-tutorial-with-tensorflow-2](https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html) \n",
    "1. [Transformer Architecture: The positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "1. [Illustrated Auto-attención](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "1. [Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)\n",
    "1. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "1. [Effective Approaches to Attention-based Neural Machine Translation (Luong et. al, 2015)](https://arxiv.org/pdf/1508.04025.pdf)\n",
    "1. [Attention Is All You Need (Vaswani et. al, 2017)](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "1. [Self-Attention GAN (Zhang et. al, 2018)](https://arxiv.org/pdf/1805.08318.pdf)\n",
    "1. [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "1. [TensorFlow’s seq2seq Tutorial with Attention (Tutorial on seq2seq+attention)](https://github.com/tensorflow/nmt)\n",
    "1. [Lilian Weng’s Blog on Attention (Great start to attention)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)\n",
    "1. [Jay Alammar’s Blog on Seq2Seq with Attention (Great illustrations and worked example on seq2seq+attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "1. [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "1. [Adam: A method for stochastic optimization](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0da79b-44dc-460c-ac1e-b71ade087a9c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15195d1b-1dfc-4e35-a28d-5c6dad544252",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Los modelos esperan un lote de entrada](#Los-modelos-esperan-un-lote-de-entradas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe895d3-c25d-49d7-9b47-ff4835ba2b95",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be1833-6cb5-4ec7-9b09-320a67e9097e",
   "metadata": {},
   "source": [
    "En esta lección estudiamos:\n",
    "\n",
    "1. Cómo manejar mútlples secuencias.\n",
    "2. Cómo manejar secuencias cosn distintos tamaños.\n",
    "3. Que otras entradas manejan los modelos además de los índices de vocabulario\n",
    "4. Cómo manejar secuencias muy largas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb3463-df86-4f45-acb5-74240b5f814c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Los modelos esperan un lote de entradas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e387873-9395-4dd2-a855-288252560cdf",
   "metadata": {},
   "source": [
    "En lo cuadernos anteriores entregamos secuencias de palabras al tokenizador. Pero los modelos esperan lotes de secuecias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7e0f2-4a2d-4d9c-bcb8-9f605a0d247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tranformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncase-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4047871-a1a2-44c9-b8ec-54e1ba1c3b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
