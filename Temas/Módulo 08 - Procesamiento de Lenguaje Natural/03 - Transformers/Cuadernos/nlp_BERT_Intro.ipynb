{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Diplomado en Inteligencia Artificial y Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"><center>BERT: el estado del arte en PLN</center></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/BERT-EL-NUEVO-ALGORITMO-DE-GOOGLE.jpg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-Transformer</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Bert el nuevo algoritmo de Google](https://blog.sinapsis.agency/bert-el-nuevo-algoritmo-de-google/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Camilo José Torres Jiménez, Msc, cjtorresj@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com\n",
    "5. Jessica López Mejía, jelopezme@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Jefe Jurídica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Paula Andrea Guzmán, guzmancruz.paula@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Coordinador Jurídico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. David Fuentes, fuentesd065@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Desarrolladores Principales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Dairo Moreno, damoralesj@unal.edu.co\n",
    "9. Joan Castro, jocastroc@unal.edu.co\n",
    "10. Bryan Riveros, briveros@unal.edu.co\n",
    "11. Rosmer Vargas, rovargasc@unal.edu.co\n",
    "12. Venus Puertas, vpuertasg@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Expertos en Bases de Datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Giovvani Barrera, udgiovanni@gmail.com\n",
    "14. Camilo Chitivo, cchitivo@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Devlin et. al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, mayo 2019](https://arxiv.org/pdf/1810.04805.pdf).\n",
    "1. [Montenegro y Montenegro, Aprendizaje Profundo-Diplomado, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Montenegro y Montenegro, Aprendizaje Profundo-PLN, 2022](https://github.com/AprendizajeProfundo/PLN)\n",
    "1. [Vaswani et al.,   Attention Is All You Need, diciembre 2017](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "1. [Dennis Rothman, Transformers for Natural Language processing, 2021](http://libgen.rs/search.php?req=Transformers+for+Natural+Language+processing&open=0&res=25&view=simple&phrase=1&column=def)\n",
    "1. [ Varios,  Dive into deep learning, , enero 2021](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "* [Introducción a BERT](#Introducción-a-BERT)\n",
    "* [BERT-Input](#BERT-Input)\n",
    "* [BERT-Entrenamiento Cloze task](#BERT-Entrenamiento-Cloze-task)\n",
    "* [BERT-Entrenamiento predicción siguiente sentencia](#BERT-Entrenamiento-predicción-siguiente-sentencia)\n",
    "* [BERT-Transferencia de conocimiento](#BERT-Transferencia-de-conocimiento)\n",
    "* [Tokenizadores usados con Transformers](#Tokenizadores-usados-con-Transformers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción a BERT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la implementación de HuggingFace in en Pytorch. \n",
    "\n",
    "+ BERT es un modelo con incrustaciones de posición absoluta, por lo que generalmente se recomienda rellenar (padding) las entradas a la derecha en lugar de a la izquierda.\n",
    "\n",
    "+ BERT tiene la estructura básica del codificador del transformer, con más cabezas, más capas y tres embedding posicionales que son aprendidos en el entrenamiento.\n",
    "\n",
    "+ BERT fue entrenado con el modelado de lenguaje enmascarado (MLM) y los objetivos de predicción de la siguiente oración (NSP). Es eficiente para predecir tokens enmascarados y en NLU en general, pero no es óptimo para la generación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Arquitectura modular del modelo Transformer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la arquitectura del modelo del Transformer, definimos los bloques de construcción de la arquitectura del Transformer original. Podemos pensar en el Transformer original  como un  modelo construido con ladrillos LEGO®. El conjunto de construcción contiene ladrillos como \n",
    "\n",
    "+ codificadores, \n",
    "+ decodificadores, \n",
    "+ capas de incrustación, \n",
    "+ codificación posicional\n",
    "+ capas de atención de múltiples cabezas, \n",
    "+ capas de atención de múltiples cabezas enmascaradas, \n",
    "+ capa posterior hacia adelante\n",
    "+ normalización, \n",
    "+ subcapas de avance y \n",
    "+ capas de salida lineal. \n",
    "\n",
    "Los ladrillos vienen en varios tamaños y formas. Puede pasar horas construyendo todo tipo de modelos usando\n",
    "el mismo kit de construcción. Algunas construcciones solo requerirán algunos de los ladrillos. Otro\n",
    "construcciones agregarán una nueva pieza, al igual que cuando obtenemos ladrillos adicionales para\n",
    "un modelo construido con componentes LEGO®."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modelos BERT de ajuste fino</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT agrega al Transformer una nueva pieza de construcción: `una subcapa de atención multicabeza bireccional`.\n",
    "\n",
    "BERT significa `Bidirectional Encoder\n",
    "Representations from Transformers`. El modelo solamente usa bloques de codificación.\n",
    "\n",
    "El modelo fue introducido por Devlin et. al. en artículo [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) en octubre de 2018, aunque la última versión oubilcada en Arxiv es de mayo de 2019. Como el título indica, BERT no solamente un Transformer, sino que además se introduce la idea de modelo pre-entrenado. \n",
    "\n",
    "\n",
    "Lo que Devlin y sus colegas de Google desarrollaron fue un Transformer que entrenaron con una cantidad impresionante de datos provenientes de\n",
    "\n",
    "* BooksCorpus (800 millones de palabras) ([Zhu et al.,2015](https://arxiv.org/pdf/1506.06724.pdf)) \n",
    "* [Wikipedia](https://es.wikipedia.org/wiki/Wikipedia:Portada) en inglés (2.500 millones de palabras).\n",
    "\n",
    "Para Wikipedia extrajeron solo los pasajes de texto\n",
    "e ignoraron listas, tablas y encabezados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Arquitectura BERT original </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT es un Transformer codificador bidireccional multicapa basado en la implementación original descrita en [Vaswani et al.,   Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "Usaremos la misma notación de Devlin et. al.. Denotamos el número de capas\n",
    "(es decir, bloques de Transformers) como $L$, el tamaño oculto como $H$ (tamaño del emebedding de cada palabra), y el número de cabezas de auto-atención como $A$. Los autores reportaron resultados en dos tamaños de modelo:\n",
    "\n",
    "* **BERTBASE** (L = 12, H = 768, A = 12, Parámetros totales = 110M) y \n",
    "* **BERTLARGE** (L = 24, H = 1024,\n",
    "A = 16, Parámetros totales = 340M).\n",
    "\n",
    "\n",
    "BERTBASE fue elegido para tener el mismo modelo\n",
    "tamaño de OpenAI GPT para fines de comparación.\n",
    "Críticamente, sin embargo, el transformador BERT utiliza auto-atención bidireccional, mientras que el transformador GPT utiliza la auto-atención restringida donde cada token solo puede atender al contexto a su izquierda, de acuerdo con la arquitectura original de Vaswani et. al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">BERT-Input</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/BERT-Arquitectura.png\" width=\"680\" height=\"680\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-Arquitectura</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente:[BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">BERT-Entrenamiento Cloze task</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/BERT-Masket.png\" width=\"450\" height=\"450\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-predicción del token enmascarado</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente:[Finnish Language Modeling with Deep Transformer Models](https://arxiv.org/pdf/2003.11562.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">BERT-Entrenamiento predicción siguiente sentencia</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/BERT2sentence.jpeg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-Predicción de la siguiente sentencia/p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente:[geeksforgeeks](https://www.geeksforgeeks.org/next-sentence-prediction-using-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">BERT-Transferencia de conocimiento</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/BERT-TAREAS.jpeg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-Transferencia de conocimiento</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente:[BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Tokenizadores usados con Transformers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tokenizadores constituyen una parte clave del entubamiento de PLN. La traea de un tokenizador es transformar un texto en datos que pueden ser procesados por el modelo.\n",
    "\n",
    "Los modelos solamente procesan números. Así que un tokenizador transforma un texto en bruto (*raw text*) en una secuencia de números."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenizadores basados en palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente vamos a pensar en tokenización basada en palabras (*word-based*). La siguiente imagen ilustra le meta inicial de un tokenizador: dividir el texto bruto en palabras (o subpalabras) para encontrar una representación numérica del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/word_based_tokenization.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Ejemplos de tokenización por palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [HuggingFace Transformers course](https://huggingface.co/course/chapter2/4?fw=tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La división del texto en palabras puede hacerse directamente con la función **split** de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daniel', 'es', 'un', 'profesor', 'de', 'inteligencia', 'artificial']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \" Daniel es un profesor de inteligencia artificial\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenización basada en caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el otro extremo se encuentran los tokenizadores basados en caracteres. En general estos generan vocabularios mucho mas cortos. En este caso aparecen problemas diferentes a los tokenizadores por palabras. A diferencia de la tokenización por  palabras en donde existen similaridades entre ellas, en la tokenización por caracteres, tal similaridad se pierde. Además algunos problemas aparecen, relacionados con el manejo de la puntuación. \n",
    "\n",
    "Sin  embargo esto difiere entre los diferente lenguajes naturales. Por ejemplo en chino, cada caracter carga más información que un caracter en un  lenguaje latino. La siguiente imagen ilustra una tokenización por caracteres latinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/character_based_tokenization.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Ejemplos de tokenización por caracteres</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [HuggingFace Transformers course](https://huggingface.co/course/chapter2/4?fw=tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenización basada en subpalabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tipo de tokenizadores se toma lo mejor de los dos mundos anteriores. La tokenización basada en subpalabras descansa en el principio de que *palabras frecuentemente usadas no deben subdividirse más, pero palabras menos frecuentes pueden eventualmente ser subdivididas en subpalabras cons significado. Dos ejemplos de tokenización basados en subpalabras son los siguientes.\n",
    "\n",
    "1. Las palabras *perro* y *perros* son diferentes. Pero es claro que se refieren a lo mismo. Si se codifican separadamente con ID's distintos, esta similaridad natural se pierde. Una solución que puede adoptarse es por ejemplo codificar las palabras 'perro' y 's'. por separado. Un tokenizador entrenado de esta forma haría la tokenización de las dos palabras de la siguiente forma:\n",
    "\n",
    "    * 'perro' -> {'perro'} -> {{35}}\n",
    "    * 'perros' -> {'perro', 's'} -> {{35}, {60}}\n",
    "    * 'casa' -> {'casa'} -> {{85}}\n",
    "    * 'casas' -> {'casa', 's'} -> {{85}, {60}}\n",
    "    \n",
    "Observe que 'perros' y 'casas' comparten el hecho de ser palabras plurales.\n",
    "\n",
    "2. Las palabras *bailar*, *bailamos*, *bailaríamos*, *bailaremos* se tokenizarían como\n",
    "    * 'bailar* -> {'baila', 'r'} -> {{110},{59}}\n",
    "    * 'bailamos* -> {'baila', 'mos'} -> {{110},{90}}\n",
    "    * 'bailaríamos* -> {'baila', 'ríamos'} -> {{110},{95}}\n",
    "    * 'bailaremos* -> {'baila', 'remos'} -> {{110},{98}}\n",
    "    \n",
    "Así, sucesivamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenización basada en otros enfoques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Existen muchas más técnicas. Por ejemplo:\n",
    "\n",
    "1. Byte-level, usado en [GPT-2](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "1. WordPiece, usado en [BERT](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "1. SentencePiece o Unigram, usado en varios modelos multilingua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/word_based_tokenization.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">BERT-word-piece tokenizer</p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
