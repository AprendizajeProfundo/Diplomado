{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5074df5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Inteligencia Artificial y Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5c892",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Canalización  de datos. La API tf.data</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0539598",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872cb6",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed2fad",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f4142",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec4da7",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98590fdc",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707016b5",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8edd0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d3f523d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad97642",
   "metadata": {},
   "source": [
    "Basado en [tf.data](https://www.tensorflow.org/guide/data).\n",
    "\n",
    "La API `tf.data` permite crear tuberías de entrada complejas a partir de piezas simples y reutilizables. Por ejemplo, la canalización de un modelo de imagen podría agregar datos de archivos en un sistema de archivos distribuido, aplicar perturbaciones aleatorias a cada imagen y fusionar imágenes seleccionadas al azar en un lote para entrenamiento. La canalización de un modelo de texto puede implicar extraer símbolos de datos de texto sin procesar, convertirlos en identificadores incrustados con una tabla de búsqueda y agrupar secuencias de diferentes longitudes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de6549",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Importa librerías</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f41e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc78a78",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Esenciales</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e38b26",
   "metadata": {},
   "source": [
    "Para crear una canalización de entrada, debe comenzar con una fuente de datos. Por ejemplo, para construir un `Dataset` de datos a partir de datos en la memoria, puede usar *tf.data.Dataset.from_tensors()* o *tf.data.Dataset.from_tensor_slices()*. Alternativamente, si sus datos de entrada están almacenados en un archivo en el formato *TFRecord* de TensorFlow puede usar *tf.data.TFRecordDataset()*.\n",
    "\n",
    "El objeto Dataset es un iterable de Python. Esto hace posible consumir sus elementos usando un bucle for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7c6414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee51191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce27eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce0753",
   "metadata": {},
   "source": [
    "o se pueden crear explícitamente un iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cddee075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "print(next(it).numpy())\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9df2a",
   "metadata": {},
   "source": [
    "### Consumo de datos usando reducción: reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36c0ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(dataset.reduce(0, lambda state, value: state+value).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acccba",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Estructura del conjunto de datos</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecb904",
   "metadata": {},
   "source": [
    "Un conjunto de datos produce una secuencia de elementos , donde cada elemento tiene la misma estructura (anidada) de componentes . \n",
    "\n",
    "Los componentes individuales de la estructura pueden ser de cualquier tipo representable por *tf.TypeSpec*, incluidos *tf.Tensor* , *tf.sparse.SparseTensor* ,*tf.RaggedTensor* , *tf.TensorArray* o *tf.data.Dataset*.\n",
    "\n",
    "Las construcciones de Python que se pueden usar para expresar la estructura (anidada) de elementos incluyen *tuple , dict , NamedTuple y OrderedDict*. \n",
    "\n",
    "En particular, *list* no es una construcción válida para expresar la estructura de los elementos del conjunto de datos. \n",
    "\n",
    "Si desea que una entrada de *list* se trate como una estructura, debe convertirla en tuple y si desea que una lista de salida, entonces debe empaquetarla explícitamente usando *tf.stack*.\n",
    "\n",
    "\n",
    "La propiedad *Dataset.element_spec* permite inspeccionar el tipo de cada componente del elemento. La propiedad devuelve una estructura anidada de objetos *tf.TypeSpec*, que coincide con la estructura del elemento, que puede ser un solo componente, una tupla de componentes o una tupla anidada de componentes. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9d8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (10,), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b1ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4269 0.2259 0.1341 0.0592 0.3902 0.9653 0.2403 0.403  0.92   0.8353]\n",
      "[0.3683 0.1005 0.1756 0.6626 0.6547 0.6166 0.8312 0.1788 0.7246 0.8445]\n",
      "[0.1888 0.1275 0.0228 0.4373 0.0738 0.8916 0.8793 0.9238 0.2597 0.1893]\n",
      "[0.8314 0.8599 0.6729 0.3692 0.6338 0.9502 0.5941 0.3125 0.5312 0.3395]\n"
     ]
    }
   ],
   "source": [
    "for i in dataset1:\n",
    "    print(i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e91392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3b2fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]),\n",
    "     tf.random.uniform([4,100], maxval=100, dtype=tf.int32)))\n",
    "\n",
    "dataset2.element_spec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9948d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f1759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(10,), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e7e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b70e98c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.ZipDataset"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09644a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6734eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
      "array([0.4269, 0.2259, 0.1341, 0.0592, 0.3902, 0.9653, 0.2403, 0.403 ,\n",
      "       0.92  , 0.8353], dtype=float32)>, (<tf.Tensor: shape=(), dtype=float32, numpy=0.32003403>, <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([13, 26, 50, 52, 84,  8,  3,  6, 25, 50, 35, 25, 25, 80, 25, 97, 90,\n",
      "       91, 77, 17, 89, 82, 32,  1,  4, 93, 43, 39, 48, 38, 12, 23, 10, 98,\n",
      "       30, 33, 79, 46, 51, 94, 60, 94, 89, 28,  4, 92, 88, 91, 32, 63, 17,\n",
      "       80, 18, 15, 52, 29, 67,  2, 85, 94, 24, 95, 89, 15, 37, 77, 54,  8,\n",
      "       99, 15, 56, 28, 82, 95, 91, 32, 37, 36, 52, 63,  1, 32, 50, 90, 17,\n",
      "       88, 26, 16, 22, 65, 50, 93, 20, 58, 86, 56, 26, 17, 81, 82],\n",
      "      dtype=int32)>)) \n",
      "\n",
      "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
      "array([0.3683, 0.1005, 0.1756, 0.6626, 0.6547, 0.6166, 0.8312, 0.1788,\n",
      "       0.7246, 0.8445], dtype=float32)>, (<tf.Tensor: shape=(), dtype=float32, numpy=0.34353054>, <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([70,  5, 72, 38, 24, 25,  6, 74, 46, 47, 94, 89, 58, 26, 79, 86, 70,\n",
      "        7, 88, 88, 99, 93, 13, 52, 99,  8, 59, 74,  5, 63, 53, 76, 99,  4,\n",
      "        4, 10, 28, 20, 27, 93, 36, 19, 60, 11, 54, 34, 36,  7, 20, 55, 66,\n",
      "       57, 54, 77, 95, 37, 76, 85, 37, 48, 41, 54, 26, 84, 12, 57, 73, 61,\n",
      "       96, 60, 25, 80, 74, 47, 59, 62,  3, 68, 13, 65, 79, 64,  3, 99, 71,\n",
      "       40,  5, 48, 50, 62,  1, 77, 43,  3, 20, 53, 85, 26, 93, 66],\n",
      "      dtype=int32)>)) \n",
      "\n",
      "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
      "array([0.1888, 0.1275, 0.0228, 0.4373, 0.0738, 0.8916, 0.8793, 0.9238,\n",
      "       0.2597, 0.1893], dtype=float32)>, (<tf.Tensor: shape=(), dtype=float32, numpy=0.5671661>, <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([ 8, 81, 97, 28, 60, 96,  5, 19, 66, 34, 99, 36, 96, 47, 16, 31,  6,\n",
      "       76, 44, 82, 83, 70, 90, 42, 39, 31, 89, 59, 94, 73, 12, 38, 83, 38,\n",
      "        3, 83, 91, 87, 38, 31, 37, 84, 33, 75,  2, 64, 20, 26, 87, 86, 69,\n",
      "       27, 66, 25,  5, 37, 22, 69, 79, 77, 60,  8, 64, 48, 18, 99, 14, 31,\n",
      "       45, 32, 84,  9, 73, 50, 41, 59,  0,  7,  3,  6, 12,  7, 10, 59, 53,\n",
      "       86, 45, 12, 93, 49, 49, 95, 24, 42, 31, 22, 20, 82, 89, 73],\n",
      "      dtype=int32)>)) \n",
      "\n",
      "(<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
      "array([0.8314, 0.8599, 0.6729, 0.3692, 0.6338, 0.9502, 0.5941, 0.3125,\n",
      "       0.5312, 0.3395], dtype=float32)>, (<tf.Tensor: shape=(), dtype=float32, numpy=0.88297546>, <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([16, 13, 75, 62,  2, 21, 35,  9, 50, 34, 14, 89, 46, 10, 15,  6, 26,\n",
      "       70, 67, 82, 11, 65, 80, 64, 70, 73, 41, 34, 17, 22, 38, 86, 19, 84,\n",
      "       99, 22, 52, 52, 92, 12, 89, 50, 32, 36, 12, 94, 80, 72, 95, 34, 89,\n",
      "       10, 38,  2, 42, 48, 50, 20, 78, 68, 64,  4, 57, 48, 16, 17,  5, 23,\n",
      "       56, 33, 15, 62, 93, 90, 27, 59, 82, 69,  6, 57,  2, 70, 93, 50, 74,\n",
      "       26, 82, 43,  5, 24, 20, 26, 17, 76, 50, 10, 51, 83, 18, 44],\n",
      "      dtype=int32)>)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(i.next(), \"\\n\")\n",
    "print(i.next(), \"\\n\")\n",
    "print(i.next(), \"\\n\")\n",
    "print(i.next(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9c6428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensorSpec(TensorShape([3, 4]), tf.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset con tensores dispersos\n",
    "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0],[1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
    "dataset4.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ebc87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.sparse_tensor.SparseTensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset4.element_spec.value_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71453ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n"
     ]
    }
   ],
   "source": [
    "for a, (b,c) in dataset3:\n",
    "    print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbde1c0",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Leer datos de entrada</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bad6cc",
   "metadata": {},
   "source": [
    "### Consumir matrices Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10878a",
   "metadata": {},
   "source": [
    "Si todos sus datos de entrada caben en la memoria, la forma más sencilla de crear un Dataset a partir de ellos es convertirlos en objetos tf.Tensor y usar Dataset.from_tensor_slices() ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c3b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daf3e99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenes, labels  = train\n",
    "imagenes = imagenes /255.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((imagenes, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554deffb",
   "metadata": {},
   "source": [
    "### Consumir generadores de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c676a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def count(stop):\n",
    "    i=0\n",
    "    while i<stop:\n",
    "        yield i\n",
    "        i+= 1\n",
    "        \n",
    "for n in count(5):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb1ed9",
   "metadata": {},
   "source": [
    "El constructor `Dataset.from_generator` convierte el generador de Python en un `tf.data.Dataset` completamente funcional.\n",
    "\n",
    "El constructor toma un invocable como entrada, no un iterador. Esto le permite reiniciar el generador cuando llega al final. Toma un argumento args opcional, que se pasa como argumentos del invocable.\n",
    "\n",
    "El argumento *output_types* es necesario porque *tf.data* crea un *tf.Graph* internamente y los bordes del gráfico requieren un tf.dtype .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1919e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes=(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85aaefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d3b1f",
   "metadata": {},
   "source": [
    "El argumento `output_shapes` no es necesario, pero se recomienda, ya que muchas operaciones de flujo tensorial no admiten tensores con rango desconocido. Si la longitud de un eje en particular es desconocida o variable, output_shapes puede colcarse como None.\n",
    "\n",
    "También es importante tener en cuenta que `output_shapes` y `output_types` siguen las mismas reglas de anidamiento que otros métodos de conjuntos de datos.\n",
    "\n",
    "Aquí hay un generador de ejemplo que demuestra ambos aspectos, devuelve tuplas de matrices, donde la segunda matriz es un vector con longitud desconocida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a648a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_series():\n",
    "    i = 0\n",
    "    while True:\n",
    "        size = np.random.randint(0,10)\n",
    "        yield i, np.random.normal(size = (size,))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "042af4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [-2.0768 -1.0769 -0.607  -1.2759 -0.5384  0.5257  0.3877  1.434   0.0673]\n",
      "1 : [-1.2147  0.2586 -0.4508  0.2487 -0.0622]\n",
      "2 : [-2.2237  0.1287  0.6476 -0.3983]\n",
      "3 : [ 0.7714  1.9097 -0.6291  1.146  -0.8916 -0.7287  0.9366]\n",
      "4 : [ 0.1155  0.0809  0.242  -0.8083 -0.2963 -1.691 ]\n",
      "5 : [ 0.5732 -0.9829 -0.189   0.7797]\n",
      "6 : [ 0.3613  0.0955  0.1289  0.143   0.4906 -0.6897  1.6203  0.9932 -0.9631]\n"
     ]
    }
   ],
   "source": [
    "for i, series in gen_series():\n",
    "    print(i, \":\", str(series))\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8ae22",
   "metadata": {},
   "source": [
    "La primera salida es un *int32* la segunda es un *float32*.\n",
    "\n",
    "El primer elemento es un escalar, forma () , y el segundo es un vector de longitud desconocida, forma (None,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdca507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((), (None,)), types: (tf.int32, tf.float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_series = tf.data.Dataset.from_generator(\n",
    "    gen_series,\n",
    "    output_types=(tf.int32, tf.float32),\n",
    "    output_shapes=((), (None, )))\n",
    "\n",
    "ds_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcc2ad",
   "metadata": {},
   "source": [
    "Ahora se puede utilizar como un *tf.data.Dataset* normal. Tenga en cuenta que al procesar por lotes un conjunto de datos con una forma variable, debe usar *Dataset.padded_batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3a31b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 18 19  5 20 10 16 24 13 22]\n",
      "\n",
      "[[ 0.3074  0.3081  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.3368 -0.2093  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.5042 -1.4457  0.1762 -0.1034 -0.9581 -0.7326  0.      0.      0.    ]\n",
      " [-0.3319 -0.0889 -0.6708  1.345   0.6263 -0.2858  0.      0.      0.    ]\n",
      " [ 0.4582 -1.589  -0.5823 -0.7809 -0.7744  1.2009 -1.8654 -1.1426 -0.4586]\n",
      " [-0.6649 -1.3044 -0.9678  0.1575 -0.3356 -0.2489  0.      0.      0.    ]\n",
      " [ 2.4825  0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.9283 -0.8629 -0.1599 -0.4868 -1.0403 -2.613  -0.1975  0.      0.    ]\n",
      " [-1.1349  0.9815  0.      0.      0.      0.      0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "ds_series_batch = ds_series.shuffle(20).padded_batch(10)\n",
    "\n",
    "ids, sequence_batch = next(iter(ds_series_batch))\n",
    "\n",
    "print (ids.numpy())\n",
    "print()\n",
    "print(sequence_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a9a3535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 19 18  6  0  9 22 20 27 13]\n",
      "\n",
      "[[ 0.0938 -0.1603 -0.7027 -0.7624  0.      0.      0.      0.      0.    ]\n",
      " [ 0.8982  1.2909 -1.274   0.8219  0.427   0.0669 -1.2219  2.1049  0.3012]\n",
      " [-0.5357 -0.3188 -0.2439  0.7155 -0.4688 -1.2357  1.5803  0.      0.    ]\n",
      " [ 1.0053  0.3697  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.9785 -1.0723  0.5539  0.0298  0.8192 -2.1612  0.3339  0.      0.    ]\n",
      " [-1.6389  1.2007 -1.0191 -0.4484  0.6041  0.5507  0.332   0.      0.    ]\n",
      " [ 0.3613 -1.2548 -0.3565 -0.2443 -0.4117  0.      0.      0.      0.    ]\n",
      " [-0.2451 -0.2993  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.1213  1.2395  1.1652 -1.4538  0.1137  0.5088  0.      0.      0.    ]\n",
      " [ 0.4269 -1.5848 -0.8784 -0.8858  1.0743  0.9459  0.      0.      0.    ]]\n",
      "\n",
      "[15 21  2 10  8 14 31 12  1  3]\n",
      "\n",
      "[[ 0.2356 -0.486  -1.65    0.612   1.1179 -0.9846  0.      0.      0.    ]\n",
      " [ 1.3357 -1.4823 -0.6392 -0.3135  0.7844  0.8805 -0.4989  0.      0.    ]\n",
      " [ 0.4366 -0.6321  1.0615  0.1411  0.      0.      0.      0.      0.    ]\n",
      " [ 0.4207 -1.056  -1.0102 -0.4138  0.8201 -0.4991  0.      0.      0.    ]\n",
      " [-1.2623 -0.5359 -0.8157  0.5334  0.9962  0.      0.      0.      0.    ]\n",
      " [-0.8954 -0.1347 -0.7888  0.      0.      0.      0.      0.      0.    ]\n",
      " [ 1.1904 -0.971   1.224   0.1556  0.      0.      0.      0.      0.    ]\n",
      " [ 0.2138  0.2268 -0.5066  0.3458 -0.4304 -0.9001 -0.0213 -0.0852  1.2321]\n",
      " [ 0.3176  0.5251  0.1809  0.6761  0.9922  0.      0.      0.      0.    ]\n",
      " [-0.0476 -0.4505  0.8337 -0.5385  1.4075 -1.1674  0.      0.      0.    ]]\n",
      "\n",
      "[29 32 33 30  5 39  4 35 24 42]\n",
      "\n",
      "[[-0.467  -0.0357 -0.5347 -1.4737  2.2324  0.2198  0.4467  0.      0.    ]\n",
      " [-0.1247  0.263   2.0202  0.625   0.2308  0.3823  0.      0.      0.    ]\n",
      " [-0.9247 -1.0643 -0.5837  0.053  -0.3179  0.      0.      0.      0.    ]\n",
      " [-1.1201 -1.0768 -1.9706 -1.5015  0.1517 -0.528   1.0977 -1.3657  0.    ]\n",
      " [-0.8059  0.4646  0.0464 -0.8236  0.5782  0.7445  0.9643 -0.0559  1.1125]\n",
      " [ 1.5065  0.7637  1.1742 -1.5854  0.      0.      0.      0.      0.    ]\n",
      " [ 0.7142 -0.9123 -0.5514 -0.5275 -1.1773 -0.687   1.43    0.6081  0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.6843  0.5089  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]]\n",
      "\n",
      "[41 37 43 48 52 47  7 34 53 36]\n",
      "\n",
      "[[ 0.639   0.      0.      0.      0.      0.      0.    ]\n",
      " [-1.1417  0.9354  2.5507  0.2327  0.      0.      0.    ]\n",
      " [-0.7189  0.0248 -0.9836  0.      0.      0.      0.    ]\n",
      " [ 0.2823 -0.1755  1.0275 -0.3293 -2.8161  0.2561 -0.0201]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.5232  1.3861  0.7089  0.61   -0.9355  0.      0.    ]\n",
      " [-0.7106 -0.3189 -0.3574  0.3599 -0.4692 -1.8298  0.4833]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.1738  0.1086  0.9455 -0.1833 -1.4567  0.2467  0.    ]\n",
      " [ 1.6965 -0.4141 -0.9743 -0.4285  0.1616  0.1273 -0.3633]]\n",
      "\n",
      "[46 59 44 45 11 63 54 60 57 61]\n",
      "\n",
      "[[ 0.9948 -0.7678 -1.285   0.0923  0.0254 -1.4941  0.      0.      0.    ]\n",
      " [-0.5962 -0.6967 -1.5647 -0.5218  0.      0.      0.      0.      0.    ]\n",
      " [ 2.7032 -0.4256  0.2997  0.5117  0.0241  1.3504  1.6363 -0.8216  0.    ]\n",
      " [-0.1627  2.541  -0.6672  1.8093 -1.5332  0.      0.      0.      0.    ]\n",
      " [-0.9318  0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.6308 -1.0087  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.5711  1.0256  0.5678  0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.2277  1.2378 -1.4039  0.2264  0.      0.      0.      0.      0.    ]\n",
      " [-0.2518 -0.1307  0.8069  0.7368 -0.0607 -0.6001  1.3164  0.3858 -2.1493]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]]\n",
      "\n",
      "[65 28 66 58 49 73 50 69 74 70]\n",
      "\n",
      "[[-0.6872  0.5025  0.9198 -0.5524 -0.0216 -0.4376  0.2244 -0.8829  0.    ]\n",
      " [ 0.1599 -0.2033  1.463  -1.121  -0.6675 -0.0764  0.3533  0.      0.    ]\n",
      " [-2.3076  0.1051  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.4383  1.0663  0.0092 -0.4025 -2.472   1.1039 -1.3267 -1.3829  0.7638]\n",
      " [-0.5834 -1.9723 -0.6184  0.9552  0.0432  0.      0.      0.      0.    ]\n",
      " [-1.5185  1.2408  0.7817 -0.9546 -0.1122  0.      0.      0.      0.    ]\n",
      " [ 0.8129  1.1332  2.1041  0.3564  0.2183  0.      0.      0.      0.    ]\n",
      " [-0.7107 -0.0253 -0.2071  0.      0.      0.      0.      0.      0.    ]\n",
      " [-1.2362  1.6983 -1.0704  0.7336 -0.6958 -0.0943  0.6855 -0.0764  0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]]\n",
      "\n",
      "[56 77 40 71 51 79 62 84 87 68]\n",
      "\n",
      "[[ 0.556   1.9623  0.5596 -0.7303  0.      0.      0.      0.      0.    ]\n",
      " [ 0.3839 -0.2474  0.0147 -1.9246 -1.8855 -0.772   1.7581 -0.2693 -1.0247]\n",
      " [ 1.2893  0.7191 -0.2629  0.      0.      0.      0.      0.      0.    ]\n",
      " [-1.5552 -2.6982 -1.7221 -0.0291  2.8813  0.      0.      0.      0.    ]\n",
      " [ 0.3078 -0.0466  0.1985  0.      0.      0.      0.      0.      0.    ]\n",
      " [ 1.0374 -0.8316  1.2988  0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.5589  1.6932 -1.5481  0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.46    0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.4189  0.2651  0.5997  1.1556  0.3746 -1.5029  0.7356 -0.6256  0.    ]\n",
      " [ 0.2929  0.7778  0.7741 -0.1986 -1.1999 -0.3131 -0.5341  0.      0.    ]]\n",
      "\n",
      "[38 26 17 76 64 82 95 86 89 23]\n",
      "\n",
      "[[ 0.2212  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.5779  2.0797  1.0246  0.      0.      0.      0.      0.    ]\n",
      " [ 0.9231 -0.0648 -1.4679 -1.4414  0.5852 -0.8951  0.7999  1.0216]\n",
      " [-2.8572 -0.6151  0.4835  1.11   -0.5344  0.7747  0.7364 -0.014 ]\n",
      " [-0.3519 -0.4013  1.176   0.9122 -0.0953  0.      0.      0.    ]\n",
      " [ 0.5485  0.643   0.5775 -1.039   0.3515  0.9663  0.0565  0.0132]\n",
      " [ 1.9781  0.7976 -0.6003  0.      0.      0.      0.      0.    ]\n",
      " [-0.5299  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.    ]]\n",
      "\n",
      "[25 93 81 75 97 80 99 98 96 92]\n",
      "\n",
      "[[ 0.6918 -0.1636  0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.2697  1.6629  2.2597 -0.0599 -0.1087  0.776   0.4276 -0.7616]\n",
      " [-0.5939  0.8137  0.4668 -1.1511  0.      0.      0.      0.    ]\n",
      " [-0.4646 -1.0311  0.141   1.5927  0.      0.      0.      0.    ]\n",
      " [ 0.7093  0.9063 -0.0424 -0.5233  0.      0.      0.      0.    ]\n",
      " [-0.3806  1.1682  0.4596 -0.8767 -1.4929 -0.2311 -1.0071  2.5639]\n",
      " [-0.8753  2.156   1.0263 -1.4651 -0.84   -1.1335  0.2586  0.2292]\n",
      " [ 0.0443 -0.0923 -1.1114  0.2491  0.041   0.      0.      0.    ]\n",
      " [-0.8715  0.9976  0.      0.      0.      0.      0.      0.    ]\n",
      " [-1.5354 -0.2843  0.      0.      0.      0.      0.      0.    ]]\n",
      "\n",
      "[107  55  94  88 101 100 102 106  72  90]\n",
      "\n",
      "[[-0.6622  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.9447  1.464  -0.4598  0.9879  0.      0.      0.      0.    ]\n",
      " [ 0.8634  0.206   1.2903 -2.0079  0.4742  0.8593  0.587  -0.4128]\n",
      " [-0.2753  0.5119  0.7697  0.      0.      0.      0.      0.    ]\n",
      " [-0.0549  0.3254 -0.9575 -1.3798  0.4681 -0.6249 -0.7518  0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 2.1137  0.1059  0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.4828 -1.7773  0.2514 -0.099  -0.585   0.5443  0.      0.    ]\n",
      " [ 0.6679  0.3791  0.      0.      0.      0.      0.      0.    ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = iter(ds_series_batch)\n",
    "for i in range(10):\n",
    "    ids, sequence_batch = next(it)\n",
    "    print (ids.numpy())\n",
    "    print()\n",
    "    print(sequence_batch.numpy())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbb823",
   "metadata": {},
   "source": [
    "### Ejemplo realista con imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814a393",
   "metadata": {},
   "source": [
    "Para obtener un ejemplo más realista, intente `tf.data.Dataset` `preprocessing.image.ImageDataGenerator` como un `tf.data.Dataset` .\n",
    "\n",
    "Primero descargue los datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f530088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228818944/228813984 [==============================] - 35s 0us/step\n"
     ]
    }
   ],
   "source": [
    "flowers = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd776d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thejarmanitor/.keras/datasets/flower_photos\n"
     ]
    }
   ],
   "source": [
    "print(flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115a5e3",
   "metadata": {},
   "source": [
    "Cree la `image.ImageDataGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e17aa219",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beffb9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(image_gen.flow_from_directory(flowers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1540deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (32, 256, 256, 3)\n",
      "float32 (32, 5)\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4340c8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    lambda: image_gen.flow_from_directory(flowers),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([32,256,256,3],[32,5]))\n",
    "\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb1008",
   "metadata": {},
   "source": [
    "### Consumir datos de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f175f",
   "metadata": {},
   "source": [
    "Muchos conjuntos de datos se distribuyen como uno o más archivos de texto. `tf.data.TextLineDataset` proporciona una manera fácil de extraer líneas de uno o más archivos de texto. \n",
    "\n",
    "Dados uno o más nombres de archivo, un `TextLineDataset` producirá un elemento con valor de cadena por línea de esos archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c9412ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
      "819200/815980 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
      "811008/809730 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
      "811008/807992 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "file_paths = [\n",
    "    tf.keras.utils.get_file(file_name, directory_url +file_name)\n",
    "    for file_name in file_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f65d97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/thejarmanitor/.keras/datasets/cowper.txt',\n",
       " '/home/thejarmanitor/.keras/datasets/derby.txt',\n",
       " '/home/thejarmanitor/.keras/datasets/butler.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a17cdf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52876af3",
   "metadata": {},
   "source": [
    "Estas son las primeras líneas del primer archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67cc19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Illustrious into Ades premature,'\n",
      "b'And Heroes gave (so stood the will of Jove)'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554efa1a",
   "metadata": {},
   "source": [
    "Para alternar líneas entre archivos, use `Dataset.interleave` . Esto facilita la reproducción aleatoria de archivos. Aquí están la primera, segunda y tercera líneas de cada traducción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e53620c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ds = tf.data.Dataset.from_tensor_slices(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2edab625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/thejarmanitor/.keras/datasets/cowper.txt'\n",
      "b'/home/thejarmanitor/.keras/datasets/derby.txt'\n",
      "b'/home/thejarmanitor/.keras/datasets/butler.txt'\n"
     ]
    }
   ],
   "source": [
    "for i in file_ds: print(i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d28b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\"\n",
      "b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'\n",
      "\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b'The vengeance, deep and deadly; whence to Greece'\n",
      "b'countless ills upon the Achaeans. Many a brave soul did it send'\n",
      "\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Unnumbered ills arose; which many a soul'\n",
      "b'hurrying down to Hades, and many a hero did it yield a prey to dogs and'\n"
     ]
    }
   ],
   "source": [
    "line_ds = file_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
    "\n",
    "for i, line in enumerate(line_ds.take(9)):\n",
    "    if i%3 ==0:\n",
    "        print()\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a1dab",
   "metadata": {},
   "source": [
    "De manera predeterminada, `TextLineDataset` produce todas las lineas de cada archivo, lo cual tal vez no sea lo que se quiera. Tal vez el archivo empieza con el encabezado, o contiene comentarios. Para remover o pasarse estas lineas se usan las transformaciones `Dataset.skip()` o `Dataset.filter()`\n",
    "\n",
    "\n",
    "A continuación, trabajamos con el archivo de la tragedia del Titanic. Se salta la primera linea, y filtramos para tener solo a los sobrevivientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17ae3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "32768/30874 [===============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f7140fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'\n",
      "b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'\n",
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'\n",
      "b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in titanic_lines.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e3fae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def survived(line):\n",
    "    return tf.not_equal(tf.strings.substr(line,0,1), '0')\n",
    "\n",
    "survivors=titanic_lines.skip(1).filter(survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "849225ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n",
      "b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'\n",
      "b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'\n",
      "b'1,male,28.0,0,0,35.5,First,A,Southampton,y'\n",
      "b'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in survivors.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214389a",
   "metadata": {},
   "source": [
    "### Consumir Datos CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434499f",
   "metadata": {},
   "source": [
    "El formato CSV es muy popular para guardar datos tabulares en forma de texto.\n",
    "\n",
    "Ya subimos el archivo del titanic, el cual es csv. Podemos subirlo en este mismo formato usando pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2691cf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(titanic_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d8fd9",
   "metadata": {},
   "source": [
    "Si se tiene suficiente memoria, pueden transformar a diccionario el Dataframe e importar los datos con facilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7069794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'survived'          : 0\n",
      "  'sex'               : b'male'\n",
      "  'age'               : 22.0\n",
      "  'n_siblings_spouses': 1\n",
      "  'parch'             : 0\n",
      "  'fare'              : 7.25\n",
      "  'class'             : b'Third'\n",
      "  'deck'              : b'unknown'\n",
      "  'embark_town'       : b'Southampton'\n",
      "  'alone'             : b'n'\n"
     ]
    }
   ],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "for feature_batch in titanic_slices.take(1):\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c29a39",
   "metadata": {},
   "source": [
    "Un acercamiento más ameno es cargar desde el disco cuando sea necesario.\n",
    "\n",
    "el modulo tiene métodos para extraer rgistros de uno o más archivos CSV que cumplan con la [RFC 4180](https://tools.ietf.org/html/rfc4180)\n",
    "\n",
    "la función `experimental.make_csv_dataset` es una interfaz para leer conjuntos de archivos CSV, con lo cual podemos hacer inferencia por columna y crear lotes de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d965f02",
   "metadata": {},
   "source": [
    "Se puede usar el argumento `select_columns` si solo se necesitan algunas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62531867",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name=\"survived\", select_columns=['class', 'fare', 'survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72675326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [0 0 0 0]\n",
      "  'fare'              : [7.775  7.8542 0.     8.05  ]\n",
      "  'class'             : [b'Third' b'Third' b'Second' b'Third']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "  print(\"'survived': {}\".format(label_batch))\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d292cff",
   "metadata": {},
   "source": [
    "### Consumir conjuntos de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b5127",
   "metadata": {},
   "source": [
    "Es normal que los datos estén distribuidos en múltiples archivos, con cada archivo teniendo ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6206b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "flowers_root = pathlib.Path(flowers_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4de1b",
   "metadata": {},
   "source": [
    "Cada directorio de la carpeta raíz contiene un directorio de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "138868e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunflowers\n",
      "daisy\n",
      "LICENSE.txt\n",
      "tulips\n",
      "roses\n",
      "dandelion\n"
     ]
    }
   ],
   "source": [
    "for item in flowers_root.glob(\"*\"):\n",
    "  print(item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80512b",
   "metadata": {},
   "source": [
    "Cada archivo en los directorios son ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d69529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/thejarmanitor/.keras/datasets/flower_photos/roses/534228982_4afbcece9b_m.jpg'\n",
      "b'/home/thejarmanitor/.keras/datasets/flower_photos/dandelion/19526570282_1d1e71b0f3_m.jpg'\n",
      "b'/home/thejarmanitor/.keras/datasets/flower_photos/roses/15222804561_0fde5eb4ae_n.jpg'\n",
      "b'/home/thejarmanitor/.keras/datasets/flower_photos/roses/3576488381_611f3446e0_n.jpg'\n",
      "b'/home/thejarmanitor/.keras/datasets/flower_photos/roses/2408236801_f43c6bcff2.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951794cb",
   "metadata": {},
   "source": [
    "usando `tf.io.read_file` podemos ler los datos y extraer las etiquetas, obteniendo (imagen, etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d231a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function process_path at 0x7fb944615a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function process_path at 0x7fb944615a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def process_path(file_path):\n",
    "  label = tf.strings.split(file_path, os.sep)[-2]\n",
    "  return tf.io.read_file(file_path), label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b94fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x03\\x02\\x02\\x03\\x02\\x02\\x03\\x03\\x03\\x03\\x04\\x03\\x03\\x04\\x05\\x08\\x05\\x05\\x04\\x04\\x05\\n\\x07\\x07\\x06\\x08\\x0c\\n\\x0c\\x0c\\x0b\\n\\x0b\\x0b\\r\\x0e\\x12\\x10\\r\\x0e\\x11\\x0e\\x0b\\x0b\\x10\\x16\\x10\\x11\\x13\\x14\\x15\\x15\\x15\\x0c\\x0f\\x17\\x18\\x16\\x14\\x18\\x12\\x14\\x15\\x14\\xff\\xdb\\x00C\\x01\\x03\\x04\\x04\\x05\\x04\\x05'\n",
      "\n",
      "b'sunflowers'\n"
     ]
    }
   ],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "  print(repr(image_raw.numpy()[:100]))\n",
    "  print()\n",
    "  print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb5a76",
   "metadata": {},
   "source": [
    "## Loteo de elementos del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f292b",
   "metadata": {},
   "source": [
    "### Loteo simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad6e23",
   "metadata": {},
   "source": [
    "La transformación `Dataset.batch()` es la forma más sencilla de hacer un lote de `n` elementos consecutivos. Para cada componente, todos los elementos deben tener un tensor de exactamente la misma dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcdd0276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3]), array([ 0, -1, -2, -3])]\n",
      "[array([4, 5, 6, 7]), array([-4, -5, -6, -7])]\n",
      "[array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11])]\n",
      "[array([12, 13, 14, 15]), array([-12, -13, -14, -15])]\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "for batch in batched_dataset.take(4):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b27335",
   "metadata": {},
   "source": [
    "### Loteo de tensores con acolchamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11103b",
   "metadata": {},
   "source": [
    "Con el loteo simple todos los tensores debe tener la misma dimensión, pero esto no va a ser el caso todas las veces. Utilizando `Dataset.padded_batch` se hace un acolchamiento de los tensores de distintas formas, específicando las dimensiones a las cuales hay que aplicar acolchamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17ee5754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
    "\n",
    "for batch in dataset.take(2):\n",
    "  print(batch.numpy())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0dddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
