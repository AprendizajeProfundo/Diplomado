{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Variacional  Dense MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Estructura Matemática del modelo](#Estructura-Matemática-del-modelo)\n",
    "* [Diseño del autoencoder](#Diseño-del-autoencoder)\n",
    "* [Modelo API funcional de Keras](#Modelo-API-funcional-de-Keras)\n",
    "* [Modelo Orientado a Objetos (sub-classing)](#Modelo-Orientado-a-Objetos ) \n",
    "* [Conceptos Teóricos](VAE_Introduction.ipynb#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno se implementa un Autoencoder Clásico para el conjunto de datos MINIST. La implementación se base únicamente en capas densas. vea el ejemplo con capas convolucionales para una versión más avanzada.\n",
    "\n",
    "\n",
    "Se muestran dos implementaciones:\n",
    "\n",
    "1. Modelo API funcional de tf.keras\n",
    "2. Modelo Orientado a objetos (subclassing)\n",
    "\n",
    "Adicionalmente, al final se muestra la implementación de un algortimo de entrenamiento personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura Matemática del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nuestro autocodificador (autoencoder) variacional (VAE) tendrá variables latentes gaussianas y una distribución posterior gaussiana   $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$  con una matriz de covarianza diagonal.\n",
    "\n",
    "Recordemos que  un VAE de cuatro elementos escenciales:\n",
    "\n",
    "1. Una  variable  latente $\\boldsymbol{z}$ con distribución  $p(\\boldsymbol{z})$  que en nuestro casoserá una variable  aleatoria Gaussiana con media cero y varianza 1 y que denotamos   $\\epsilon$.\n",
    "2. Un decodificador(decoder)  $p(\\boldsymbol{x}|\\boldsymbol{z})$  que mapea las  variables latentes  $\\boldsymbol{z}$  a variables observables $\\boldsymbol{x}$. En este ejemplo este codificador implementa un perceptron multicapa (MLP), es decir una red neuronal con una capa oculta.\n",
    "3. Un codificador (encoder)  $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$  que mapea ejemplos de entrada al espacio latente. Como se stá cosntruyendo un autoencoder variacional se tiene que este mapeo se hace generando muestras aleatorias de distribciones Gaussianas con medias y varianzas que dependen de  la entrada:   $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})=N(\\boldsymbol{z},\\boldsymbol{\\mu}(x),\\text{diag}(\\boldsymbol{\\sigma}^2(\\boldsymbol{x})))$. \n",
    "4. Una función de costo que tiene dos términos: el  error de construcción que corresponde al modelo generativo implementado en el decoder y un término adicional de regularización que minimiza la divergencia KL. El error de reconstrucción es medido por el error cuadrático medio y la divergencia por el término\n",
    "$-D_{KL}(q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})|p(\\boldsymbol{z}))=\\tfrac{1}{2}\\sum_{j=1}^{J}(1+\\log \\boldsymbol{\\sigma}^2_j(\\boldsymbol{x})-\\boldsymbol{\\mu}^2_j(\\boldsymbol{x})-\\boldsymbol{\\sigma}^2_j(\\boldsymbol{x}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño del autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Clase Sampling\n",
    "\n",
    "Para implementar el muestreo se implementa la clase Sampling, la cual se  derivada de tf.keras.layers.Layer\n",
    "\n",
    "#### Entrada\n",
    "\n",
    "Los datos de MNIST entran como una areglo numérico de tamaño $28\\times 28 = 748$, de valores en el intervalo $[0,1]$. Nótese que se usa el tipo *float32*. Se hace esto porque la implementacion de tf.keras de Tensorflow 2.XX es basada en este tipo de datos.\n",
    "\n",
    "#### Capas intermedias\n",
    "Se asume una capa intermedia de dimension 64.\n",
    "\n",
    "#### Espacio Latente\n",
    "La dimensión del espacio latente será 32.\n",
    "\n",
    "#### Entrenamiento\n",
    "se implementa 2 epochs y lotes de tamaño 64 para ser pasados al optimizador\n",
    "\n",
    "#### Encoder \n",
    "\n",
    "El encoder tiene la siguiente estructura de grafo.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/vae_encoder_dense_minist.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Autoencoder Variacional: Encoder</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/vae_decoder_dense_minist.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Autoencoder Variacional: Decoder</p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo API funcional de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "\n",
    "# Net parameters\n",
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "\n",
    "# Train parameters\n",
    "epochs = 3\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "#  Sampling layer\n",
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    \n",
    "    def __init__(self,name=None):\n",
    "        super(Sampling,self).__init__(name=name)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch,dim))\n",
    "        return z_mean + tf.exp(0.5*z_log_var)*epsilon\n",
    "   \n",
    "\n",
    "original_inputs = Input(shape=(original_dim,),name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation ='relu',name='intermediate_layer')(original_inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling(name='z_sample')([z_mean, z_log_var])\n",
    "encoder = Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "encoder.summary()\n",
    "plot_model(encoder, to_file='./Images/vae_encoder_dense_minist.png', \n",
    "           show_shapes=True)\n",
    "\n",
    "\n",
    "# Define decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sample')\n",
    "x = Dense(intermediate_dim,activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "decoder.summary()\n",
    "plot_model(decoder, to_file='./Images/vae_decoder_dense_minist.png', \n",
    "           show_shapes=True)\n",
    "\n",
    "\n",
    "# Define VAE model\n",
    "outputs = decoder(z)\n",
    "vae = Model(inputs=original_inputs,outputs=outputs, name='vae_model')\n",
    "\n",
    "\n",
    "# Add KL divergencia regularization loss\n",
    "kl_loss = -0.5*tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) +1 )\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "\n",
    "# Compile\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "loss_fn = MeanSquaredError()\n",
    "vae.compile(optimizer= optimizer, loss = loss_fn)\n",
    "\n",
    "\n",
    "# Data\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "\n",
    "# Train\n",
    "vae.fit(x_train, x_train, epochs = epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# summary\n",
    "vae.summary()\n",
    "plot_model(vae, to_file='./Images/vae_minist.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Orientado a Objetos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso Sampling Encoder y  Decoder son clases derivadas de tf.keras.layers.Layer. El modelo es una clase derivada  de f.keras.models.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Sampling\n",
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    \n",
    "    def __init__(self,name=None):\n",
    "        super(Sampling,self).__init__(name=name)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch,dim))\n",
    "        return z_mean + tf.exp(0.5*z_log_var)*epsilon\n",
    "\n",
    "# Encoder\n",
    "class Encoder(Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs) \n",
    "        self.dense_proj = Dense(intermediate_dim, activation='relu', name='intermediate_layer')\n",
    "        self.dense_mean = Dense(latent_dim, name='z_mean')\n",
    "        self.dense_log_var = Dense(latent_dim, name='z_var_log')\n",
    "        self.sampling = Sampling('z_sample')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling([z_mean,z_log_var])\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "\n",
    "# Decoder\n",
    "class Decoder(Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "    \n",
    "    def __init__(self, original_dim, intermediate_dim=64, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = Dense(intermediate_dim, activation='relu', \n",
    "                                name='intermediate_layer')\n",
    "        self.dense_output = Dense(original_dim, activation='sigmoid',\n",
    "                                  name='reconstruction_layer')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x= self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "\n",
    "# Model\n",
    "class VariationalAutoEncoder(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim, intermediate_dim=64, latent_dim=32,\n",
    "        name='autoencoder', **kwargs):\n",
    "        \n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim = latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_batch = -0.5 * tf.reduce_sum(1 + z_log_var -\n",
    "                                 tf.square(z_mean) -\n",
    "                                 tf.exp(z_log_var), axis=-1)\n",
    "        self.add_loss(tf.reduce_mean(kl_batch))\n",
    "        return reconstructed\n",
    "\n",
    "    \n",
    "# Compilation\n",
    "\n",
    "#Instance the model\n",
    "vae = VariationalAutoEncoder(784,64,32)\n",
    "\n",
    "# Optimizer function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# compile\n",
    "vae.compile(optimizer=optimizer, loss = loss)  \n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "# Net parameters\n",
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "\n",
    "# Train parameters\n",
    "epochs = 3\n",
    "batch_size=64\n",
    "\n",
    "# Data\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "# \n",
    "\n",
    "# Run training\n",
    "vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Extract latent vectors\n",
    "z=vae.encoder(x_train[:10,]) # (z_mean, z_log_var, z)\n",
    "z_sample = z[2]\n",
    "\n",
    "#plots of latent vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple training loop (additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss +=sum(vae.losses) # Add KLD regularization loss\n",
    "        \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    \n",
    "        loss_metric(loss)\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print('step %d: mean loss = %4f' % (step, loss_metric.result()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
