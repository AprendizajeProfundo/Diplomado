{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Introduction to LDA</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Latent Dirichlet Allocation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Authors</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">References</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Blei et al.,[Latent Dirichlet Allocation, 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "2. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python, 2018](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24), en Toward data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Content</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#Introducción)\n",
    "* [Superficial Analysis of Texts](#Superficial-Analysis-of-Texts)\n",
    "* [General Terminology](#General-Terminology)\n",
    "* [Textual data preprocessing](#Textual-data-preprocessing)\n",
    "* [TF-IDF](#TF-IDF)\n",
    "* [Generative Models: Latent Dirichlet Allocation](#Generative-Models:-Latent-Dirichlet-Allocation)\n",
    "* [Example: One million headlines](#Example:-One-million-headlines)\n",
    "* [Example: Airlines Tweets](#Example:-Airlines-Tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans communicate using natural languages. Natural languages differ from programming languages in recent times, they follow strict syntactic and semantic rules, while the former, due to their complexity, depend on the context.\n",
    "\n",
    "In general, text analysis has two large subareas: superficial text analysis and natural language processing.\n",
    "\n",
    "In this lesson we deal with the superficial analysis of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Superficial Analysis of Texts</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subarea was developed first, because the problems associated with natural language in this case are simpler. These are techniques in which it is sought to find the underlying topics in the text. In this sense, they are unsupervised and consequential type models based on automatic classification techniques.\n",
    "\n",
    "These techniques are aimed at detecting clusters of words and documents in large data corpus.\n",
    "\n",
    "A document is in this case a distinguishable unit from others in the corpus. For example an open response in a survey, a comment in a review, an abstract of a document, etc.\n",
    "\n",
    "After omitting terms that are considered not contributing to the detection of topics (themes), usually known as *empty words* (`stop words`) and other preprocessing processes such as stemming, clipping (`steeming`), it is common construct an array named document-term (`dtm`).\n",
    "\n",
    "This `dtm` matrix represents by the rows each one of the individual documents of the corpus and by the columns each one of the terms conserved in the analysis. Each position in the array contains the number of times a term appears in the document. In some cases this is a binary array, in which case the dtm indicates when a term appears in a document.\n",
    "\n",
    "The `dtm` is the basis of the techniques known generically as *word-bag* (`word-bag`). The name derives from the fact that when organizing the dtm, the context of the words in each document is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">General Terminology</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words or Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are the minimum units of information in natural language work.\n",
    "\n",
    "From a very modern perspective, words are objects that can be thought of as points that are in a high-dimensional space, in such a way that close points in some sense of distance correspond to words that have a closeness within a universe of words considered.\n",
    "\n",
    "The following image corresponds to a set of **astrophysics words**, considered in a study of abstracts of scientific articles. This is a graph obtained after processing like what we show today, developed by Montenegro and Montenegro using an analysis technique based on multidimensional item response theory (TRIM).\n",
    "\n",
    "In this document the words will be denoted as $w_i, i = 1,2, \\ldots, K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cluster_kmeans_10.png\" width=\"700\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Astrophysics knowledge areas, based on scientific articles</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "Source: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents are the subjects in superficial textual analyzes. We assume that we have a set of individual documents, each of which will be denoted by $ \\mathbf {w} $. A document is considered to be a sequence of $ N $ words. Thus we have that a document is denoted as $ \\mathbf {w} = \\{w_1, \\ldots, w_N \\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus is a collection of documents on a particular problem.\n",
    "\n",
    "This means that a corpus can be writen as $C = \\{\\text{doc}_{1},\\text{doc}_{2},\\text{doc}_{3},\\dots\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are latent areas to which both words and documents are associated. \n",
    "\n",
    "One of the main purposes of text analysis is to discover or highlight such topics.\n",
    "\n",
    "The previous figure shows, for example, the presence of 10 topics in the set of astrophysics documents analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Textual data preprocessing</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we are going to use the terms token and tokenize, which are not yet adopted by the Royal Academy of the Language, but which we believe will soon be like so many others from English due to their enormous current use, due to the scientific and technological developments.\n",
    "\n",
    "We will carry out the following steps:\n",
    "\n",
    "- **Cleansing raw data**: Cleaning strange simbols, tags or another kind of unnecessary elements on the text.\n",
    "- **Tokenization**: divide the text into sentences and sentences into words. Put the words in lowercase and remove the punctuation.\n",
    "- Text is cleaned using **regular expresions**.\n",
    "- Words are **lemmatized**: words in the third person are changed to the first person and the verbs in the past and future tense are changed to the present.\n",
    "- All **stopwords** are removed. (**CAREFULLY**)\n",
    "- Words **that have less than 3 characters are eliminated**. (**CAREFULLY**)\n",
    "- Words are stemming (**stemming**): words are reduced to their root form. (**OPTIONAL**)\n",
    "\n",
    "We will use the *gensim* and *nltk* libraries to do this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms that will be used frequently are:\n",
    "\n",
    "- `Corpus`: body of the text, singular. Corpora is the plural of corpus.\n",
    "- `Lexicon`: words and their meanings.\n",
    "- `Token`: each *entity* that is part of whatever was divided according to the rules that we establish for the analysis. For example, each word is a token when a sentence is tokenized into words. Each sentence can also be a token, if you have converted the sentences to a paragraph.\n",
    "\n",
    "Basically, tokenize involves splitting sentences and words from the body of text.\n",
    "\n",
    "See the following example taken from [Geek for Geeks](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/?ref=rp). We use *nltk* library.\n",
    "\n",
    "Lets suppose that our goal is to analize the following toy example:\n",
    "\n",
    "*Natural language processing **(NLP)** is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.* \n",
    "\n",
    "*Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof. There are 365 days usually. This year is 2020.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"foo\">Natural language processing <b>(NLP)</b> is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.<br/> Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof. There are 365 days usually. This year is 2020.</p>\n"
     ]
    }
   ],
   "source": [
    "raw_text = '<p class=\"foo\">Natural language processing <b>(NLP)</b> is a field ' \\\n",
    "       + 'of computer science, artificial intelligence ' \\\n",
    "       + 'and computational linguistics concerned with ' \\\n",
    "       +'the interactions between computers and human ' \\\n",
    "       + '(natural) languages, and, in particular, ' \\\n",
    "       + 'concerned with programming computers to ' \\\n",
    "       + 'fruitfully process large natural language ' \\\n",
    "       + 'corpora.<br/> Challenges in natural language ' \\\n",
    "       + 'processing frequently involve natural ' \\\n",
    "       + 'language understanding, natural language ' \\\n",
    "       + 'generation frequently from formal, machine' \\\n",
    "       + '-readable logical forms), connecting language ' \\\n",
    "       + 'and machine perception, managing human-' \\\n",
    "       + 'computer dialog systems, or some combination ' \\\n",
    "       + 'thereof. There are 365 days usually. ' \\\n",
    "       + 'This year is 2020.</p>'\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing resources from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moury/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/moury/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/moury/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing libraries \n",
    "import nltk\n",
    "\n",
    "# tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# For tweets\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Special dictionaries for punctuation and stopwords\n",
    "nltk.download('punkt') # Punctuation\n",
    "nltk.download('stopwords') # stopwords\n",
    "\n",
    "# Large lexical database of English\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lematizador basado en WordNet de nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# notlk's steemer. Extract root of words.\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing resources from `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Importing stopwords using gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof. There are 365 days usually. This year is 2020. \n"
     ]
    }
   ],
   "source": [
    "# Transforms html to text\n",
    "import html2text\n",
    "# Regular Expressions\n",
    "import re\n",
    "\n",
    "# Transform html to text\n",
    "text = html2text.html2text(''.join(str(raw_text)))\n",
    "# Drop breakline\n",
    "text = re.sub(r'\\n',' ',text)\n",
    "# Drop *\n",
    "text = re.sub(r'\\*',' ',text)\n",
    "# Drop extra-spaces\n",
    "text = re.sub(r'\\s\\s+',' ',text)\n",
    "# Unify w1- w2 to w1-w2 \n",
    "text = re.sub(r'\\-\\s','-',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences Tokenization:\n",
      "\n",
      "Sentence 0:\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.\n",
      "\n",
      "Sentence 1:\n",
      "Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\n",
      "\n",
      "Sentence 2:\n",
      "There are 365 days usually.\n",
      "\n",
      "Sentence 3:\n",
      "This year is 2020.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentences\n",
    "print('Sentences Tokenization:\\n')\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "for i,sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i}:\\n{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with Tokens(Sentences):\n",
      "\n",
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural language generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.', 'There are 365 days usually.', 'This year is 2020.']\n"
     ]
    }
   ],
   "source": [
    "print('List with Tokens(Sentences):\\n')\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "\n",
      "Tokens: 98\n",
      "\n",
      "Word 0: Natural\n",
      "Word 1: language\n",
      "Word 2: processing\n",
      "Word 3: (\n",
      "Word 4: NLP\n",
      "Word 5: )\n",
      "Word 6: is\n",
      "Word 7: a\n",
      "Word 8: field\n",
      "Word 9: of\n",
      "Word 10: computer\n",
      "Word 11: science\n",
      "Word 12: ,\n",
      "Word 13: artificial\n",
      "Word 14: intelligence\n",
      "Word 15: and\n",
      "Word 16: computational\n",
      "Word 17: linguistics\n",
      "Word 18: concerned\n",
      "Word 19: with\n",
      "Word 20: the\n",
      "Word 21: interactions\n",
      "Word 22: between\n",
      "Word 23: computers\n",
      "Word 24: and\n",
      "Word 25: human\n",
      "Word 26: (\n",
      "Word 27: natural\n",
      "Word 28: )\n",
      "Word 29: languages\n",
      "Word 30: ,\n",
      "Word 31: and\n",
      "Word 32: ,\n",
      "Word 33: in\n",
      "Word 34: particular\n",
      "Word 35: ,\n",
      "Word 36: concerned\n",
      "Word 37: with\n",
      "Word 38: programming\n",
      "Word 39: computers\n",
      "Word 40: to\n",
      "Word 41: fruitfully\n",
      "Word 42: process\n",
      "Word 43: large\n",
      "Word 44: natural\n",
      "Word 45: language\n",
      "Word 46: corpora\n",
      "Word 47: .\n",
      "Word 48: Challenges\n",
      "Word 49: in\n",
      "Word 50: natural\n",
      "Word 51: language\n",
      "Word 52: processing\n",
      "Word 53: frequently\n",
      "Word 54: involve\n",
      "Word 55: natural\n",
      "Word 56: language\n",
      "Word 57: understanding\n",
      "Word 58: ,\n",
      "Word 59: natural\n",
      "Word 60: language\n",
      "Word 61: generation\n",
      "Word 62: frequently\n",
      "Word 63: from\n",
      "Word 64: formal\n",
      "Word 65: ,\n",
      "Word 66: machine-readable\n",
      "Word 67: logical\n",
      "Word 68: forms\n",
      "Word 69: )\n",
      "Word 70: ,\n",
      "Word 71: connecting\n",
      "Word 72: language\n",
      "Word 73: and\n",
      "Word 74: machine\n",
      "Word 75: perception\n",
      "Word 76: ,\n",
      "Word 77: managing\n",
      "Word 78: human-computer\n",
      "Word 79: dialog\n",
      "Word 80: systems\n",
      "Word 81: ,\n",
      "Word 82: or\n",
      "Word 83: some\n",
      "Word 84: combination\n",
      "Word 85: thereof\n",
      "Word 86: .\n",
      "Word 87: There\n",
      "Word 88: are\n",
      "Word 89: 365\n",
      "Word 90: days\n",
      "Word 91: usually\n",
      "Word 92: .\n",
      "Word 93: This\n",
      "Word 94: year\n",
      "Word 95: is\n",
      "Word 96: 2020\n",
      "Word 97: .\n"
     ]
    }
   ],
   "source": [
    "# palabras\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print('Word Tokenization:\\n')\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "for i,token in enumerate(tokens):\n",
    "    print(f'Word {i}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with Tokens (Words):\n",
      "\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'There', 'are', '365', 'days', 'usually', '.', 'This', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "print('List with Tokens (Words):\\n')\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization:\n",
      "N|a|t|u|r|a|l| |l|a|n|g|u|a|g|e| |p|r|o|c|e|s|s|i|n|g| |(|N|L|P|)| |i|s| |a| |f|i|e|l|d| |o|f| |c|o|m|p|u|t|e|r| |s|c|i|e|n|c|e|,| |a|r|t|i|f|i|c|i|a|l| |i|n|t|e|l|l|i|g|e|n|c|e| |a|n|d| |c|o|m|p|u|t|a|t|i|o|n|a|l| |l|i|n|g|u|i|s|t|i|c|s| |c|o|n|c|e|r|n|e|d| |w|i|t|h| |t|h|e| |i|n|t|e|r|a|c|t|i|o|n|s| |b|e|t|w|e|e|n| |c|o|m|p|u|t|e|r|s| |a|n|d| |h|u|m|a|n| |(|n|a|t|u|r|a|l|)| |l|a|n|g|u|a|g|e|s|,| |a|n|d|,| |i|n| |p|a|r|t|i|c|u|l|a|r|,| |c|o|n|c|e|r|n|e|d| |w|i|t|h| |p|r|o|g|r|a|m|m|i|n|g| |c|o|m|p|u|t|e|r|s| |t|o| |f|r|u|i|t|f|u|l|l|y| |p|r|o|c|e|s|s| |l|a|r|g|e| |n|a|t|u|r|a|l| |l|a|n|g|u|a|g|e| |c|o|r|p|o|r|a|.| |C|h|a|l|l|e|n|g|e|s| |i|n| |n|a|t|u|r|a|l| |l|a|n|g|u|a|g|e| |p|r|o|c|e|s|s|i|n|g| |f|r|e|q|u|e|n|t|l|y| |i|n|v|o|l|v|e| |n|a|t|u|r|a|l| |l|a|n|g|u|a|g|e| |u|n|d|e|r|s|t|a|n|d|i|n|g|,| |n|a|t|u|r|a|l| |l|a|n|g|u|a|g|e| |g|e|n|e|r|a|t|i|o|n| |f|r|e|q|u|e|n|t|l|y| |f|r|o|m| |f|o|r|m|a|l|,| |m|a|c|h|i|n|e|-|r|e|a|d|a|b|l|e| |l|o|g|i|c|a|l| |f|o|r|m|s|)|,| |c|o|n|n|e|c|t|i|n|g| |l|a|n|g|u|a|g|e| |a|n|d| |m|a|c|h|i|n|e| |p|e|r|c|e|p|t|i|o|n|,| |m|a|n|a|g|i|n|g| |h|u|m|a|n|-|c|o|m|p|u|t|e|r| |d|i|a|l|o|g| |s|y|s|t|e|m|s|,| |o|r| |s|o|m|e| |c|o|m|b|i|n|a|t|i|o|n| |t|h|e|r|e|o|f|.| |T|h|e|r|e| |a|r|e| |3|6|5| |d|a|y|s| |u|s|u|a|l|l|y|.| |T|h|i|s| |y|e|a|r| |i|s| |2|0|2|0|.| |"
     ]
    }
   ],
   "source": [
    "# characters\n",
    "chars = [char for char in text]\n",
    "print('Character Tokenization:')\n",
    "for i,char in enumerate(chars):\n",
    "    print(f'{char}', end='|')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List with Tokens (Characters):\n",
      "\n",
      "['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', '(', 'N', 'L', 'P', ')', ' ', 'i', 's', ' ', 'a', ' ', 'f', 'i', 'e', 'l', 'd', ' ', 'o', 'f', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 's', 'c', 'i', 'e', 'n', 'c', 'e', ',', ' ', 'a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'l', 'i', 'n', 'g', 'u', 'i', 's', 't', 'i', 'c', 's', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'a', 'c', 't', 'i', 'o', 'n', 's', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'u', 'm', 'a', 'n', ' ', '(', 'n', 'a', 't', 'u', 'r', 'a', 'l', ')', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', 's', ',', ' ', 'a', 'n', 'd', ',', ' ', 'i', 'n', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', ',', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', 's', ' ', 't', 'o', ' ', 'f', 'r', 'u', 'i', 't', 'f', 'u', 'l', 'l', 'y', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', '.', ' ', 'C', 'h', 'a', 'l', 'l', 'e', 'n', 'g', 'e', 's', ' ', 'i', 'n', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'i', 'n', 'v', 'o', 'l', 'v', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ',', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'r', 'e', 'q', 'u', 'e', 'n', 't', 'l', 'y', ' ', 'f', 'r', 'o', 'm', ' ', 'f', 'o', 'r', 'm', 'a', 'l', ',', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', '-', 'r', 'e', 'a', 'd', 'a', 'b', 'l', 'e', ' ', 'l', 'o', 'g', 'i', 'c', 'a', 'l', ' ', 'f', 'o', 'r', 'm', 's', ')', ',', ' ', 'c', 'o', 'n', 'n', 'e', 'c', 't', 'i', 'n', 'g', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'p', 'e', 'r', 'c', 'e', 'p', 't', 'i', 'o', 'n', ',', ' ', 'm', 'a', 'n', 'a', 'g', 'i', 'n', 'g', ' ', 'h', 'u', 'm', 'a', 'n', '-', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 'd', 'i', 'a', 'l', 'o', 'g', ' ', 's', 'y', 's', 't', 'e', 'm', 's', ',', ' ', 'o', 'r', ' ', 's', 'o', 'm', 'e', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'a', 't', 'i', 'o', 'n', ' ', 't', 'h', 'e', 'r', 'e', 'o', 'f', '.', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', '3', '6', '5', ' ', 'd', 'a', 'y', 's', ' ', 'u', 's', 'u', 'a', 'l', 'l', 'y', '.', ' ', 'T', 'h', 'i', 's', ' ', 'y', 'e', 'a', 'r', ' ', 'i', 's', ' ', '2', '0', '2', '0', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "print('List with Tokens (Characters):\\n')\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "print('Tokens:\\n')\n",
    "print(tknzr.tokenize(s0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Toknization using `strip_handles` and `reduce_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "tw = tknzr.tokenize(s1)\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Transform Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 98\n",
      "\n",
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '365', 'days', 'usually', '.', 'this', 'year', 'is', '2020', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove special characters - regular expressions (regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are mathematical objects that allow you to interpret pieces of text.\n",
    "\n",
    "They are key in the construction of programming languages. Here we are going to use the Python [re](https://docs.python.org/3/library/re.html) library created for handling regular expressions. \n",
    "\n",
    "We suggest this [re in Python tutorial](https://www.w3schools.com/python/python_regex.asp) to learn how to use the re library.\n",
    "\n",
    "Aditionally, you can get the [Cheat-Sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) for regular expressions and an online tester [here](https://regexr.com/).\n",
    "\n",
    "We will use here to remove some symbols: numbers and parentheses for example. This is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 98\n",
      "\n",
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '', 'days', 'usually', '.', 'this', 'year', 'is', '', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# digits (CAREFULLY)\n",
    "tokens = [re.sub(r'\\d+', '',token) for token in tokens]\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 98\n",
      "\n",
      "['natural', 'language', 'processing', '', 'nlp', '', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '', 'natural', '', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', '', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.', 'there', 'are', '', 'days', 'usually', '.', 'this', 'year', 'is', '', '.']\n"
     ]
    }
   ],
   "source": [
    "# parenthesis\n",
    "tokens = [re.sub(r'[()]', '',token) for token in tokens]\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 98\n",
      "\n",
      "['natural', 'language', 'processing', '', 'nlp', '', 'is', 'a', 'field', 'of', 'computer', 'science', '', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '', 'natural', '', 'languages', '', 'and', '', 'in', 'particular', '', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', '', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', '', 'machinereadable', 'logical', 'forms', '', '', 'connecting', 'language', 'and', 'machine', 'perception', '', 'managing', 'humancomputer', 'dialog', 'systems', '', 'or', 'some', 'combination', 'thereof', '', 'there', 'are', '', 'days', 'usually', '', 'this', 'year', 'is', '', '']\n"
     ]
    }
   ],
   "source": [
    "# Take out punctuations and other symbols\n",
    "tokens = [re.sub(r'[^\\w\\s]', '',token) for token in tokens]\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 78\n",
      "\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machinereadable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'humancomputer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof', 'there', 'are', 'days', 'usually', 'this', 'year', 'is']\n"
     ]
    }
   ],
   "source": [
    "# Drop empty spaces\n",
    "tokens = [token for token in tokens if len(token)>0]\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the **word's lemma**, or dictionary form.\n",
    "\n",
    "**Stemming** is the process of reducing inflected (or sometimes derived) words to their **word stem**, base or root form—generally a written word form.\n",
    "\n",
    "Therefore, it links words with a meaning similar to a word.\n",
    "\n",
    "Text preprocessing includes both `Stemming` and `Lemmatization`.\n",
    "\n",
    "Many times people find these two terms confusing. Some treat these two as equals.\n",
    "\n",
    "Actually, **lematization is preferred to stemming** because stemming performs morphological analysis of words.\n",
    "\n",
    "The applications of the stemming are:\n",
    "\n",
    "- It is used in comprehensive retrieval systems such as search engines.\n",
    "- Used in compact indexing\n",
    "- Examples of stemming:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* rocks -> rock\n",
    "* corpora -> corpus\n",
    "* better -> good\n",
    "\n",
    "An important difference from stemming is that lemmatization takes a part of the voice parameter, \"pos\". If not provided, the default is \"noun\". In the following example we are going to place *pos = 'a'* which means adjective. If *pos = 'v'* is placed, it means verb. By default it is *pos = 'n'*, that is, a noun.\n",
    "\n",
    "The following is the stemming implementation of some English words using the *nltk* library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks   -> rock\n",
      "corpora -> corpus\n",
      "better  -> good\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks   ->\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora ->\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better  ->\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos lematizar el texto de ejemplo, primero con verbos y luego con sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machinereadable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'humancomputer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof', 'there', 'are', 'days', 'usually', 'this', 'year', 'is']\n",
      "\n",
      "Lemmas with pos=\"v\":\n",
      "\n",
      "['natural', 'language', 'process', 'nlp', 'be', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concern', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concern', 'with', 'program', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenge', 'in', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machinereadable', 'logical', 'form', 'connect', 'language', 'and', 'machine', 'perception', 'manage', 'humancomputer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof', 'there', 'be', 'days', 'usually', 'this', 'year', 'be']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "# verbs\n",
    "lemma_text =[]\n",
    "for token in tokens:\n",
    "    lemma_text.append(WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "\n",
    "print('Tokens:\\n')\n",
    "print(tokens)\n",
    "print('\\nLemmas with pos=\"v\":\\n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmas with pos=\"n\":\n",
      "\n",
      "['natural', 'language', 'process', 'nlp', 'be', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concern', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'natural', 'language', 'and', 'in', 'particular', 'concern', 'with', 'program', 'computer', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpus', 'challenge', 'in', 'natural', 'language', 'process', 'frequently', 'involve', 'natural', 'language', 'understand', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machinereadable', 'logical', 'form', 'connect', 'language', 'and', 'machine', 'perception', 'manage', 'humancomputer', 'dialog', 'system', 'or', 'some', 'combination', 'thereof', 'there', 'be', 'day', 'usually', 'this', 'year', 'be']\n"
     ]
    }
   ],
   "source": [
    "# nouns\n",
    "for i in range(len(lemma_text)):\n",
    "    lemma_text[i] = WordNetLemmatizer().lemmatize(lemma_text[i], pos=\"n\")\n",
    "print('\\nLemmas with pos=\"n\":\\n')\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steeming is the process of producing morphological variants of a root / base word. Bypass programs are commonly known as steeming or derivation algorithms. A stemming algorithm reduces the words as in the following examples\n",
    "\n",
    "+ \"chocolates\", \"chocolates\", \"choco\" at the root of the word, \"chocolate\"\n",
    "+ \"recovery\", \"recovered\", \"recover\" is reduced to the root \"recover\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two problems in stemming: overstemming and understemming.\n",
    "\n",
    "Excessive overclipping occurs when two words are derived from the same stem that have different roots.\n",
    "\n",
    "Undercutting occurs when two words are derived from the different stem but have the same root.\n",
    "\n",
    "For example, the widely used Porter stemmer stems \"universal\", \"university\", and \"universe\" to \"univers\". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.\n",
    "\n",
    "An example of understemming in the Porter stemmer is \"alumnus\" → \"alumnu\", \"alumni\" → \"alumni\", \"alumna\"/\"alumnae\" → \"alumna\". This English word keeps Latin morphology, and so these near-synonyms are not conflated.\n",
    "\n",
    "Lets see Stemming in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'be', 'a', 'field', 'of', 'comput', 'scienc', 'artifici', 'intellig', 'and', 'comput', 'linguist', 'concern', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', 'natur', 'languag', 'and', 'in', 'particular', 'concern', 'with', 'program', 'comput', 'to', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpu', 'challeng', 'in', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'gener', 'frequent', 'from', 'formal', 'machineread', 'logic', 'form', 'connect', 'languag', 'and', 'machin', 'percept', 'manag', 'humancomput', 'dialog', 'system', 'or', 'some', 'combin', 'thereof', 'there', 'be', 'day', 'usual', 'thi', 'year', 'be']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "# crea una instancia de PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(lemma_text)):\n",
    "    lemma_text[i] = ps.stem(lemma_text[i])\n",
    "print(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes words of length less than or equal to two (CAREFULLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 56\n",
      "\n",
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machinereadable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'humancomputer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_4 = []\n",
    "\n",
    "for token in tokens:\n",
    "    if len(token) > 2:\n",
    "        tokens_4.append(token)\n",
    "\n",
    "tokens = tokens_4\n",
    "\n",
    "# equivalent to\n",
    "#tokens_4 = [token for token in tokens if len(token)>3]\n",
    "\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty words or stopwords are words that in common language are **considered not to contribute to the semantic content of texts**. In the bag of words technique they are omitted, because they cause confusing classifications. Actually, the concept of empty words depends on the answer the researcher wants to get.\n",
    "\n",
    "The following example shows the dictionary of English stopwords contained in the `gensim` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in Gensim: 337\n",
      "\n",
      "['front', 'hasnt', 'everywhere', 'an', 'thence', 'hence', 'quite', 'your', 'some', 'take', 'why', 'elsewhere', 'whether', 'own', 'go', 'a', 'every', 'seems', 'either', 'doing', 'anything', 'interest', 'get', 'of', 'me', 'into', 'part', 'else', 'wherein', 'under', 'less', 'full', 'still', 'below', 'seeming', 'been', 'since', 'using', 'toward', 'eight', 'hereby', 'couldnt', 'anywhere', 'until', 'within', 'is', 'although', 'latterly', 'doesn', 'had', 'more', 'otherwise', 'thereupon', 'top', 'rather', 'mostly', 'on', 'between', 'became', 'another', 'mill', 'whole', 'what', 'computer', 'via', 'call', 'he', 'keep', 'give', 'these', 'un', 'etc', 'put', 'detail', 'all', 'across', 'none', 'due', 'yours', 'co', 'and', 'hereupon', 'which', 'last', 'whereupon', 'or', 'name', 'were', 'though', 'who', 'does', 'this', 'cant', 'my', 'each', 'upon', 'most', 'never', 'among', 'whereafter', 'several', 'someone', 'sometimes', 'alone', 'against', 'anyone', 'there', 'those', 'fire', 'hundred', 'seemed', 'however', 'well', 'everyone', 'very', 'de', 'may', 'herein', 'when', 'was', 'say', 'hereafter', 'move', 'did', 'as', 'everything', 'by', 'if', 'thereafter', 'something', 'ltd', 'amount', 'eg', 'but', 'from', 'further', 'six', 'ten', 'ourselves', 'do', 'us', 'during', 'where', 'enough', 'whom', 'above', 'itself', 'around', 'anyway', 'over', 'nine', 'whose', 'regarding', 'first', 'system', 'them', 'even', 'other', 'sincere', 'become', 'becomes', 'only', 'once', 'then', 'after', 'except', 'should', 'besides', 'same', 'it', 'along', 'neither', 'now', 'has', 'no', 'con', 'yet', 'thick', 'beyond', 'done', 'will', 'next', 'mine', 'back', 'just', 'that', 'empty', 'must', 'anyhow', 'they', 'seem', 'yourself', 'four', 'behind', 'see', 'indeed', 'him', 'cry', 'many', 'i', 'serious', 'cannot', 'too', 'together', 'off', 'whither', 'least', 'herself', 'beside', 'its', 'thus', 'former', 'twelve', 'nevertheless', 'per', 'might', 'always', 'whereby', 'describe', 'amoungst', 'nowhere', 'also', 'both', 'didn', 'eleven', 'sixty', 'because', 'without', 'about', 'therefore', 'namely', 'fill', 'whenever', 'yourselves', 'hers', 'two', 'noone', 'often', 'one', 'could', 'amongst', 'his', 'five', 'somewhere', 'thru', 'latter', 'at', 'being', 'find', 're', 'are', 'through', 'be', 'out', 'would', 'already', 'make', 'myself', 'whatever', 'up', 'three', 'throughout', 'others', 'ever', 'formerly', 'fifteen', 'made', 'wherever', 'km', 'unless', 'used', 'themselves', 'onto', 'the', 'himself', 'not', 'ours', 'she', 'bill', 'kg', 'fifty', 'any', 'to', 'found', 'few', 'have', 'nothing', 'her', 'don', 'am', 'really', 'much', 'such', 'becoming', 'before', 'somehow', 'how', 'thin', 'side', 'for', 'third', 'again', 'so', 'twenty', 'whereas', 'thereby', 'down', 'in', 'our', 'nobody', 'please', 'while', 'inc', 'we', 'here', 'forty', 'whence', 'their', 'than', 'nor', 'almost', 'ie', 'perhaps', 'afterwards', 'moreover', 'bottom', 'various', 'show', 'whoever', 'you', 'sometime', 'with', 'meanwhile', 'can', 'therein', 'towards', 'beforehand']\n"
     ]
    }
   ],
   "source": [
    "stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "print(f'Stopwords in Gensim: {len(stopwords_gensim)}\\n')\n",
    "print(stopwords_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la librería *nltk* el diciconario de palabras vacías del inglés es actualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in nltk: 179\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#\n",
    "stopwords_nltk = stopwords.words('english')\n",
    "print(f'Stopwords in nltk: {len(stopwords_nltk)}\\n')\n",
    "print(stopwords_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two sets of stopwords are different.\n",
    "\n",
    "In fact, the common words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Stopwords: 126\n",
      "\n",
      "{'has', 'no', 'at', 'and', 'being', 'an', 'which', 're', 'your', 'or', 'some', 'were', 'who', 'are', 'be', 'does', 'through', 'this', 'why', 'my', 'out', 'own', 'each', 'will', 'a', 'most', 'myself', 'up', 'doing', 'just', 'against', 'that', 'there', 'those', 'of', 'me', 'themselves', 'into', 'they', 'the', 'yourself', 'himself', 'under', 'not', 'ours', 'she', 'below', 'been', 'him', 'i', 'too', 'off', 'any', 'to', 'few', 'have', 'very', 'her', 'don', 'when', 'until', 'was', 'herself', 'am', 'did', 'its', 'such', 'is', 'before', 'as', 'how', 'by', 'if', 'doesn', 'for', 'had', 'more', 'but', 'again', 'from', 'further', 'so', 'both', 'down', 'didn', 'ourselves', 'do', 'in', 'because', 'our', 'on', 'between', 'during', 'while', 'where', 'whom', 'we', 'about', 'here', 'their', 'what', 'than', 'nor', 'above', 'itself', 'he', 'yourselves', 'hers', 'over', 'these', 'them', 'other', 'only', 'you', 'once', 'with', 'then', 'all', 'after', 'same', 'should', 'his', 'it', 'can', 'now', 'yours'}\n"
     ]
    }
   ],
   "source": [
    "inter = set(stopwords_gensim).intersection(set(stopwords_nltk))\n",
    "print(f'Common Stopwords: {len(inter)}\\n')\n",
    "print(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we are going to remove the empty words from the tokenized text object defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another library, called **SpaCy**, that also contains another set of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in SpaCy: 326\n",
      "\n",
      "['front', 'everywhere', 'an', 'thence', 'ca', 'hence', 'quite', 'your', 'some', 'take', 'why', 'elsewhere', 'whether', 'own', 'go', 'a', 'every', 'seems', 'doing', 'anything', 'either', 'get', 'of', 'me', 'into', 'part', 'else', 'wherein', 'under', 'less', 'full', 'still', 'below', 'seeming', 'been', 'since', 'toward', 'using', 'eight', 'hereby', 'anywhere', 'until', 'within', \"'m\", 'is', 'although', 'latterly', 'had', 'more', 'otherwise', 'thereupon', 'top', 'rather', \"n't\", 'mostly', 'on', 'between', 'became', 'another', 'whole', 'what', '’d', 'via', 'call', 'he', 'keep', 'give', 'these', 'put', 'all', 'across', 'none', 'due', 'yours', 'and', 'hereupon', 'which', 'last', 'whereupon', 'or', 'name', 'were', 'though', 'who', 'does', 'this', 'my', 'each', 'upon', 'most', 'never', \"'d\", 'among', 'whereafter', 'several', 'someone', 'sometimes', 'alone', 'against', 'anyone', '‘ll', 'there', 'those', 'hundred', 'however', 'seemed', 'well', 'everyone', 'very', 'may', 'herein', 'when', 'was', 'n’t', 'say', 'hereafter', 'move', 'did', 'as', 'by', 'everything', 'if', 'thereafter', 'something', 'amount', 'but', 'from', 'further', '’s', 'six', 'ten', '‘ve', 'ourselves', 'do', 'us', 'during', 'where', 'enough', 'whom', \"'s\", 'above', 'itself', 'around', 'anyway', 'nine', 'over', 'whose', 'regarding', 'first', 'even', 'them', 'other', 'become', 'becomes', 'only', '‘m', \"'re\", 'once', 'then', 'after', 'except', 'same', 'besides', 'should', 'it', 'along', 'neither', 'now', 'has', 'no', 'yet', 'beyond', 'done', 'will', 'next', 'mine', 'back', 'just', 'that', 'empty', '‘d', 'must', 'anyhow', 'seem', 'they', 'yourself', 'four', 'behind', 'see', 'indeed', 'him', 'i', 'many', 'serious', 'cannot', '’re', 'too', 'off', 'together', 'whither', 'least', 'herself', 'beside', 'its', 'thus', 'former', 'nevertheless', 'twelve', 'per', 'might', 'always', '’ve', 'whereby', 'nowhere', 'also', 'both', 'eleven', 'sixty', 'because', 'without', 'about', 'therefore', 'namely', 'whenever', 'hers', 'yourselves', 'two', 'noone', 'often', 'could', 'one', 'amongst', 'his', 'five', 'n‘t', '‘s', 'somewhere', 'thru', 'at', 'latter', 'being', 're', 'are', 'be', 'through', 'out', 'would', 'already', 'make', \"'ll\", 'myself', 'whatever', 'up', 'three', 'throughout', 'others', 'ever', 'formerly', 'fifteen', 'made', 'wherever', 'unless', 'themselves', 'used', 'onto', 'the', 'himself', 'not', 'ours', 'she', 'fifty', 'any', 'to', 'few', 'have', 'nothing', 'her', 'am', 'really', 'much', 'such', 'becoming', 'before', 'somehow', 'how', 'side', 'for', 'third', '‘re', 'again', 'so', \"'ve\", 'twenty', 'whereas', 'thereby', 'down', 'in', '’ll', 'nobody', 'our', 'please', 'while', 'we', 'here', 'forty', 'whence', 'their', 'than', 'almost', 'nor', 'perhaps', 'afterwards', 'moreover', 'bottom', 'various', 'show', 'whoever', 'you', 'sometime', 'with', 'meanwhile', 'can', 'therein', '’m', 'towards', 'beforehand']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# install models with spaCy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Take stopwords from SpaCy\n",
    "stopwords_spacy = list(nlp.Defaults.stop_words)\n",
    "\n",
    "print(f'Stopwords in SpaCy: {len(stopwords_spacy)}\\n')\n",
    "print(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords in Spanish using ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras Vacías en nltk, español: 313\n",
      "\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "palabrasVacias_nltk = stopwords.words('spanish')\n",
    "print(f'Palabras Vacías en nltk, español: {len(palabrasVacias_nltk)}\\n')\n",
    "print(palabrasVacias_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove the stop words from the example using nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 63\n",
      "\n",
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'with', 'interactions', 'between', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'with', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', 'machinereadable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'humancomputer', 'dialog', 'systems', 'some', 'combination', 'thereof', 'there', 'days', 'usually', 'this', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 56\n",
      "\n",
      "['natural', 'language', 'processing', 'field', 'computer', 'science', 'artificial', 'intelligence', 'computational', 'linguistics', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'concerned', 'programming', 'computers', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'challenges', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'frequently', 'formal', 'machinereadable', 'logical', 'forms', 'connecting', 'language', 'machine', 'perception', 'managing', 'humancomputer', 'dialog', 'systems', 'combination', 'thereof', 'days', 'usually', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokens_n_e = [token for token in tokens if token not in stopwords_nltk]\n",
    "#\n",
    "tokens = tokens_n_e\n",
    "print(f'Tokens: {len(tokens)}\\n')\n",
    "print(tokens)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">TF-IDF</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [Wikipedia](https://es.wikipedia.org/wiki/Tf-idf).\n",
    "\n",
    "Tf-idf (Term frequency - Inverse document frequency), term frequency - inverse document frequency (that is, the frequency of occurrence of the term in the corpus of documents), is a numerical measure that expresses how relevant a word is for a document in a corpus. This measure is often used as a weighting factor in information retrieval and text mining.\n",
    "\n",
    "\n",
    "The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the document corpus, which allows handling of the fact that some words are generally more common than others.\n",
    "\n",
    "Variations of the tf-idf weight scheme are frequently used by search engines as a fundamental tool to measure the relevance of a document given a user's query, thus establishing an ordering or ranking of them.\n",
    "\n",
    "\n",
    "Tf-idf can be used successfully for filtering stop-words, in different fields of pre-word processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is the product of two measurements, *term frequency* and *inverse document frequency*. There are several ways to determine the value of both.\n",
    "\n",
    "In the case of the term frequency $ \\text {tf} (t, d) $, the simplest option is to use the raw frequency of the term $ t $ in the document $ d $, that is, the number of times that the term $ t $ occurs in the document $ d $. If we denote the raw frequency of $ t $ by $ f (t, d) $, then the simple $ \\text {tf} $ schema is $ \\text {tf} (t, d) = f (t, d) $ .\n",
    "\n",
    "\n",
    "Other possibilities are:\n",
    "\n",
    "- *Boolean \"frequencies*: tf (t, d) = 1 if t occurs in d, and 0 if not;\n",
    "- *logarithmically scaled frequency*: tf (t, d) = 1 + log f (t, d) (y 0 if f (t, d) = 0);\n",
    "- *standardized frequency*, to avoid a bias towards long documents. For example, divide the raw frequency by the maximum frequency of some term in the document:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {tf} (t,d)={\\frac {\\mathrm {f} (t,d)}{\\max\\{\\mathrm {f} (t,d):t\\in d\\}}}}\n",
    "$$\n",
    "\n",
    "The inverse document frequency is a measure of whether the term is common or not, in the corpus of documents. It is obtained by dividing the total number of documents by the number of documents that contain the term, and the logarithm of this quotient is taken:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- ${\\displaystyle |D|}$: cardinality of $D$, or number of documents in the corpus.\n",
    "- ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$ : number of documents where the term $ t $ appears. If the term is not in the collection, a division-by-zero will occur. Therefore, it is common to fit this formula to ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$.\n",
    "\n",
    "Mathematically, the base of the logarithm function is not important and is a constant factor in the final result.\n",
    "\n",
    "Hencem *tf-idf* is calculated as:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\text{tf-idf} (t,d,D)=\\mathrm {tf} (t,d)\\times \\mathrm {idf} (t,D)}\n",
    "$$\n",
    "\n",
    "A high weight in *tf-idf* is reached with a high frequency of term (in the given document) and a low frequency of occurrence of the term in corpus of documents.\n",
    "\n",
    "Since the quotient within the logarithm function of the idf is always greater than or equal to 1, the value of *idf* (and of *tf-idf*) is greater than or equal to 0.\n",
    "\n",
    "When a term appears in many documents, the quotient within the logarithm approaches 1, giving a value of *idf* and *tf-idf* close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Generative Models: Latent Dirichlet Allocation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Latent Dirichlet Allocation (LDA) technique is currently the most used for the extraction of documents from document corpus and is due to [Blei et al](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central ideas behind LDA, Blei et al.(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central ideas behind LDA are as follows. The generative model assumes that the documents are generated as follows:\n",
    "\n",
    "1. The size $ N $ of the document is generated by a Poisson distribution $ \\text {Poi} (\\xi) $.\n",
    "2. The topics are generated from a multinomial distribution with a probability vector $ \\mathbf {\\theta} $.\n",
    "3. A priori it is assumed that the vector $ \\mathbf {\\theta} $ is generated by a Dirichlet distribution with vector of parameters $ \\boldsymbol {\\alpha} $. From this derives the name of the technique.\n",
    "4. Each of the $ N $ words in a document is generated according to the following algorithm.\n",
    "      - A topic is chosen $ z_n \\sim \\text {Multinomial} (\\mathbf {\\theta}) $.\n",
    "      - The word $ w_n \\sim \\text {P} (w_n | z_n, \\mathbf {\\beta}) $ is chosen. Where $ \\mathbf {\\beta} $ is a matrix of probabilities of the words belonging to the topics. $ P $ is a multinomial probability conditional on the topic $ z_n $ and the vector of parameters $ \\mathbf {\\beta} $.\n",
    "\n",
    "\n",
    "To the reader interested in the details, we refer him to the original paper of [Blei et al.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image tries to show the central ideas behind the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Diagram_Blei.png\" width=\"800\" height=\"700\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Intuition behind LDA</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: \n",
    "[Intuition behind LDA](http://www.cs.cornell.edu/courses/cs6784/2010sp/lecture/30-BleiEtAl03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a type of statistical modeling to discover the abstract \"themes\" that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of a topic model and is used to classify the text of a document on a particular topic.\n",
    "\n",
    "Build a topic model per document and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "Here we are going to apply LDA to a set of documents and divide them into topics. Let us begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/moury/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to write a function that lemmatizes and preprocesses the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokenizes el texto\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Example: One million headlines</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use is a list of over a million news headlines published over a 15-year period and can be downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines/metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example adapted from [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../Datos/abcnews-date-text.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226253</th>\n",
       "      <td>20201231</td>\n",
       "      <td>what abc readers learned from 2020 looking bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226254</th>\n",
       "      <td>20201231</td>\n",
       "      <td>what are the south african and uk variants of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226255</th>\n",
       "      <td>20201231</td>\n",
       "      <td>what victorias coronavirus restrictions mean f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226256</th>\n",
       "      <td>20201231</td>\n",
       "      <td>whats life like as an american doctor during c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226257</th>\n",
       "      <td>20201231</td>\n",
       "      <td>womens shed canberra reskilling unemployed pan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1226258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1226253      20201231  what abc readers learned from 2020 looking bac...\n",
       "1226254      20201231  what are the south african and uk variants of ...\n",
       "1226255      20201231  what victorias coronavirus restrictions mean f...\n",
       "1226256      20201231  whats life like as an american doctor during c...\n",
       "1226257      20201231  womens shed canberra reskilling unemployed pan...\n",
       "\n",
       "[1226258 rows x 2 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431728</th>\n",
       "      <td>second teacher charged over scots school assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334778</th>\n",
       "      <td>alcohol a factor in assault increase stirling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114105</th>\n",
       "      <td>rural sa mining tees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653306</th>\n",
       "      <td>gag lifted on palm island rioter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250541</th>\n",
       "      <td>crocs seen around resort islands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text\n",
       "431728   second teacher charged over scots school assault\n",
       "334778      alcohol a factor in assault increase stirling\n",
       "1114105                              rural sa mining tees\n",
       "653306                   gag lifted on palm island rioter\n",
       "250541                   crocs seen around resort islands"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = data[['headline_text']]\n",
    "documents = data_text\n",
    "documents.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: 689750\n",
      "['broken', 'hill', 'woman', 'competes', 'for', 'miss', 'universe', 'australia']\n",
      "\n",
      "\n",
      "Tokenized and Lemmatized Document: \n",
      "['break', 'hill', 'woman', 'compet', 'miss', 'univers', 'australia']\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents.index)\n",
    "doc_sample = documents.iloc[sample].values[0]\n",
    "print(f'Original Document: {sample}')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\nTokenized and Lemmatized Document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pre-process the texts, saving the results in the *processed_docs* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764805                    [rudd, wont, wind, public, appear]\n",
       "129826                        [seek, strengthen, tie, timor]\n",
       "152039     [union, seek, probe, corbi, airport, drug, claim]\n",
       "229507                                 [seven, brumbi, riot]\n",
       "486015                               [leav, hitchhik, crash]\n",
       "1086893               [cosbi, juri, deliv, verdict, holdout]\n",
       "878429                      [power, leav, late, beat, demon]\n",
       "627628                         [apra, ask, bank, live, will]\n",
       "442389                [want, live, paradis, perish, kinglak]\n",
       "1065663                    [fake, news, trump, blast, media]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary from *processed_docs* that contains the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the tokens that appear in\n",
    "less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of the total size of the corpus, not an absolute number).\n",
    "After the previous two steps, keep only the first 100,000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we create a dictionary that informs how many\n",
    "words and how many times those words appear.\n",
    "\n",
    "We put this in the *bow_corpus* object, then check our previously selected document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document: 533361\n",
      "captive bred wallabies breeding in wild \n",
      "\n",
      "Bag of Words (BoW):\n",
      "\n",
      "[(982, 2), (2069, 1), (4153, 1), (5099, 1)] \n",
      "\n",
      "Word 982 (\"breed\") appears 2 time.\n",
      "Word 2069 (\"wild\") appears 1 time.\n",
      "Word 4153 (\"wallabi\") appears 1 time.\n",
      "Word 5099 (\"captiv\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents.index)\n",
    "\n",
    "doc_sample = documents.iloc[sample].values[0]\n",
    "print(f'\\nOriginal Document: {sample}')\n",
    "print(doc_sample,'\\n')\n",
    "\n",
    "print('Bag of Words (BoW):\\n')\n",
    "print(bow_corpus[sample],'\\n')\n",
    "\n",
    "bow_doc_4310 = bow_corpus[sample]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a preview of the bag of words in the preprocessed document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model object *tf-idf* using `models.TfidfModel` from \"bow_corpus\" and put it in *tfidf*, then we apply the transformation to the whole corpus and call it *corpus_tfidf*. Finally, we preview the *TF-IDF* scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decid', 'commun', 'broadcast', 'licenc'] \n",
      "\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1)] \n",
      "\n",
      "[(0, 0.5852942020878993),\n",
      " (1, 0.38405854933668493),\n",
      " (2, 0.5017732999224691),\n",
      " (3, 0.5080878695349914)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print(processed_docs[0],'\\n')\n",
    "print(bow_corpus[0],'\\n')\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=4, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many topics should we choose?** You can read [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.017*\"coronaviru\" + 0.011*\"coast\" + 0.011*\"covid\" + 0.010*\"miss\" + 0.009*\"search\" + 0.008*\"polic\" + 0.007*\"hill\" + 0.00\n",
      "Topic: 1 Word: 0.009*\"friday\" + 0.008*\"farm\" + 0.007*\"morrison\" + 0.007*\"care\" + 0.007*\"violenc\" + 0.007*\"juli\" + 0.006*\"age\" + 0.006*\"\n",
      "Topic: 2 Word: 0.024*\"news\" + 0.019*\"rural\" + 0.009*\"thursday\" + 0.008*\"grandstand\" + 0.008*\"nation\" + 0.008*\"busi\" + 0.007*\"financ\" + \n",
      "Topic: 3 Word: 0.016*\"charg\" + 0.016*\"murder\" + 0.013*\"court\" + 0.011*\"donald\" + 0.010*\"jail\" + 0.010*\"polic\" + 0.009*\"assault\" + 0.009\n",
      "Topic: 4 Word: 0.009*\"south\" + 0.008*\"north\" + 0.007*\"turnbul\" + 0.007*\"korea\" + 0.006*\"east\" + 0.006*\"asylum\" + 0.006*\"australia\" + 0.\n",
      "Topic: 5 Word: 0.021*\"crash\" + 0.012*\"polic\" + 0.012*\"driver\" + 0.010*\"die\" + 0.009*\"fatal\" + 0.009*\"woman\" + 0.008*\"road\" + 0.008*\"kil\n",
      "Topic: 6 Word: 0.014*\"trump\" + 0.011*\"elect\" + 0.009*\"countri\" + 0.008*\"govern\" + 0.008*\"hour\" + 0.007*\"health\" + 0.007*\"fund\" + 0.006*\n",
      "Topic: 7 Word: 0.012*\"drum\" + 0.012*\"market\" + 0.011*\"price\" + 0.010*\"rise\" + 0.010*\"share\" + 0.008*\"rat\" + 0.007*\"abbott\" + 0.006*\"fal\n",
      "Topic: 8 Word: 0.009*\"weather\" + 0.009*\"farmer\" + 0.008*\"monday\" + 0.007*\"andrew\" + 0.007*\"climat\" + 0.007*\"drought\" + 0.006*\"export\" +\n",
      "Topic: 9 Word: 0.012*\"interview\" + 0.009*\"final\" + 0.009*\"world\" + 0.008*\"australia\" + 0.007*\"leagu\" + 0.006*\"scott\" + 0.006*\"michael\" \n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic[:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation classifying the sample document using the LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 0.6003087124903306), (440, 0.7997683725355745)]"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document: 1170265\n",
      "asteroids and apocalypse and life on earth \n",
      "\n",
      "Score: 0.5456940531730652\t Topic: 0.024*\"news\" + 0.019*\"rural\" + 0.009*\"thursday\" + 0.008*\"grandstand\" + 0.008*\"nation\"\n",
      "Score: 0.20384711027145386\t Topic: 0.021*\"crash\" + 0.012*\"polic\" + 0.012*\"driver\" + 0.010*\"die\" + 0.009*\"fatal\"\n",
      "Score: 0.03131704032421112\t Topic: 0.009*\"weather\" + 0.009*\"farmer\" + 0.008*\"monday\" + 0.007*\"andrew\" + 0.007*\"climat\"\n",
      "Score: 0.03131170570850372\t Topic: 0.014*\"trump\" + 0.011*\"elect\" + 0.009*\"countri\" + 0.008*\"govern\" + 0.008*\"hour\"\n",
      "Score: 0.03131140395998955\t Topic: 0.009*\"south\" + 0.008*\"north\" + 0.007*\"turnbul\" + 0.007*\"korea\" + 0.006*\"east\"\n",
      "Score: 0.03130675479769707\t Topic: 0.012*\"drum\" + 0.012*\"market\" + 0.011*\"price\" + 0.010*\"rise\" + 0.010*\"share\"\n",
      "Score: 0.03130556643009186\t Topic: 0.009*\"friday\" + 0.008*\"farm\" + 0.007*\"morrison\" + 0.007*\"care\" + 0.007*\"violenc\"\n",
      "Score: 0.03130359575152397\t Topic: 0.012*\"interview\" + 0.009*\"final\" + 0.009*\"world\" + 0.008*\"australia\" + 0.007*\"leagu\"\n",
      "Score: 0.03130142763257027\t Topic: 0.017*\"coronaviru\" + 0.011*\"coast\" + 0.011*\"covid\" + 0.010*\"miss\" + 0.009*\"search\"\n",
      "Score: 0.031301338225603104\t Topic: 0.016*\"charg\" + 0.016*\"murder\" + 0.013*\"court\" + 0.011*\"donald\" + 0.010*\"jail\"\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents.index)\n",
    "\n",
    "doc_sample = documents.iloc[sample].values[0]\n",
    "\n",
    "print(f'\\nOriginal Document: {sample}')\n",
    "print(doc_sample,'\\n')\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with a document not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document:\n",
      "How a Pentagon deal became an identity crisis for Google \n",
      "\n",
      "Score: 0.545668363571167\t Topic: 0.024*\"news\" + 0.019*\"rural\" + 0.009*\"thursday\" + 0.008*\"grandstand\" + 0.008*\"nation\"\n",
      "Score: 0.203868106007576\t Topic: 0.021*\"crash\" + 0.012*\"polic\" + 0.012*\"driver\" + 0.010*\"die\" + 0.009*\"fatal\"\n",
      "Score: 0.031321410089731216\t Topic: 0.009*\"weather\" + 0.009*\"farmer\" + 0.008*\"monday\" + 0.007*\"andrew\" + 0.007*\"climat\"\n",
      "Score: 0.03131203353404999\t Topic: 0.014*\"trump\" + 0.011*\"elect\" + 0.009*\"countri\" + 0.008*\"govern\" + 0.008*\"hour\"\n",
      "Score: 0.0313115268945694\t Topic: 0.009*\"south\" + 0.008*\"north\" + 0.007*\"turnbul\" + 0.007*\"korea\" + 0.006*\"east\"\n",
      "Score: 0.03130674362182617\t Topic: 0.012*\"drum\" + 0.012*\"market\" + 0.011*\"price\" + 0.010*\"rise\" + 0.010*\"share\"\n",
      "Score: 0.03130554035305977\t Topic: 0.009*\"friday\" + 0.008*\"farm\" + 0.007*\"morrison\" + 0.007*\"care\" + 0.007*\"violenc\"\n",
      "Score: 0.03130355849862099\t Topic: 0.012*\"interview\" + 0.009*\"final\" + 0.009*\"world\" + 0.008*\"australia\" + 0.007*\"leagu\"\n",
      "Score: 0.031301386654376984\t Topic: 0.017*\"coronaviru\" + 0.011*\"coast\" + 0.011*\"covid\" + 0.010*\"miss\" + 0.009*\"search\"\n",
      "Score: 0.03130129724740982\t Topic: 0.016*\"charg\" + 0.016*\"murder\" + 0.013*\"court\" + 0.011*\"donald\" + 0.010*\"jail\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "\n",
    "print(f'\\nOriginal Document:')\n",
    "print(unseen_document,'\\n')\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "tfidf_vector = tfidf[bow_vector]\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Example: Airlines Tweets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset can be found in [Kaggle](https://www.kaggle.com/c/spanish-arilines-tweets-sentiment-analysis/data?select=tweets_public.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = nlp('Hola como estas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text,nlp):\n",
    "    # can be parallelized\n",
    "    doc = nlp(text)\n",
    "    lemma = [n.lemma_ for n in doc]\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def preprocess(text,nlp):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokenizes el texto\n",
    "        token = ''.join(x for x in token.lower() if x.isalpha())\n",
    "        token = re.sub(r'http*','',token)\n",
    "        #token = re.sub(r'\\s\\s+',' ',token)\n",
    "        if token not in palabrasVacias_nltk and len(token) > 2:\n",
    "            result.append(token)\n",
    "        result = lemmatize(' '.join(result),nlp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Parallel\n",
    "\n",
    "# batch_size=200 is approx 8.3 gb of RAM\n",
    "for essay in nlp.pipe(documents_spanish['text'], batch_size=200, n_process=4):\n",
    "    # is_parsed is deprecated\n",
    "    if essay.has_annotation(\"DEP\"):\n",
    "        lemma.append([n.lemma_ for n in essay])\n",
    "\n",
    "\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        lemma.append(None)\n",
    "\n",
    "\n",
    "documents_spanish['Lemas']  = lemma\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spanish = pd.read_csv('../Datos/tweets_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>926419989107798016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>934854385577943041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>945318406441635840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>927540721296568320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>947965901332197376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @iberiaexpress especialistas en dejart...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Dec 28 22:34:23 +0000 2017</td>\n",
       "      <td>946509662341554176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Con @Iberia, mi destino a un solo click. ¡Dese...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed Nov 29 18:59:49 +0000 2017</td>\n",
       "      <td>935946417495035904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia Muy bien. Muchas gracias</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Dec 26 21:38:36 +0000 2017</td>\n",
       "      <td>945770846949396480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Greenland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Es que volar con Ryanair es peor que irte a ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Dec 19 09:08:35 +0000 2017</td>\n",
       "      <td>943045386570223616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Iberia inaugura un nuevo espacio Premium para ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Nov 28 16:30:58 +0000 2017</td>\n",
       "      <td>935546571663527936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7867 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline_sentiment  is_reply  ...  tweet_location               user_timezone\n",
       "0              neutral     False  ...             NaN                      Madrid\n",
       "1              neutral      True  ...             NaN                 Mexico City\n",
       "2             negative     False  ...             NaN                      Madrid\n",
       "3             negative     False  ...             NaN  Pacific Time (US & Canada)\n",
       "4             positive      True  ...             NaN                Buenos Aires\n",
       "...                ...       ...  ...             ...                         ...\n",
       "7862          negative      True  ...             NaN                         NaN\n",
       "7863           neutral     False  ...             NaN  Eastern Time (US & Canada)\n",
       "7864          positive      True  ...             NaN                   Greenland\n",
       "7865          negative     False  ...             NaN      Atlantic Time (Canada)\n",
       "7866          negative     False  ...             NaN                      Madrid\n",
       "\n",
       "[7867 rows x 10 columns]"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5694</th>\n",
       "      <td>@Ryanair solicita licencia en #UK en previsión...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>@diana_twittea @JenHerranz @Iberia @iberiaexpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7155</th>\n",
       "      <td>@Iberia Estimados:  estoy intentando hacer el ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>@ibarretxec @Iberia Otra historia, vuelo de Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>Q plomo resultó ni viaje a Londres en @Iberia....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "5694  @Ryanair solicita licencia en #UK en previsión...\n",
       "5402  @diana_twittea @JenHerranz @Iberia @iberiaexpr...\n",
       "7155  @Iberia Estimados:  estoy intentando hacer el ...\n",
       "6666  @ibarretxec @Iberia Otra historia, vuelo de Ma...\n",
       "5408  Q plomo resultó ni viaje a Londres en @Iberia...."
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_spanish = data_spanish[['text']]\n",
    "documents_spanish = data_text_spanish\n",
    "documents_spanish.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Un año de estos nos unirán con el resto de ibe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Hoy iberia cumple 90 años de su primer vuelo. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>@Zane_GH Pues menudo timo @Zane_GH @ibexpress_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>@AeroAsturias @aena Hoy en el aeropuerto en la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Todo en exceso es malo, menos ¡viajar! #HolaEu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7803</th>\n",
       "      <td>#note8abordo, samsung regala teléfonos a todos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7840</th>\n",
       "      <td>@InfoViajera una duda,  viajo a guarulhos. lle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7841</th>\n",
       "      <td>Lo de que iberia ponga café aguado y leche en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>@elespectador Si es iberia voy a pensar que a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>@Iberia @iberiaexpress especialistas en dejart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "14    Un año de estos nos unirán con el resto de ibe...\n",
       "66    Hoy iberia cumple 90 años de su primer vuelo. ...\n",
       "76    @Zane_GH Pues menudo timo @Zane_GH @ibexpress_...\n",
       "88    @AeroAsturias @aena Hoy en el aeropuerto en la...\n",
       "91    Todo en exceso es malo, menos ¡viajar! #HolaEu...\n",
       "...                                                 ...\n",
       "7803  #note8abordo, samsung regala teléfonos a todos...\n",
       "7840  @InfoViajera una duda,  viajo a guarulhos. lle...\n",
       "7841  Lo de que iberia ponga café aguado y leche en ...\n",
       "7848  @elespectador Si es iberia voy a pensar que a ...\n",
       "7862  @Iberia @iberiaexpress especialistas en dejart...\n",
       "\n",
       "[446 rows x 1 columns]"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_spanish[documents_spanish['text'].str.contains('iberia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Iberia @KilliMR Pues realojad en vuelos de otras compañias que sabemos perfectamente que podéis o alquilad y fleta… https://t.co/JiUbV3nG4B'"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: 2134\n",
      "['Viajo', 'con', '@Iberia', 'Valecia-', 'Melilla', 'con', 'transbordo', 'en', 'Madrid', 'y', 'no', 'me', 'llega', 'la', 'maleta', 'facturada,', 'como', 'siempre!', 'Si', 'no…', 'https://t.co/wsIHbRT6xx']\n",
      "Lemmatized text\n",
      "['viajar', 'con', '@iberia', 'valecia-', 'Melilla', 'con', 'transbordo', 'en', 'Madrid', 'y', 'no', 'yo', 'llegar', 'el', 'maleta', 'facturado', ',', 'como', 'siempre', '!', 'si', 'no', '…', 'https://t.co/wsihbrt6xx']\n",
      "Clean text\n",
      "['viajar', 'iberio', 'valecia', 'melilla', 'transbordo', 'madrid', 'llegar', 'maleta', 'facturado', 'siempre', 'wsihbrt']\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents_spanish.index)\n",
    "doc_sample = documents_spanish.iloc[sample].values[0]\n",
    "print(f'Original Document: {sample}')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('Lemmatized text')\n",
    "print(lemmatize(doc_sample,nlp))\n",
    "print('Clean text')\n",
    "print(preprocess(doc_sample,nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pre-process the texts, saving the results in the *processed_docs* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs_spanish = documents_spanish['text'].apply(lambda x: preprocess(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['trabajar', 'ryanair', 'tmar', 'ruuarbe', 'empleo']),\n",
       "       list(['iberia', 'fionaferrer', 'gustar', 'cancún', 'viajar', 'disfrutar', 'manera', 'igual']),\n",
       "       list(['sabiai', 'iberia', 'tratar', 'bien', 'santiago', 'chile', 'cambiar', 'asiento', 'mandar', 'volar', 'trasero', 'uansbonn']),\n",
       "       list(['nunca', 'nunca', 'nunca', 'pidar', 'café', 'ryanair', 'bueno', 'vender', 'bordo']),\n",
       "       list(['cristortu', 'dakar', 'iberia', 'mitsubishies', 'bfgoodricheu', 'burgostur', 'astintlogistics', 'uremovil', 'karbium', 'éxito', 'vrkyvu']),\n",
       "       list(['mgd', 'wow', 'bonito', 'solo', 'volado', 'uno', 'vez', 'iberia', 'siempre', 'tierra']),\n",
       "       list(['iberia', 'plus', 'cumplir', 'año', 'querer', 'celebrar', 'él', 'contigo', 'manera', 'especial', 'elegir', 'número', 'favorito', 'wuujr', 'doge']),\n",
       "       list(['barómetro', 'business', 'iberia', 'vueling', 'compañía', 'aéreo', 'utilizado', 'viaje', 'jyr']),\n",
       "       list(['iberia', 'felicitación', 'iberia']),\n",
       "       list(['cbellolio', 'iberia', 'nuevopudahuel', 'chuata', 'madrid', 'stgo', 'par', 'semana', 'cruzar', 'dedo'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs_spanish.head(10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary from *processed_docs* that contains the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 empleo\n",
      "1 ruuarbe\n",
      "2 ryanair\n",
      "3 tmar\n",
      "4 trabajar\n",
      "5 cancún\n",
      "6 disfrutar\n",
      "7 fionaferrer\n",
      "8 gustar\n",
      "9 iberia\n",
      "10 igual\n"
     ]
    }
   ],
   "source": [
    "dictionary_spanish = gensim.corpora.Dictionary(processed_docs_spanish)\n",
    "count = 0\n",
    "for k, v in dictionary_spanish.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iberia', 4265),\n",
       " ('ryanair', 2128),\n",
       " ('vuelo', 1356),\n",
       " ('iberio', 1308),\n",
       " ('hacer', 612),\n",
       " ('hola', 574),\n",
       " ('destino', 545),\n",
       " ('poder', 497),\n",
       " ('madrid', 456),\n",
       " ('mejor', 435)]"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_spanish.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chicrevista', 1),\n",
       " ('maldad', 1),\n",
       " ('tana', 1),\n",
       " ('xcroh', 1),\n",
       " ('saplj', 1),\n",
       " ('especialista', 1),\n",
       " ('jguhm', 1),\n",
       " ('wbi', 1),\n",
       " ('chingar', 1),\n",
       " ('dbkvzo', 1)]"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_spanish.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the tokens that appear in\n",
    "less than 15 documents (absolute number) or\n",
    "more than 0.5 documents (fraction of the total size of the corpus, not an absolute number).\n",
    "After the previous two steps, keep only the first 100,000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_spanish.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ryanair', 2128),\n",
       " ('vuelo', 1356),\n",
       " ('iberio', 1308),\n",
       " ('hacer', 612),\n",
       " ('hola', 574),\n",
       " ('destino', 545),\n",
       " ('poder', 497),\n",
       " ('madrid', 456),\n",
       " ('mejor', 435),\n",
       " ('solo', 434)]"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_spanish.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ejemplo', 15),\n",
       " ('palma', 15),\n",
       " ('estar', 15),\n",
       " ('encima', 15),\n",
       " ('bilbao', 15),\n",
       " ('bogotá', 15),\n",
       " ('rumbo', 15),\n",
       " ('imaginar', 15),\n",
       " ('diferente', 15),\n",
       " ('tren', 15)]"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_spanish.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we create a dictionary that informs how many\n",
    "words and how many times those words appear.\n",
    "\n",
    "We put this in the *bow_corpus* object, then check our previously selected document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary_spanish.doc2bow(doc) for doc in processed_docs_spanish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document: 7427\n",
      "@Iberia @vueling hola, me sabríais decir en que horarios van a ocurrir las huelgas de vuestros funcionarios? cuanto… https://t.co/XGCz7vMkq4 \n",
      "\n",
      "Bag of Words (BoW):\n",
      "\n",
      "[(41, 1), (54, 1), (61, 1), (189, 1), (256, 1), (372, 1), (392, 1), (443, 1)] \n",
      "\n",
      "Word 41 (\"vueling\") appears 1 time.\n",
      "Word 54 (\"hola\") appears 1 time.\n",
      "Word 61 (\"decir\") appears 1 time.\n",
      "Word 189 (\"huelga\") appears 1 time.\n",
      "Word 256 (\"ir\") appears 1 time.\n",
      "Word 372 (\"cuanto\") appears 1 time.\n",
      "Word 392 (\"ocurrir\") appears 1 time.\n",
      "Word 443 (\"horario\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents_spanish.index)\n",
    "\n",
    "doc_sample = documents_spanish.iloc[sample].values[0]\n",
    "print(f'\\nOriginal Document: {sample}')\n",
    "print(doc_sample,'\\n')\n",
    "\n",
    "print('Bag of Words (BoW):\\n')\n",
    "print(bow_corpus[sample],'\\n')\n",
    "\n",
    "bow_doc_4310 = bow_corpus[sample]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary_spanish[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a preview of the bag of words in the preprocessed document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model object *tf-idf* using *models.TfidfModel* from \"bow_corpus\" and put it in *tfidf*, then we apply the transformation to the whole corpus and call it *corpus_tfidf*. Finally, we preview the *TF-IDF* scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trabajar', 'ryanair', 'tmar', 'ruuarbe', 'empleo'] \n",
      "\n",
      "[(0, 1), (1, 1), (2, 1)] \n",
      "\n",
      "[(0, 0.7085014039582359), (1, 0.17743377739348923), (2, 0.6830395414828386)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print(processed_docs_spanish[0],'\\n')\n",
    "print(bow_corpus[0],'\\n')\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary_spanish, passes=4, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many topics should we choose?** You can read [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"querer\" + 0.020*\"número\" + 0.019*\"plus\" + 0.019*\"cumplir\" + 0.018*\"especial\" + 0.018*\"elegir\" + 0.017*\"favorito\" \n",
      "Topic: 1 Word: 0.019*\"vuelo\" + 0.016*\"seguir\" + 0.016*\"poder\" + 0.014*\"ryanair\" + 0.012*\"iberio\" + 0.010*\"caso\" + 0.010*\"ser\" + 0.009*\"\n",
      "Topic: 2 Word: 0.033*\"ryanair\" + 0.028*\"spanair\" + 0.027*\"accidente\" + 0.026*\"piloto\" + 0.026*\"españa\" + 0.026*\"huelga\" + 0.026*\"así\" +\n",
      "Topic: 3 Word: 0.030*\"vuelo\" + 0.030*\"iberio\" + 0.027*\"madrid\" + 0.021*\"ryanair\" + 0.019*\"vueling\" + 0.015*\"buen\" + 0.013*\"vía\" + 0.011\n",
      "Topic: 4 Word: 0.022*\"pasar\" + 0.019*\"iberio\" + 0.018*\"ir\" + 0.018*\"ryanair\" + 0.016*\"hacer\" + 0.016*\"viajar\" + 0.011*\"poder\" + 0.011*\"\n",
      "Topic: 5 Word: 0.024*\"iberio\" + 0.022*\"hacer\" + 0.021*\"ryanair\" + 0.019*\"año\" + 0.018*\"gracia\" + 0.017*\"mucho\" + 0.015*\"feliz\" + 0.014*\n",
      "Topic: 6 Word: 0.055*\"ryanair\" + 0.018*\"express\" + 0.014*\"empresa\" + 0.014*\"billete\" + 0.014*\"decir\" + 0.014*\"euros\" + 0.013*\"iberio\" +\n",
      "Topic: 7 Word: 0.020*\"ryanair\" + 0.016*\"iberio\" + 0.013*\"nuevo\" + 0.013*\"dar\" + 0.011*\"vuelo\" + 0.011*\"él\" + 0.010*\"persona\" + 0.010*\"e\n",
      "Topic: 8 Word: 0.066*\"destino\" + 0.046*\"suerte\" + 0.046*\"hola\" + 0.046*\"europeo\" + 0.045*\"clickir\" + 0.045*\"deseadme\" + 0.045*\"mejor\" +\n",
      "Topic: 9 Word: 0.021*\"gracias\" + 0.021*\"ryanair\" + 0.018*\"iberio\" + 0.018*\"pasajero\" + 0.013*\"vuelo\" + 0.013*\"equipaje\" + 0.013*\"avión\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic[:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation classifying the sample document using the LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document: 2069\n",
      "VIDEO: Reconstruyen el trágico accidente de Spanair ocurrido en Madrid en 2008 https://t.co/RfXoncfMqI https://t.co/LpoASexogj \n",
      "\n",
      "Score: 0.4162733554840088\t Topic: 0.033*\"ryanair\" + 0.028*\"spanair\" + 0.027*\"accidente\" + 0.026*\"piloto\" + 0.026*\"españa\"\n",
      "Score: 0.2856001555919647\t Topic: 0.030*\"vuelo\" + 0.030*\"iberio\" + 0.027*\"madrid\" + 0.021*\"ryanair\" + 0.019*\"vueling\"\n",
      "Score: 0.037270687520504\t Topic: 0.021*\"gracias\" + 0.021*\"ryanair\" + 0.018*\"iberio\" + 0.018*\"pasajero\" + 0.013*\"vuelo\"\n",
      "Score: 0.037269432097673416\t Topic: 0.020*\"ryanair\" + 0.016*\"iberio\" + 0.013*\"nuevo\" + 0.013*\"dar\" + 0.011*\"vuelo\"\n",
      "Score: 0.037266235798597336\t Topic: 0.022*\"pasar\" + 0.019*\"iberio\" + 0.018*\"ir\" + 0.018*\"ryanair\" + 0.016*\"hacer\"\n",
      "Score: 0.037265997380018234\t Topic: 0.019*\"vuelo\" + 0.016*\"seguir\" + 0.016*\"poder\" + 0.014*\"ryanair\" + 0.012*\"iberio\"\n",
      "Score: 0.037264082580804825\t Topic: 0.020*\"querer\" + 0.020*\"número\" + 0.019*\"plus\" + 0.019*\"cumplir\" + 0.018*\"especial\"\n",
      "Score: 0.03726400062441826\t Topic: 0.024*\"iberio\" + 0.022*\"hacer\" + 0.021*\"ryanair\" + 0.019*\"año\" + 0.018*\"gracia\"\n",
      "Score: 0.037263400852680206\t Topic: 0.055*\"ryanair\" + 0.018*\"express\" + 0.014*\"empresa\" + 0.014*\"billete\" + 0.014*\"decir\"\n",
      "Score: 0.037262678146362305\t Topic: 0.066*\"destino\" + 0.046*\"suerte\" + 0.046*\"hola\" + 0.046*\"europeo\" + 0.045*\"clickir\"\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.choice(documents_spanish.index)\n",
    "\n",
    "doc_sample = documents_spanish.iloc[sample].values[0]\n",
    "\n",
    "print(f'\\nOriginal Document: {sample}')\n",
    "print(doc_sample,'\\n')\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(doc_sample,nlp))\n",
    "tfidf_vector = tfidf[bow_vector]\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro documento de prueba tiene la mayor probabilidad de ser parte del tema que asignó nuestro modelo, que es la clasificación precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with a document not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Document:\n",
      "Terrible el servicio brindado. No volaré nunca más con ustedes. \n",
      "\n",
      "Score: 0.41381752490997314\t Topic: 0.033*\"ryanair\" + 0.028*\"spanair\" + 0.027*\"accidente\" + 0.026*\"piloto\" + 0.026*\"españa\"\n",
      "Score: 0.28805580735206604\t Topic: 0.030*\"vuelo\" + 0.030*\"iberio\" + 0.027*\"madrid\" + 0.021*\"ryanair\" + 0.019*\"vueling\"\n",
      "Score: 0.03727062791585922\t Topic: 0.021*\"gracias\" + 0.021*\"ryanair\" + 0.018*\"iberio\" + 0.018*\"pasajero\" + 0.013*\"vuelo\"\n",
      "Score: 0.03726978972554207\t Topic: 0.020*\"ryanair\" + 0.016*\"iberio\" + 0.013*\"nuevo\" + 0.013*\"dar\" + 0.011*\"vuelo\"\n",
      "Score: 0.03726619854569435\t Topic: 0.022*\"pasar\" + 0.019*\"iberio\" + 0.018*\"ir\" + 0.018*\"ryanair\" + 0.016*\"hacer\"\n",
      "Score: 0.03726596385240555\t Topic: 0.019*\"vuelo\" + 0.016*\"seguir\" + 0.016*\"poder\" + 0.014*\"ryanair\" + 0.012*\"iberio\"\n",
      "Score: 0.037264056503772736\t Topic: 0.020*\"querer\" + 0.020*\"número\" + 0.019*\"plus\" + 0.019*\"cumplir\" + 0.018*\"especial\"\n",
      "Score: 0.03726397454738617\t Topic: 0.024*\"iberio\" + 0.022*\"hacer\" + 0.021*\"ryanair\" + 0.019*\"año\" + 0.018*\"gracia\"\n",
      "Score: 0.037263378500938416\t Topic: 0.055*\"ryanair\" + 0.018*\"express\" + 0.014*\"empresa\" + 0.014*\"billete\" + 0.014*\"decir\"\n",
      "Score: 0.03726266324520111\t Topic: 0.066*\"destino\" + 0.046*\"suerte\" + 0.046*\"hola\" + 0.046*\"europeo\" + 0.045*\"clickir\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Terrible el servicio brindado. No volaré nunca más con ustedes.'\n",
    "\n",
    "print(f'\\nOriginal Document:')\n",
    "print(unseen_document,'\\n')\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(doc_sample,nlp))\n",
    "tfidf_vector = tfidf[bow_vector]\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Go Back]](#Content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
