{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2231055-6a63-4425-9248-c5aae455396e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/logo_final.png\"  align=\"left\"/> \n",
    "</center>   \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce5373-7375-4e11-872a-d58813d66c24",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>BERT-Fine-tuning</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30115409-9d8e-45df-a9cd-072858c02397",
   "metadata": {},
   "source": [
    "<center>Ajuste fino de modelos pre-entrenados </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23484740-ab13-447a-bc3f-01686f528f49",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405456a4-e556-41d2-bf60-593a5055f8b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaec87-e77d-4152-bb83-bc9e62d4a94c",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Diseño gráfico y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3be8e-e519-43bf-9bd9-9903a27b570b",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro Reyes, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4663a-f0f2-42d8-a044-6e3ab6d16e3e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b038974-8718-4142-a66e-895520f68ca7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a15511c-982d-4ff7-96fd-a6f1c95f39a1",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d6ac6-19de-4dcf-8904-a9e158c84d34",
   "metadata": {},
   "source": [
    "1. [Zero-shot learning](https://en.wikipedia.org/wiki/Zero-shot_learning)\n",
    "1. [HuggingFace BERT model](https://huggingface.co/transformers/model_doc/bert.html)\n",
    "1. [Getting Started with Google BERT: Build and train state-of-the-art natural language processing models using BERT](http://library.lol/main/A0CA3A1276D07957FD7B28F843C299BA)\n",
    "1. [Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more](http://library.lol/main/A8C97E552646B3F194ECA333221CEE88)\n",
    "1. [HuggingFace. Transformers ](https://huggingface.co/transformers/)\n",
    "1. [HuggingFace. Intro pipeline](https://huggingface.co/course/chapter1/3?fw=pt)\n",
    "1. [Tutorial Transformer de Google](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "1. [Transformer-chatbot-tutorial-with-tensorflow-2](https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html) \n",
    "1. [Transformer Architecture: The positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "1. [Illustrated Auto-attención](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "1. [Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458)\n",
    "1. [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et. al, 2015)](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "1. [Effective Approaches to Attention-based Neural Machine Translation (Luong et. al, 2015)](https://arxiv.org/pdf/1508.04025.pdf)\n",
    "1. [Attention Is All You Need (Vaswani et. al, 2017)](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "1. [Self-Attention GAN (Zhang et. al, 2018)](https://arxiv.org/pdf/1805.08318.pdf)\n",
    "1. [Sequence to Sequence Learning with Neural Networks (Sutskever et. al, 2014)](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "1. [TensorFlow’s seq2seq Tutorial with Attention (Tutorial on seq2seq+attention)](https://github.com/tensorflow/nmt)\n",
    "1. [Lilian Weng’s Blog on Attention (Great start to attention)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)\n",
    "1. [Jay Alammar’s Blog on Seq2Seq with Attention (Great illustrations and worked example on seq2seq+attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n",
    "1. [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (Wu et. al, 2016)](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "1. [Adam: A method for stochastic optimization](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "1.[Stanford IMBS sentiment analysis data base](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9be15-8ae8-4715-b57f-a6e8a8111c0c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a382773-c33f-4247-b44c-6e3806b9af20",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Clasificación de textos](#Clasificación-de-textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c46451-72e1-4588-b271-855af1898df1",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cd6c6-0240-4bd0-b2e6-427aff064ebb",
   "metadata": {},
   "source": [
    "Usaremos la implementación de HuggingFace in en Pytorch. \n",
    "\n",
    "En esta lección revisamo como hacer el ajuste-fino de modelos preentrenados para la siguientes tareas de lenguaje natural.\n",
    "\n",
    "* Clasificación de textos.\n",
    "* Inferencia de lenguaje natural.\n",
    "* Reconocimiento de nombres de entidades. NER.\n",
    "* Pregunta-Respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ab415-c6d7-4d89-bbae-098b30f95578",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Clasificación de textos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd69e7e-8b83-4a1b-ad59-8d5f01381baa",
   "metadata": {},
   "source": [
    "La tarea de PLN es análisis de sentimiento. El primer experimento lo hacemos en inglés y luego en Español. Para esta tarea esta bién usar el modelo uncase (eliminando mayúsculas). \n",
    "\n",
    "Los datos corresponde a [Stanford IMBS sentiment analysis data base](http://ai.stanford.edu/~amaas/data/sentiment/). Son 25K comentarios para entrenamiento y 25K para prueba. Los datos fueron obtenidos de [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/version/1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e8155-a37a-402b-905b-a20ff14a4861",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50d8bb14-8674-4da3-aea2-438283f60125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('../Datos/IMDB/IMDB Dataset.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "#df[\"sentiment\"] = df[\"sentiment\"].astype('category')\n",
    "\n",
    "df.rename(columns = {'review': 'text', 'sentiment':'label'}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86af0db0-2282-4032-89c7-870b56d099ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "label    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6ec1410-7acb-4aab-92d9-a7a749a86770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "text       object\n",
      "label    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16a8a908-8336-42b6-8bca-f6f8e9263858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62260ea-53c3-41ba-933d-547604d4c0fa",
   "metadata": {},
   "source": [
    "#### Construyendo nlp.Dataset a partir de Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b42d4f4-f892-4335-af59-691465c3fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa los datos en entranamiento y validación\n",
    "import numpy as np\n",
    "\n",
    "# inicializa el generador de pseudo-aleatorios\n",
    "#np.random.RandomState(2021)\n",
    "np.random.seed(2021)\n",
    "size = df.shape[0]\n",
    "\n",
    "train_size = int(size*0.7)\n",
    "test_size = size - train_size\n",
    "\n",
    "train_index = np.random.choice(np.arange(size), size=train_size, replace=False)\n",
    "test_index = np.array(list(set(np.arange(size)) - set(train_index)))\n",
    "\n",
    "# separa los conjuntos de datos\n",
    "#train_dataset = df.loc[train_index]\n",
    "#test_dataset = df.loc[test_idx]\n",
    "#train_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# crea el npl.Dataset\n",
    "from nlp import Dataset\n",
    "train_ds = Dataset.from_pandas(df.loc[train_index])\n",
    "test_ds = Dataset.from_pandas(df.loc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d54ab3b-7786-4ad6-a513-b7e3f9e7dd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}, num_rows: 35000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cc54e-bdeb-48b8-b0eb-fdb2d7f54d59",
   "metadata": {},
   "source": [
    "Alternativamente podemos leer todos los datos y subdividirlos en el datses así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da6af5-05c0-46dc-b144-84d247e70c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = Dataset.from_pandas(df)\n",
    "#dset = dset.train_test_split(test_size=0.3)# por defecto shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6f6d900-1518-4261-9f04-629fa6b089d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DatasetDict in module nlp.dataset_dict object:\n",
      "\n",
      "class DatasetDict(builtins.dict)\n",
      " |  A dictionary (dict of str: nlp.Dataset) with dataset transforms methods (map, filter, etc.)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DatasetDict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  cast_(self, features: nlp.features.Features)\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `feature` but :func:`cast_`\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          features (:class:`nlp.Features`): New features to cast the dataset to.\n",
      " |              The name and order of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
      " |  \n",
      " |  filter(self, function, with_indices=False, batch_size: Union[int, NoneType] = 1000, remove_columns: Union[List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, str], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, verbose: bool = True) -> 'DatasetDict'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          `function` (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict) -> bool` if `with_indices=False`\n",
      " |              - `function(example: Dict, indices: int) -> bool` if `with_indices=True`\n",
      " |          `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          `cache_file_names` (`Optional[Dict[str, str]]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          `verbose` (`bool`, default: `True`): Set to `False` to deactivate the tqdm progress bar and informations.\n",
      " |  \n",
      " |  formated_as(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      To be used in a `with` statement. Set __getitem__ return format (type and columns)\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formated columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  map(self, function, with_indices: bool = False, batched: bool = False, batch_size: Union[int, NoneType] = 1000, remove_columns: Union[List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, str], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, features: Union[nlp.features.Features, NoneType] = None, disable_nullable: bool = True, verbose: bool = True) -> 'DatasetDict'\n",
      " |      Apply a function to all the elements in the table (individually or in batches)\n",
      " |      and update the table (if function does updated examples).\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          `function` (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n",
      " |              - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n",
      " |              - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n",
      " |              - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n",
      " |          `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          `batched` (`bool`, default: `False`): Provide batch of examples to `function`\n",
      " |          `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          `cache_file_names` (`Optional[Dict[str, str]]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          `features` (`Optional[nlp.Features]`, default: `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          `disable_nullable` (`bool`, default: `True`): Allow null values in the table.\n",
      " |          `verbose` (`bool`, default: `True`): Set to `False` to deactivate the tqdm progress bar and informations.\n",
      " |  \n",
      " |  remove_column_(self, column_name: str)\n",
      " |      Remove a column in the dataset and the features associated to the column.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_name (:obj:`str`): Name of the column to remove.\n",
      " |  \n",
      " |  rename_column_(self, original_column_name: str, new_column_name: str)\n",
      " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also rename a column using :func:`Dataset.map` with `remove_columns` but the present method:\n",
      " |          - takes care of moving the original features under the new column name.\n",
      " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
      " |          new_column_name (:obj:`str`): New name for the column.\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset __getitem__ return format to python objects and all columns.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Same as ``self.set_format()``\n",
      " |  \n",
      " |  set_format(self, type: Union[str, NoneType] = None, columns: Union[List, NoneType] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set __getitem__ return format (type and columns)\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas']\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (Optional ``List[str]``): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (``bool`` default to False): keep un-formated columns as well in the output (as python objects)\n",
      " |          format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  shuffle(self, seeds: Union[Dict[str, int], NoneType] = None, generators: Union[Dict[str, numpy.random._generator.Generator], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, str], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, verbose: bool = True)\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Args:\n",
      " |          `seeds` (Optional `Dict[str, int]`): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |              You have to provide one :obj:`seed` per dataset in the dataset dictionary.\n",
      " |          `generators` (Optional `Dict[str, np.random.Generator]`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |              You have to provide one :obj:`generator` per dataset in the dataset dictionary.\n",
      " |          `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          `cache_file_names` (`Optional[Dict[str, str]]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          `verbose` (`bool`, default: `True`): Set to `False` to deactivate the tqdm progress bar and informations.\n",
      " |  \n",
      " |  sort(self, column: str, reverse: bool = False, kind: str = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Union[Dict[str, str], NoneType] = None, writer_batch_size: Union[int, NoneType] = 1000, verbose: bool = True) -> 'DatasetDict'\n",
      " |      Create a new dataset sorted according to a column.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently sorting according to a column name uses numpy sorting algorithm under the hood.\n",
      " |      The column should thus be a numpy compatible type (in particular not a nested type).\n",
      " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
      " |      \n",
      " |      Args:\n",
      " |          `column` (`str`): column name to sort by.\n",
      " |          `reverse`: (`bool`, default: `False`): If True, sort by descending order rather then ascending.\n",
      " |          `kind` (Optional `str`): Numpy algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
      " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
      " |          `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          `cache_file_names` (`Optional[Dict[str, str]]`, default: `None`): Provide the name of a cache file to use to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n",
      " |          `verbose` (`bool`, default: `True`): Set to `False` to deactivate the tqdm progress bar and informations.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the dict keys.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(self, /)\n",
      " |      Remove and return a (key, value) pair as a 2-tuple.\n",
      " |      \n",
      " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      " |      Raises KeyError if the dict is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a984c0b-96db-4b8a-8d9f-ef3955c933af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(features: {'review': Value(dtype='string', id=None), 'sentiment': Value(dtype='string', id=None)}, num_rows: 35000),\n",
       " 'test': Dataset(features: {'review': Value(dtype='string', id=None), 'sentiment': Value(dtype='string', id=None)}, num_rows: 15000)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e772cb-bfda-4f07-81bc-db8b5502454d",
   "metadata": {},
   "source": [
    "#### Carga los datos desde HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2c19c-ca2c-45e8-8acd-a89be5e74ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de datasets\n",
    "from nlp import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset.id for dataset in datasets_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ebecc-e660-4ef3-a7d1-6b828d189ea1",
   "metadata": {},
   "source": [
    "Encontramos dos datasets: imdb, imdb_urdu_reviews. Vamos a explorarlos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159ad31-fe6f-41d2-8732-04097e94c20e",
   "metadata": {},
   "source": [
    "* [imdb dataset](https://huggingface.co/datasets/viewer/?dataset=imdb)\n",
    "* [imdb_urdu_reviews dataset](https://huggingface.co/datasets/viewer/?dataset=imdb_urdu_reviews). Traducción de imdb al lenguaje urdu.\n",
    "\n",
    "Usaremos de momento el dataset `imdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8224ab71-4637-4c8e-ae19-0db916367d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e80461b-7878-40ee-a701-7e3c9a6c60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nlp.dataset_dict.DatasetDict'>\n",
      "{'train': Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 25000), 'test': Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 25000), 'unsupervised': Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 50000)}\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5fc27-e663-4114-8352-d09bade3c855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
