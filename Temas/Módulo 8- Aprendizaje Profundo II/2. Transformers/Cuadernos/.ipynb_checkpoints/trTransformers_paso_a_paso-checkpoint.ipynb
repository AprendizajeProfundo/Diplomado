{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basado en [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n",
    "2.Ashish Vaswani, et al.,  [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Mecanismo de atención](#Mecanismo-de-atención)\n",
    "* [Ventanas infinitas](#Ventanas-infinitas)\n",
    "* [Atención es todo lo que se necesita](#Atención-es-todo-lo-que-se-necesita)\n",
    "* [Embeddings de entrada](#Embeddings-de-entrada)\n",
    "* [Codificación posicional](#Codificación-posicional)\n",
    "* [Capa codificadora](#Capa-codificadora)\n",
    "* [Atención de múltiples cabezas](#Atención-de-múltiples-cabezas)\n",
    "* [Cómputo de la atención  multi-cabeza](#Cómputo-de-la-atención-multi-cabeza)\n",
    "* [Las conexiones residuales, la normalización de capas y la red de retroalimentación](#Las-conexiones-residuales,-la-normalización-de-capas-y-la-red-de-retroalimentación)\n",
    "* [Envoltura del codificador](#Envoltura-del-codificador)\n",
    "* [Capa decodificador](#Capa-decodificador)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los transformers están arrasando en el mundo del procesamiento del lenguaje natural. \n",
    "\n",
    "Estos increíbles modelos están batiendo múltiples récords de PLN y están impulsando el estado del arte. \n",
    "\n",
    "Se utilizan en muchas aplicaciones como traducción de lenguaje automático, chatbots conversacionales e incluso para impulsar mejores motores de búsqueda. \n",
    "\n",
    "Los transformers están de moda en el aprendizaje profundo hoy en día, pero ¿cómo funcionan? ¿Por qué han superado al anterior rey de los problemas de secuencia, como las redes neuronales recurrentes, GRU y LSTM? \n",
    "\n",
    "Probablemente haya oído hablar de diferentes modelos de transformers famosos como BERT, GPT y GPT2. En esta lección, nos centraremos en el documento que lo inició todo, [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecanismo de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para entender los transformers primero debemos comprender el mecanismo de atención.\n",
    "\n",
    "El mecanismo de Atención permite que los transformers tengan una memoria a muy largo plazo. Un modelo de transformer puede \"atender\" o \"concentrarse\" en todos los tokens anteriores que se han generado.\n",
    "\n",
    "Veamos un ejemplo. Digamos que queremos escribir una novela corta de ciencia ficción con un transformer generativo. El siguiente es un ejemplo real obtenido usando  [Hugging Face’s](https://huggingface.co/) [Write With Transformer](https://transformer.huggingface.co/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nuestra entrada**: “As Aliens entered our planet”.\n",
    "\n",
    "**Salida del Transformer**: *“and began to colonized Earth, a certain group of extraterrestrials began to manipulate our society through their influences of a certain number of the elite to keep and iron grip over the populace.”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Mecanismo_Atencion.gif\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Mecanismo de atención</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventanas infinitas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales recurrentes (RNN) también son capaces de buscar entradas anteriores. \n",
    "\n",
    "Pero el poder del mecanismo de atención es que *no sufre de memoria a corto plazo*. \n",
    "\n",
    "Los RNN tienen una ventana más corta para hacer referencia, por lo que cuando la historia se alarga, los RNN no pueden acceder a las palabras generadas anteriormente en la secuencia. \n",
    "\n",
    "Esto sigue siendo cierto para las redes de unidades recurrentes cerradas (GRU) y de memoria a largo y corto plazo (LSTM), aunque tienen una mayor capacidad para lograr memoria a más largo plazo, por lo que tienen una ventana más larga para hacer referencia. \n",
    "\n",
    "El mecanismo de atención, en teoría, y con suficientes recursos informáticos, tiene una ventana infinita desde la que se puede hacer referencia a las palabras anteriores, por lo que es capaz de utilizar todo el contexto de la historia mientras se genera el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ventanas.gif\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Mecanismo de atención</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atención es todo lo que se necesita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El poder del mecanismo de atención se demostró en el artículo [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), donde los autores introdujeron una nueva red neuronal novedosa llamada Transformers, que es una arquitectura de tipo codificador-decodificador basada en la atención.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Arquitectura_Transformer.png\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Arquitectura Transformer</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un nivel alto, el codificador mapea una secuencia de entrada en una representación continua abstracta que contiene toda la información aprendida de esa entrada. \n",
    "\n",
    "El decodificador luego toma esa representación continua y paso a paso genera una única salida mientras también se alimenta la salida anterior.\n",
    "\n",
    "Veamos un ejemplo. El artículo aplicó el modelo Transformer a un problema de traducción automática neuronal. En esta lección, demostraremos cómo funcionará para un chatbot conversacional.\n",
    "\n",
    "\n",
    "**Nuestra entrada**: *“Hi how are you”*\n",
    "\n",
    "**Salida del Transformer**: *“I am fine”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es enviar la entrada a una capa de incrustación (embedding) de palabras. \n",
    "\n",
    "Una capa de embedding de palabras se puede considerar como una tabla de búsqueda para obtener una representación vectorial aprendida de cada palabra. \n",
    "\n",
    "Las redes neuronales aprenden a través de números, por lo que cada palabra se asigna a un vector con valores continuos para representar esa palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Input_embedding.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Embeddings de entrada</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación posicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es inyectar información posicional en las incrustaciones. \n",
    "\n",
    "Debido a que el codificador del transformer no tiene recurrencia como las redes neuronales recurrentes, debemos agregar algo de información sobre las posiciones en las incorporaciones de entrada. \n",
    "\n",
    "Esto se hace mediante codificación posicional. Los autores idearon un truco inteligente utilizando las funciones seno y coseno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/codificacion_posicional.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Codificación posicional</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para cada índice *impar* en el vector de entrada, se crea un vector usando la función *cos*. \n",
    "2. Para cada índice *par*, se crea un vector usando la función *sin*.\n",
    "\n",
    "Luego se suman  estos vectores a sus incrustaciones de entrada correspondientes. \n",
    "\n",
    "Esto le da a la red información sobre la posición de cada vector. \n",
    "\n",
    "Las funciones de seno y coseno se eligieron en conjunto porque tienen propiedades lineales que el modelo puede aprender a atender fácilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa codificadora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder. Ahora tenemos la capa del codificador. \n",
    "\n",
    "El trabajo de capas de codificadores consiste en mapear todas las secuencias de entrada en una representación continua abstracta que contiene la información aprendida para toda esa secuencia.\n",
    "\n",
    "Contiene 2 submódulos:\n",
    "\n",
    "1. atención de múltiples cabezas, \n",
    "2. una red completamente conectada. \n",
    "\n",
    "También hay conexiones residuales alrededor de cada una de las dos subcapas seguidas de una normalización de capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Encoder_layers.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Módulo de codificación</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atención de múltiples cabezas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-headed attention. La atención de múltiples cabezas en el codificador aplica un mecanismo de atención específico llamado auto-atención (self-attention). \n",
    "\n",
    "La auto-atención permite a los modelos asociar cada palabra en la entrada con otras palabras. \n",
    "\n",
    "Entonces, en nuestro ejemplo, es posible que nuestro modelo pueda aprender a asociar la palabra \"Hi\" con \"how\", \"are\" y \"you\". \n",
    "\n",
    "También es posible que el modelo aprenda que las palabras estructuradas en este patrón suelen ser una pregunta, de tal manera que responda de manera apropiada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/self_attention.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Mecanismo de auto-atención (self-attention)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectores de consulta, clave y valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query, Key, and Value Vectors. \n",
    "\n",
    "Para lograr la atención propiamente dicho, alimentamos la entrada en 3 capas distintas completamente conectadas para crear los vectores de consulta, clave y valor.\n",
    "\n",
    "\n",
    "El concepto de  (consulta, clave, valor) provienen del mundo de los sistemas de consulta. \n",
    "\n",
    "Por ejemplo, cuando escribe una consulta para buscar algún video en Youtube, el motor de búsqueda mapeará su consulta con un conjunto de claves (título del video, descripción, etc.) asociadas con los videos candidatos en la base de datos, luego le presentará los mejores videos (valores).\n",
    "\n",
    "\n",
    "¿Cuáles son estos vectores exactamente en el caso de un transformer? \n",
    "\n",
    "De acuerdo con el artículo [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf), de 2017, se tiene que\n",
    "\n",
    "$$\n",
    "c_i =\\sum_{j=1}^{T_x} \\alpha_{ij}h_j,\n",
    "$$\n",
    "\n",
    "el peso  $\\alpha_{ij}$ de cada anotación $h_j$ se calcula mediante\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{e^{e_{ij}}}{\\sum_{k=1}^{T_x}e^{e_{ik}}}\n",
    "$$\n",
    "\n",
    "en donde,\n",
    "\n",
    "$$\n",
    "e_{ij} = a (s_{i − 1}, h_j)\n",
    "$$\n",
    "\n",
    "Como esto resulta un poco oscuro, vamos explicar en detalle los cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar de consulta y clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de alimentar la consulta, la clave y el vector de valor a través de una capa lineal, las consultas y las claves se someten a una multiplicación de matriz de producto escalar para producir una matriz de puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Producto_query_key.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> producto matricial consulta-clave(query-key)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de puntuación (scores) determina cuánta atención se debe poner una palabra en otras palabras. \n",
    "\n",
    "Entonces, cada palabra tendrá una puntuación que se corresponde con otras palabras en el intervalo de tiempo. Cuanto mayor sea la puntuación, más atención. Así es como se asignan las consultas a las claves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Attention_scores.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Puntajes de atención(attention-scores)</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalación de las puntuaciones de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Luego, los  puntajes se escalan dividiendo por la raíz cuadrada de la dimensión de la consulta y la clave. \n",
    "\n",
    "Esto es para permitir gradientes más estables, ya que la multiplicación de valores puede tener efectos explosivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/scaling_socres.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Escalación de puntajes</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax de las puntuaciones escaladas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se calcula el *softmax* de la puntuación escalada para obtener los pesos de atención, lo que le da valores de probabilidad entre 0 y 1. \n",
    "\n",
    "Al hacer un *softmax*, las puntuaciones más altas aumentan y las puntuaciones más bajas se deprimen.\n",
    "\n",
    "Esto permite que el modelo tenga más confianza en las palabras que debe atender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Softmax_of _scaled_scores.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Softmax de los puntajes escalados</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicar la salida Softmax con el vector de valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se toman los pesos de atención y los multiplica por su vector de valor para obtener un vector de salida. \n",
    "\n",
    "Las puntuaciones softmax más altas mantendrán el valor de las palabras que el modelo aprende es más importante. Los puntajes más bajos ahogarán las palabras irrelevantes. \n",
    "\n",
    "Luego, se alimenta la salida con una capa lineal para continuar el procesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Score_por_Value.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Puntajes de atención  X valor </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómputo de la atención  multi-cabeza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para hacer de este un cálculo de atención de múltiples cabezas, se debe dividir la consulta, la clave y el valor en *N* vectores antes de aplicar la atención propia. \n",
    "\n",
    "Los vectores divididos luego pasan por el proceso de auto-atención individualmente. \n",
    "\n",
    "Cada proceso de auto-atención se llama cabeza. \n",
    "\n",
    "Cada cabezal produce un vector de salida que se concatena en un solo vector antes de pasar por la capa lineal final. \n",
    "\n",
    "En teoría, cada cabeza aprendería algo diferente, lo que le daría al modelo de codificador más poder de representación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Split_query_value_key.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Separa consulta, clave y valor en distintas cabezas de auto-atención </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, la atención de múltiples cabezas es un módulo en la red transformer que calcula los pesos de atención para la entrada y produce un vector de salida con información codificada sobre cómo cada palabra debe atender a todas las demás palabras en la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las conexiones residuales, la normalización de capas y la red de retroalimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vector de salida de atención de multi-cabeza se suma al embedding de entrada posicional original. \n",
    "\n",
    "A esto se le llama conexión residual. \n",
    "\n",
    "La salida de la conexión residual pasa por una normalización de capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Residual-conexion.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Conexión Residual y capa de normalización</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida residual normalizada se proyecta a través de una red de avance puntual para su posterior procesamiento. \n",
    "\n",
    "La red de avance puntual es un par de capas lineales con una activación ReLU en el medio. \n",
    "\n",
    "La salida de eso se suma de nuevo a la entrada de la red de avance puntual y se normaliza aún más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Suma_capas_normalizacion.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Conexión residual de la entrada y salida de la capa feedforward puntual</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las conexiones residuales ayudan al entrenamiento de la red, permitiendo que los gradientes fluyan directamente a través de las redes. Las normalizaciones de capa se utilizan para estabilizar la red lo que resulta en una reducción sustancial del tiempo de entrenamiento necesario. \n",
    "\n",
    "La capa de avance puntual se utiliza para proyectar las salidas de atención, lo que potencialmente le da una representación más rica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envoltura del codificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas estas operaciones son para codificar la entrada en una representación continua con información de atención. \n",
    "\n",
    "Esto ayudará al decodificador a enfocarse en las palabras apropiadas en la entrada durante el proceso de decodificación. \n",
    "\n",
    "Se puede apilar el codificador *N* veces para codificar aún más la información, donde cada capa tiene la oportunidad de aprender diferentes representaciones de atención, lo que potencialmente aumenta el poder predictivo de la red de transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trabajo del decodificador es generar secuencias de texto. \n",
    "\n",
    "El decodificador tiene una subcapa similar a la del codificador. \n",
    "\n",
    "Tiene dos capas de atención de múltiples cabezas, una capa de avance puntual y conexiones residuales, y una normalización de capa después de cada subcapa. \n",
    "\n",
    "Estas subcapas se comportan de manera similar a las capas del codificador, pero cada capa de atención de múltiples cabezas tiene un trabajo diferente. \n",
    "\n",
    "El decodificador está rematado con una capa lineal que actúa como clasificador y un softmax para obtener las  probabilidades de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Decoder.png\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Estructura del decodificador</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El decodificador es autorregresivo. \n",
    "\n",
    "Comienza con un token de inicio y toma una lista de salidas anteriores como entradas, así como las salidas del codificador que contienen la información de atención de la entrada. \n",
    "\n",
    "El decodificador deja de decodificar cuando genera un token como salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Autoregresivo.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> El decodificador es autorregresivo. Genera un token de uno en uno mientras se alimenta en las salidas anteriores</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se describen los pasos del decodificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings de entrada del decodificador y codificación posicional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comienzo del decodificador es prácticamente el mismo que el del codificador.\n",
    "\n",
    "La entrada pasa por una capa de incrustación y una capa de codificación posicional para obtener incrustaciones posicionales. Las incrustaciones posicionales se introducen en la primera capa de atención de múltiples cabezales que calcula las puntuaciones de atención para la entrada del decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificadores de primer nivel de atención multi-cabeza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta capa de atención de múltiples cabezas funciona de manera ligeramente diferente. \n",
    "\n",
    "Dado que el decodificador es autorregresivo y genera la secuencia palabra por palabra, debe evitar que se condicione a tokens futuros. \n",
    "\n",
    "Por ejemplo, al calcular las puntuaciones de atención en la palabra \"am\", no debería tener acceso a la palabra \"fine\", porque esa palabra es una palabra futura que se generó después. \n",
    "\n",
    "La palabra \"am\" solo debe tener acceso a sí misma y a las palabras anteriores.\n",
    "\n",
    "Esto es cierto para todas las demás palabras, donde solo pueden atender a palabras anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Firt_multi-head-attention.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Una representación de las primeras puntuaciones de atención escaladas del decoder de  atención de múltiples cabezas. La palabra \"am\" no debe incluir ningún valor para la palabra \"fine\". Esto es cierto para todas las demás palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos un método para evitar calcular las puntuaciones de atención para palabras futuras. \n",
    "\n",
    "Este método se llama enmascaramiento. \n",
    "\n",
    "Para evitar que el decodificador mire tokens futuros, se aplica una máscara de anticipación. \n",
    "\n",
    "La máscara se agrega antes de calcular el *softmax* y después de escalar las puntuaciones. Veamos cómo funciona esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máscara de anticipación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La máscara es una matriz que tiene el mismo tamaño que los puntajes de atención llenos de valores de 0 e infinitos negativos. \n",
    "\n",
    "Cuando agrega la máscara a las puntuaciones de atención escaladas, se obtiene una matriz de las puntuaciones, con el triángulo rectángulo superior lleno de infinitos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/mascara-anticipacion.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Agregar una máscara de anticipación a las puntuaciones escaladas</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La razón de la máscara es porque una vez que toma el softmax de las puntuaciones enmascaradas, los infinitos negativos se ponen a cero, dejando puntuaciones de atención cero para las fichas futuras. \n",
    "\n",
    "Como puede ver en la figura siguiente, las puntuaciones de atención para \"am\" tienen valores para sí mismo y todas las palabras anteriores, pero son cero para la palabra \"fine\".\n",
    "\n",
    "Esto esencialmente le dice al modelo que no se enfoque en esas palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/enmascaramiento.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> enmascaramiento</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enmascaramiento es la única diferencia en cómo se calculan las puntuaciones de atención en la primera capa de atención de múltiples cabezas. \n",
    "\n",
    "Esta capa todavía tiene múltiples cabezas, a las que se está aplicando la máscara, antes de concatenarse y alimentarse a través de una capa lineal para su posterior procesamiento.\n",
    "\n",
    "La salida de la primera atención de múltiples cabezas es un vector de salida enmascarado con información sobre cómo el modelo debe atender la entrada del decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atencion_mulit_cabeza_enmascaramiento.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Atención multi-cabeza con enmascaramiento</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificador de segundo nivel de atención de varios cabezales y capa de alimentación directa puntual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda capa de atención de múltiples cabezas. \n",
    "\n",
    "Para esta capa, las salidas del codificador son las consultas y las claves, y las primeras salidas de la capa de atención de varios encabezados son los valores. \n",
    "\n",
    "Este proceso hace coincidir la entrada del codificador con la entrada del decodificador, lo que le permite al decodificador decidir en qué entrada del codificador es relevante enfocarse. \n",
    "\n",
    "La salida de la segunda atención de múltiples cabezas pasa a través de una capa de avance puntual para su posterior procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador lineal y Softmax final para probabilidades de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de la capa final de avance puntual pasa por una capa lineal final, que actúa como clasificador. \n",
    "\n",
    "El clasificador es tan grande como la cantidad de clases que tienes. \n",
    "\n",
    "Por ejemplo, si tiene 10,000 clases para 10,000 palabras, la salida de ese clasificador será de tamaño 10,000. \n",
    "\n",
    "La salida del clasificador luego se alimenta a una capa softmax, que producirá puntuaciones de probabilidad entre 0 y 1. \n",
    "\n",
    "Tomamos el índice de la puntuación de probabilidad más alta, y eso es igual a nuestra palabra predicha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Clasificador_lineal.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Clasificador lineal con Softmax para obtener las probabilidades de salida</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, el decodificador toma la salida, la agrega a la lista de entradas del decodificador y continúa decodificando nuevamente hasta que se predice un token. \n",
    "\n",
    "Para nuestro caso, la predicción de mayor probabilidad es la clase final que se asigna al token final.\n",
    "\n",
    "El decodificador también se puede apilar *N* capas de altura, cada capa recibe entradas del codificador y las capas anteriores. \n",
    "\n",
    "Al apilar las capas, el modelo puede aprender a extraer y centrarse en diferentes combinaciones de atención de sus cabezas de atención, lo que potencialmente aumenta su poder predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/apilamiento_encoder_decoder.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\"> Apilamiento de encoders y decoders</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Transformers paso a paso](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y eso es!\n",
    "\n",
    "¡Y eso es! \n",
    "\n",
    "Esa es la mecánica de los transformers. \n",
    "\n",
    "Los transformers aprovechan el poder del mecanismo de atención para hacer mejores predicciones. \n",
    "\n",
    "Las redes neuronales recurrentes intentan lograr cosas similares, pero porque sufren de memoria a corto plazo. \n",
    "\n",
    "Los transformers pueden ser mejores, especialmente si desea codificar o generar secuencias largas. \n",
    "\n",
    "Debido a la arquitectura del transformer, la industria del procesamiento del lenguaje natural puede lograr resultados sin precedentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
