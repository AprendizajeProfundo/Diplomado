{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Redes Generativas Adversarias</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>GAN</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ScgciUekeLx"
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ScgciUekeLx"
   },
   "source": [
    "1. [Rowel Atiesa, Advanced Deep Learning with Tensorflow 2 and Keras, second ed., Pack, 2020](https://www.oreilly.com/library/view/advanced-deep-learning/9781838821654/)\n",
    "1. [Ejemplos de Keras](https://keras.io/examples/generative/dcgan_overriding_train_step/)\n",
    "1. [Tutoriales-Tensorflow](https://www.tensorflow.org/tutorials/generative/dcgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOKqkYEZkeL0"
   },
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Preliminares](#Preliminares)\n",
    "* [Componentes de una GAN](#Componentes-de-una-GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgu76LPXkeL1",
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué son las GAN?\n",
    "Las redes generativas adversarias (GAN) son una de las más interesantes ideas de la informática hoy en día. Dos modelos son entrenados simultáneamente por un proceso contradictorio. Un generador ( \"el artista\") aprende a crear imágenes que parecen reales, mientras que un discriminador ( \"el crítico de arte\") aprende a decir las imágenes reales, aparte de las falsificaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/gan1.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Generador y discriminador de una GAN </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Tutoriales-Tensorflow](https://www.tensorflow.org/tutorials/generative/dcgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento, el generador vuelve progresivamente más capaz de crear imágenes que parecen reales, mientras que el discriminador convierte en el mejor distinguirlos. Los alcances proceso de equilibrio cuando el discriminador ya no puede distinguir imágenes reales de los falsos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/gan2.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">GAN en acción </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Tutoriales-Tensorflow](https://www.tensorflow.org/tutorials/generative/dcgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno veremos como generar imágenes de caras usando una red generativa adeversaria convolucional (DCGAN). Utlizaremos un subconjunto del conjunto de datos de imágenes de celebridades [CelebFaces Attributes (CelebA) Dataset](https://www.kaggle.com/jessicali9530/celeba-dataset) disponible en Kaggle. CelebA tiene más de 200k imágenes de celebridades con 40 anotaciones de tipo binario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Preliminares</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Título: DCGAN para generar imágenes de caras\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2019/04/29\n",
    "Last modified: 2021/01/01\n",
    "Descripción: Una  DCGAN simple entrenada `fit()` reescribiendo del paso de entrenamiento\n",
    "`train_step` sobre imágenes CelebAs.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gdown #para bajar archivos grandes de Google-drive\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baja el conjunto de datos y lo descomprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baja los datos CelebA del drive de Google y los coloca en un archivo \n",
    "Usaremos la imágenes de caras de CelebA dataset, colocandolas en tamaño a 64x64.\n",
    "\"\"\"\n",
    "# crea la carpeta en donde recibirá los datos\n",
    "path = \"/home/alvaro/Downloads/Data/celeba_gan/\"\n",
    "os.makedirs(path)\n",
    "\n",
    "# baja los datos del drive a  la carpeta creada\n",
    "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
    "output = os.path.join(path, \"data.zip\")\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "# extrae los archivos en la carpeta\n",
    "with ZipFile(output, \"r\") as zipobj:\n",
    "    zipobj.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un dataset y preprocesa los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Crea un dataset desde la carpeta los coloca las imágens en tamaño 64*64.\n",
    "Finalmente re-escala las imágenes al rango [0-1]:\n",
    "\"\"\"\n",
    "\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    path, label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Veámos una muestra de ejemplo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0], )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Componentes de una GAN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrada del discriminador será datos reales o sintetizados. El discriminador recibe  muestras de datos genuinos y muestras  de datos falsos provienen que provienen del generador. \n",
    "\n",
    "\n",
    "Los datos válidos están etiquetados como 1.0 (es decir, un 100% de probabilidad de ser real), mientras que todos los\n",
    "los datos sintetizados están etiquetados como 0.0 (es decir, un 0% de probabilidad de ser real). \n",
    "\n",
    "\n",
    "El proceso de etiquetado está automatizado, por lo que las GAN se consideran modelos de aprendizaje no supervisado. El objetivo del discriminador es aprender a  distinguir datos reales de datos falsos. Durante esta parte del entrenamiento de la GAN, solo Los parámetros del discriminador serán actualizados.\n",
    "\n",
    "El discriminador es  un clasificador binario típico y será entrenado para predecir en un rango de 0.0 a 1.0 en valores de confianza  tan cerca de 0.0 o 1.0 como sea posible, según reciba un dato falso o uno real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea el discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El discriminador es  una red convolucional de clasificación binaria , la cual asigna una imagen de 64x64 a una puntuación de clasificación binaria: real o falsa. Usaremos la funcion activación `LeakyReLU` para cada una de las capsas internas. Esta función de activación es definida por\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\begin{cases} x &\\text{ si } x>0\\\\\n",
    "\\alpha x, \\hspace{3mm}0 <\\alpha < 1 &\\text{ si } x\\le 0 \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrada al generador es ruido, y la salida son datos sintetizados. A intervalos regulares, el generador fingirá que su salida son datos genuinos y le pedirá a la GAN que lo etiquete como 1.0. Cuando los datos falsos se presentan al discriminador, naturalmente se clasificará como falso con una etiqueta cercana a 0.0.\n",
    "\n",
    "El optimizador calcula las actualizaciones de parámetros del generador en función de lo presentado\n",
    "etiqueta (es decir, 1.0). También tiene en cuenta su propia predicción cuando se entrena con esta nueva información.  En otras palabras, el discriminador tiene algunas dudas con respecto a su predicción, y así, la GAN toma eso en consideración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea el generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hace un reflejo (como con un espejo) el discriminador, reemplazando las capas Conv2D por capas Conv2DTranspose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Redes competitivas o colaborativas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, todo el proceso es similar a dos redes que compiten entre sí\n",
    "mientras sigue cooperando al mismo tiempo. Cuando el entrenamiento de GAN converge, al final \n",
    "el resultado es un generador que puede sintetizar datos que parecen genuinos. \n",
    "\n",
    "El discriminador piensa que estos datos sintetizados son reales o con una etiqueta cercana a 1.0, lo que significa que el discriminador puede ser descartado. La parte del generador será útil para producir salidas significativas de entradas de ruido arbitrarias.\n",
    "\n",
    "La siguiente imagen describe el proceso de entrenamiento de una GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/GAN_trainig.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Esquema de entrenamiento de una GAN </p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reescribe algunos métodos de keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar una GAN requiere que se reescriba el el paso de entrenamiento de la red. Esto es necesario, debido a que tenemos dos redes compitiendo que deben entrenarse simultáneamente.\n",
    "\n",
    "Lo que haremos es lo siguiente. Creamos una clase que llamaremos GAN y que derivamos de la clase `keras.Model`. Recuerde que Model contiene los métodos completos para entrenar una red neuronal, pero podemos reescribir algunos de sus métodos. Reescribermos los métodos *\\_\\_init\\_\\_()*, *compile((* y *train_step*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método \\_\\_init\\_\\_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define los objetos discriminador y generador y el tmaño del espacio latente de representación interna de las imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido aque entrenamos dos redes simultáneamente, cada una debe tener su propio optimizador y su propia función de pérdida. Esta implementación usaremos el  mismo t tipo de función de pérdida, pero permitimos optimizadores diferentes. Definimos métricas diferenciadas basadas en la función de pérdida de cada red. En este caso será el mismo tipo de función de pérdida, pero cada red tiene su propia pérdida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedad *metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la implementación definimos esta propiedad de tal forma que sea la lista conformada por las métricas que definimos en compile para cada red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método *train_step()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinir el paso de entrenamiento es el principal ingrediente de las GAN. Se siguen los siguientes pasos:\n",
    "\n",
    "\n",
    "1. A la entrada la función recibe, como siempre ocurre, un lote (batch) de imégenes de entrenamiento reales.\n",
    "1. Genera un lote (batch) de puntos aleatorios del mismo tamaño del lote de entrada a la función.  Supongamos que el tamaño del espacio latente de representación es de tamaño $n$. Entonces genera un lotes de observaciones usando la distribución normal multivariada estándar $\\mathfrak{N}_n(\\boldsymbol{0},\\boldsymbol{I})$.\n",
    "1. Con el lote de aleatorios usa el generados para generar un lote de imágenes falsas.\n",
    "1. Combina (concatena) los dos lotes: imágenes falsas generadas e imágenes reales.\n",
    "1.  Etiqueta todas las imágenes, discriminado las reales de las falsas.\n",
    "1. Agrega ruido aleatorio a las etiquetas: ¡truco importante!. Esto hará que el disciminador reciba de vez en cuando información totalemente cambiada. En particular algunas etiquetas falsas se verán como verdaderas y viceversa. Se pretende con esto engañar al discriminador. Revise la siguiente imagen.\n",
    "1. Entrena al discriminator. Se pasan las imágenes combinadas al discriminador para que prediga las respectivas etiquetas.Cons estas prediciones se clacula la pérdida del discriminador y se actualizan sus pesos (parámetros). La generadora permance agena a este subproceso.\n",
    "1. Genera un nuevo lote de aleatorios como se hizo arriba.\n",
    "1. Etiqueta tosas esta imágenes  diciendo que todas son \"imágenes reales\"\n",
    "1. Entrena el generador (tenga en cuenta que * no * debemos actualizar los pesos del discriminador). Los aleatorios son pasados por el generador y entregados al discriminador, el cual hace la predicción. La función de pérdida hace le mismo trabajo que hace la funcipon de pérdida del discriminador (entropía cruzada), pero aquí las etiquetas de las imágenes falsas se colocaron como verdaderas, obligando al generador a hacer mejro el trabajo de falsificación.\n",
    "1. Eso es todo. AL final se recalculan las métricas y se retornan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/gan_training2.png\" width=\"400\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Entradas al discriminador </p>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Genera un lote (batch) de puntos aleatorios de dimensión latent_dim,\n",
    "        # usando un modelo normal estándar: N_{latent_dim}(0, I).\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decodifica los puntos aleatorios como imágenes falsas (fake images)\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combina los dos lotes de imágenes (generadas (falsas) y reales)\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Etiqueta todas las imágenes, discriminado las reales de las falsas\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Agrega ruido aleatorio a las etiquetas: ¡truco importante!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Entrena al discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Genera un nuevo lote de aleatorios como se hizo arriba.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "''\n",
    "        # Etiqueta tosas esta imágenes  diciendo que todas son \"imágenes reales\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Entrene el generador (tenga en cuenta que * no * debemos actualizar los pesos\n",
    "        # del discriminador)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea un callback que periodicamente graba imagenes generadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paar poder monitorear en línea el proceso de entrenamiento, Al final de cada época se generan algunas imágenes aleatoriament. En este ejercicio hemos generado 10 imágenes en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"../Datos/generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrena el modelo de lado a lado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo lo anterior, así podemos entrenar nuesto modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estas son algunas de las imágenes que se generaron durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def display_images(epoch, num_img=10): \n",
    "    f, axs = plt.subplots(epoch,num_img,figsize=(10,10))\n",
    "    for j in range(epoch):\n",
    "        for i in range(num_img):\n",
    "          axs[j,i].imshow(Image.open(\"../Datos/Datos/generated_img_%03d_%d.png\" % (j, i)))\n",
    "          axs[j,i].axis('off')\n",
    "\n",
    "\n",
    "display_images(40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
