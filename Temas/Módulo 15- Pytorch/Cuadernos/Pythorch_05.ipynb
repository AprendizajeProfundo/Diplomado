{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b67704-b90c-4972-9a06-e77d2df5d8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e679f-311a-4cbc-8a73-2781d022d31c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Pytorch</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ac5fd-f121-40bd-9f03-100e8f021593",
   "metadata": {},
   "source": [
    "<center>Diferenciación automática con torch.autograd</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae94eb3-c187-4817-86c6-1bf16c3a8eb4",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62501236-c78c-4622-90d5-e54b384986a0",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a5cb9-13e7-400e-abe1-05e4cfbd1d68",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Diseño gráfico y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8ce5a-a021-443c-b630-3d89441d222b",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro Reyes, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2034a8-1948-4d79-be17-802e4a0b1bad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160b8f4-2c02-44d8-bc58-05d1c419f316",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc6f02f-a4c3-4793-a3b8-539c5c8d2a06",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f5375-4755-4a3e-aeb0-baf86a2acda1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99e3f7b-fbdd-468f-84cf-d62033d07341",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309907e-ae9a-4550-9b55-60dff181ed54",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Pipeline de HuggingFace](#Pipeline-de-HuggingFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab6ba0-f3d0-4b3f-b897-3c2e3bea1c9d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85ccde-c92d-4a4c-bdd2-e56970f19d11",
   "metadata": {},
   "source": [
    "Al entrenar redes neuronales, el algoritmo más utilizado es la propagación hacia atrás (**back propagation**). En este algoritmo, los parámetros (pesos del modelo) se ajustan de acuerdo con el gradiente de la función de pérdida con respecto al parámetro dado.\n",
    "\n",
    "Para calcular esos gradientes, PyTorch tiene un motor de diferenciación incorporado llamado `torch.autograd`. Admite el cálculo automático del gradiente para cualquier gráfo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c62b9-5129-4e07-9cea-f1280365c078",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Tensores, funciones y grafo computacional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc36143-7e86-4600-b18f-8ccc4875ea06",
   "metadata": {},
   "source": [
    "Considere la red neuronal de una capa más simple, con entrada $x$, parámetros $w$ y $b$, y alguna función de pérdida. Se puede definir en PyTorch de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54743515-c487-431f-bf15-d7fed775b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # entrada\n",
    "y = torch.zeros(3) # etiqueta (valor verdadero)\n",
    "w = torch.randn(5,3, requires_grad=True) # matriz de pesos\n",
    "b = torch.randn(3, requires_grad=True)# bias\n",
    "z = torch.matmul(x,w)+b # calculo de la capa\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbf9d5-01f4-431b-afd4-5d19906b9ef4",
   "metadata": {},
   "source": [
    "Este código define el siguiente grafo computacional:\n",
    "\n",
    "![](../Imagenes/comp-graph.png)\n",
    "Grafo computacional del cálculo descrito arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a1630-4efc-4fe5-a734-18ad26b79b07",
   "metadata": {},
   "source": [
    "La propiedad `requires_grad` indica que son variables con respecto a las cuales se desea calcular el gradiente de la función *loss*.\n",
    "\n",
    "No es necesario declararlas como tal desde el comienzo. Puede hacerlo luego mediante el método\n",
    "\n",
    "+ x.requires_grad(True).\n",
    "\n",
    "Una función aplicada a tensores de un objeto de la clase `Function`, la cual viene equipada con lo necesario para la diferenciación automática. Una referencia a la función gradiente (back propagation) se almacena en `grad_fn`. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6a4134-877e-4c7f-b556-3bd9881913fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7fbf0013bd30>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7fbf0013bb50>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4964ae-2153-45b7-adf0-0a3f0cf7ba64",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Cálculo de gradientes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84369f-af31-471d-9753-af0638d67425",
   "metadata": {},
   "source": [
    "Vamos a calcular $\\frac{\\partial loss} {\\partial w}$ y $\\frac{\\partial loss} {\\partial b}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6c8515-a240-46e3-95f8-504cd4c48ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0293, 0.2314, 0.2462],\n",
      "        [0.0293, 0.2314, 0.2462],\n",
      "        [0.0293, 0.2314, 0.2462],\n",
      "        [0.0293, 0.2314, 0.2462],\n",
      "        [0.0293, 0.2314, 0.2462]])\n",
      "tensor([0.0293, 0.2314, 0.2462])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2332f-d6a0-4bf0-9d9f-cd7c583b2729",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964f2e1-63a3-449d-98e1-13e53978fa23",
   "metadata": {},
   "source": [
    "- Solo podemos obtener las propiedades *grad* para los nodos hoja del grafo computacional, que tienen la propiedad *require_grad* establecida en *True*. Para todos los demás nodos de nuestro gráfico, los degradados no estarán disponibles.\n",
    "- Solo podemos realizar cálculos de gradiente usando hacia atrás una vez en un gráfico dado, por razones de rendimiento. Si necesitamos hacer varias llamadas hacia atrás en el mismo gráfico, debemos pasar *retain_graph = True* a la llamada hacia atrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ff03-69b4-4a1b-9d73-263c43df1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd211a8d-bd94-4fdd-92a0-28513ff79bf0",
   "metadata": {},
   "source": [
    "### Deshabilitar el seguimiento de gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce49a1-1bea-47af-8391-5f8312a1fb27",
   "metadata": {},
   "source": [
    " De forma predeterminada, todos para los tensores con require_grad = True se está rastreando su historial computacional y admiten el cálculo del gradiente. Sin embargo, hay algunos casos en los que no necesitamos hacer eso, por ejemplo, cuando hemos entrenado el modelo y solo queremos aplicarlo a algunos datos de entrada, es decir, solo queremos hacer cálculos reenviados a través de la red. Podemos detener el seguimiento de los cálculos rodeando nuestro código de cálculo con el bloque  `torch.no_grad ()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21ff1bf-3a0d-409e-bcc8-e7f22db77d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdba607-d61d-4888-a743-7ff74ad207a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alternativamente se puede usar el método `detach`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380fff25-1799-4c22-a0b8-3ae6cbff6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c50f2-1dae-453f-a0b8-1b4b928c3297",
   "metadata": {},
   "source": [
    "Existen motivos por los que quizás desee deshabilitar el seguimiento de gradientes:\n",
    "\n",
    "- Para marcar algunos parámetros en su red neuronal como parámetros congelados. Este es un escenario muy común para ajustar una red previamente entrenada (finetunning)\n",
    "- Para acelerar los cálculos cuando solo está haciendo un paso hacia adelante, porque los cálculos en tensores que no siguen los gradientes serían más eficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb05211-77aa-4592-a95f-bcb434d013cd",
   "metadata": {},
   "source": [
    "### Más sobre gráficos computacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6e506-9045-4e77-98ef-7916c78932eb",
   "metadata": {},
   "source": [
    "Conceptualmente, *autograd* mantiene un registro de datos (tensores) y todas las operaciones ejecutadas (junto con los nuevos tensores resultantes) en un gráfico acíclico dirigido (DAG) que consta de objetos de tipo *Function*. En este DAG, las hojas son los tensores de entrada, las raíces son los tensores de salida. Al trazar este gráfico desde las raíces hasta las hojas, puede calcular automáticamente los gradientes usando la regla de la cadena.\n",
    "\n",
    "En un paso hacia adelante (foreward), *autograd* hace dos cosas simultáneamente:\n",
    "\n",
    "- ejecuta la operación solicitada para calcular un tensor resultante\n",
    "- mantiene la función de gradiente de la operación en el DAG.\n",
    "\n",
    "El paso hacia atrás(backward) comienza cuando se llama a `.backward()` en la raíz del DAG. `autograd` entonces:\n",
    "\n",
    "- calcula los gradientes de cada `.grad_fn`,\n",
    "- los acumula en el atributo `.grad` del tensor respectivo\n",
    "- utilizando la regla de la cadena, se propaga hasta los tensores de las hojas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367021f-4c14-4b57-a8d1-892b792e0560",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59078ce-701a-40a6-b3b1-c4aed3851b10",
   "metadata": {},
   "source": [
    "Los DAG son dinámicos en PyTorch. Una cosa importante a tener en cuenta es que el gráfico se recrea desde cero; después de cada llamada .backward (), autograd comienza a completar un nuevo gráfico. Esto es exactamente lo que le permite utilizar declaraciones de flujo de control en su modelo; puede cambiar la forma, el tamaño y las operaciones en cada iteración si es necesario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
