{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b67704-b90c-4972-9a06-e77d2df5d8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e679f-311a-4cbc-8a73-2781d022d31c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Pytorch</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ac5fd-f121-40bd-9f03-100e8f021593",
   "metadata": {},
   "source": [
    "<center>Transforms</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae94eb3-c187-4817-86c6-1bf16c3a8eb4",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62501236-c78c-4622-90d5-e54b384986a0",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a5cb9-13e7-400e-abe1-05e4cfbd1d68",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Diseño gráfico y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8ce5a-a021-443c-b630-3d89441d222b",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro Reyes, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2034a8-1948-4d79-be17-802e4a0b1bad",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160b8f4-2c02-44d8-bc58-05d1c419f316",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc6f02f-a4c3-4793-a3b8-539c5c8d2a06",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f5375-4755-4a3e-aeb0-baf86a2acda1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99e3f7b-fbdd-468f-84cf-d62033d07341",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309907e-ae9a-4550-9b55-60dff181ed54",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Pipeline de HuggingFace](#Pipeline-de-HuggingFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab6ba0-f3d0-4b3f-b897-3c2e3bea1c9d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928d1f1-1f06-4660-a28e-9346b6c7cea5",
   "metadata": {},
   "source": [
    "Los datos no siempre vienen en su forma procesada final que se requiere para entrenar algoritmos de aprendizaje automático. Usamos transformaciones para realizar alguna manipulación de los datos y hacerlos aptos para el entrenamiento.\n",
    "\n",
    "Todos los conjuntos de datos de TorchVision tienen dos parámetros *transform* para modificar las características y *target_transform* para modificar las etiquetas- que aceptan llamadas que contienen la lógica de transformación. El módulo *torchvision.transforms* ofrece varias transformaciones de uso común listas para usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed2454-ab8b-4d53-b4c4-586ae89cf899",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Carga de un Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54743515-c487-431f-bf15-d7fed775b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bce0621-2395-445c-b50d-54210dd4d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # convirte la imagen oun Numpy ndarray a un FloatTensor\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1c841-9343-47db-9a23-e7743b18c160",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Transformaciones</span>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e6e1f-5cfb-4448-94ab-b0c9b350482d",
   "metadata": {},
   "source": [
    "### ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec0e6e-1293-4570-b5cb-a1eea7d8aea5",
   "metadata": {},
   "source": [
    "`ToTensor` convierte una imagen PIL o NumPy ndarray en un FloatTensor. y escala los valores de intensidad de píxeles de la imagen en el rango [0., 1.]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da61c4a-2ee0-43a4-a87e-c9c8dd4d0dac",
   "metadata": {},
   "source": [
    "### Transformaciones Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d740e4-1acf-4c89-8b31-8f8ffca6f6c0",
   "metadata": {},
   "source": [
    "Las transformaciones `Lambda` aplican cualquier función *lambda* definida por el usuario. Aquí, definimos una función para convertir el número entero en un tensor codificado en caliente. Primero crea un tensor cero de tamaño 10 (el número de etiquetas en nuestro conjunto de datos) y llama a scatter_, que asigna un valor = 1 en el índice como lo indica la etiqueta y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
