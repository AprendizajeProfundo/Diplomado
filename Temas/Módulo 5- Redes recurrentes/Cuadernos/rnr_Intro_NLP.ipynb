{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:green\"><center>Diplomado en Inteligencia Artificial y Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción al preprocesamiento de textos</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Procesamiento de Lenguaje Natural</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Campo Elías Pardo Turriago, cepardot@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introducción a Redes LSTM](Intro_LSTM.ipynb)\n",
    "2. [Time Series Forecasting with LSTMs using TensorFlow 2 and Keras in Python](https://towardsdatascience.com/time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python-6ceee9c6c651/)\n",
    "3. [Dive into Deep Learnig](https://d2l.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Herramientas básicas de programación](#Herramientas-básicas-de-programación)\n",
    "* [Lectura del conjunto de datos](#Lectura-del-conjunto-de-datos)\n",
    "* [Tokenización](#Tokenización)\n",
    "* [Vocabulario](#Vocabulario)\n",
    "* [Poniendo todas las cosas juntas](#Poniendo-todas-las-cosas-juntas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Herramientas básicas de programación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptadas del texto guía [Dive into Deep learning](https://d2l.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#from collections import defaultdict\n",
    "from IPython import display\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "#import shutil\n",
    "import sys\n",
    "#import tarfile\n",
    "#import time\n",
    "import requests\n",
    "#import zipfile\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Alias\n",
    "size = lambda a: tf.size(a).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "Hemos revisado y evaluado herramientas estadísticas y desafíos de predicción.\n",
    "para datos secuenciales.\n",
    "\n",
    "Estos datos pueden adoptar muchas formas. \n",
    "\n",
    "El texto es uno de los ejemplos más populares de datos secuenciales.\n",
    "\n",
    "Por ejemplo, un artículo puede verse simplemente como una secuencia de palabras o incluso como una secuencia de caracteres.\n",
    "\n",
    "Para facilitar nuestros experimentos futuros con datos secuenciales,\n",
    "dedicaremos esta sección a explicar los pasos comunes de preprocesamiento de texto.\n",
    "\n",
    "Por lo general, estos pasos son:\n",
    "\n",
    "1. Cargue texto como cadenas en la memoria.\n",
    "2. Divida las cadenas en tokens (por ejemplo, palabras y caracteres).\n",
    "3. Construya una tabla de vocabulario para mapear las claves divididas en índices numéricos.\n",
    "4. Convierta texto en secuencias de índices numéricos para que los modelos puedan manipularlos fácilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## <span style=\"color:blue\">Lectura del conjunto de datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Para comenzar, cargamos texto de [*The Time Machine*] de H. G. Wells (http://www.gutenberg.org/ebooks/35).\n",
    "Este es un corpus bastante pequeño de poco más de 30000 palabras, pero para el propósito de lo que queremos ilustrar, está bien.\n",
    "\n",
    "Las colecciones de documentos más realistas contienen muchos miles de millones de palabras.\n",
    "La siguiente función lee el conjunto de datos en una lista de líneas de texto, donde cada línea es una cadena.\n",
    "Para simplificar, aquí ignoramos la puntuación y las mayúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### Funciones especiales de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 4
   },
   "outputs": [],
   "source": [
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'Datos')):\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../Datos/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n",
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  \n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## <span style=\"color:blue\">Tokenización</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "La siguiente función `tokenize` \n",
    "toma una lista (`líneas`) como entrada,\n",
    "donde cada lista es una secuencia de texto (por ejemplo, una línea de texto).\n",
    "\n",
    "Cada secuencia de texto se divide en una lista de tokens.\n",
    "\n",
    "Un *token* es la unidad básica en el texto.\n",
    "\n",
    "Al final, se devuelve una lista de listas de tokens,\n",
    "donde cada token es una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'): \n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## <span style=\"color:blue\">Vocabulario</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "El tipo de cadena del token es inconveniente para ser utilizado por modelos, que toman entradas numéricas.\n",
    "\n",
    "Ahora construyamos un diccionario, a menudo también llamado *vocabulario*, para mapear tokens de cadena en índices numéricos únicos partir de 0.\n",
    "\n",
    "Para hacerlo, primero contamos los tokens únicos en todos los documentos del conjunto de entrenamiento, que llamremos, un *corpus*, y luego asignamos un índice numérico a cada token único posiblemente de acuerdo con su frecuencia.\n",
    "\n",
    "Los tokens que aparecen raramente se eliminan a menudo para reducir la complejidad.\n",
    "Cualquier token que no exista en el corpus o que haya sido eliminado se asigna a un token especial desconocido. “&lt;unk&gt;”.\n",
    "\n",
    "Opcionalmente, agregamos una lista de tokens reservados, como\n",
    "\n",
    "+  “&lt;unk&gt;” para token eliminado o desconocido,\n",
    "+ “&lt;pad&gt;” para relleno (padding),\n",
    "+ “&lt;bos&gt;” para indicar el comienzo de una secuencia, y \n",
    "+ “&lt;eos&gt;” para el final de una secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:  \n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = [] \n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def count_corpus(tokens): \n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Construimos un vocabulario usando el conjunto de datos del texto *The Time Machine* como corpus.\n",
    "Luego imprimimos los primeros tokens con sus índices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "Ahora podemos convertir cada línea de texto en una lista de índices numéricos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## <span style=\"color:blue\">Poniendo todas las cosas juntas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "Usando las funciones anteriores, empaquetamos todo en la función `load_corpus_time_machine`, que devuelve `corpus`, una lista de índices de tokens, y `vocab`, el vocabulario del corpus de la máquina del tiempo.\n",
    "Las modificaciones que hicimos aquí son:\n",
    "\n",
    "1.  convertimos el texto en caracteres, no en palabras, para simplificar el entrenamiento en secciones posteriores;\n",
    "2.  `corpus` es una lista única, no una lista de listas de claves, ya que cada línea de texto en el conjunto de datos de la máquina del tiempo no es necesariamente una oración o un párrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  \n",
    "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # Since each text line in the time machine dataset is not necessarily a\n",
    "    # sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " 'o',\n",
       " 's',\n",
       " 'h',\n",
       " 'r',\n",
       " 'd',\n",
       " 'l',\n",
       " 'm',\n",
       " 'u',\n",
       " 'c',\n",
       " 'f',\n",
       " 'w',\n",
       " 'g',\n",
       " 'y',\n",
       " 'p',\n",
       " 'b',\n",
       " 'v',\n",
       " 'k',\n",
       " 'x',\n",
       " 'z',\n",
       " 'j',\n",
       " 'q']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(list(range(28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 13,\n",
       " 4,\n",
       " 15,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 21,\n",
       " 19,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 18,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 12,\n",
       " 12,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " 22,\n",
       " 2,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 16]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## <span style=\"color:blue\">Resumen</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "* El texto es una forma importante de secuencia de datos.\n",
    "* Para preprocesar texto, generalmente dividimos el texto en tokens, construimos un vocabulario para mapear cadenas de tokens en índices numéricos y convertir datos de texto en índices de tokens para que los modelos los manipulen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
