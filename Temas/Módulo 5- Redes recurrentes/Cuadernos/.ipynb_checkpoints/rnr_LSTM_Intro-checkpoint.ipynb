{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes LSTM (Long Short Term Memory Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El problema de las dependencias a largo plazo](El-problema-de-las-dependencias-a-largo-plazo)\n",
    "* [Redes LSTM](#Redes-LSTM)\n",
    "* [La idea central detras de las redes LSTM](#La-idea-central-detras-de-las-redes-LSTM)\n",
    "* [Caminando a lo largo de una red LSTM. Paso a paso](#Caminando-a-lo-largo-de-una-red-LSTM.-Paso-a-paso)\n",
    "* [Estructura Matemática de una red LSTM](#Estructura-Matemática-de-una-red-LSTM)\n",
    "* [Cálculo del número de neuronas de una capa LSTM](#Cálculo-del-número-de-neuronas-de-una-capa-LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. Ralf C. Staudemeyer and Eric Rothstein Morris,[*Understanding LSTM a tutorial into Long Short-Term Memory Recurrent Neural Networks*](https://arxiv.org/pdf/1909.09586.pdf), arxiv, September 2019\n",
    "3. Karpathy, [*The Unreasonable Effectiveness of Recurrent Neural Networks*](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introducción\n",
    "\n",
    "\n",
    "Los humanos no comienzan a pensar desde cero cada segundo. A medida que usted lee este documento, comprende cada palabra en función de su comprensión de las palabras anteriores. No lee todo y empieza a pensar desde cero de nuevo. Sus pensamientos tienen persistencia.\n",
    "\n",
    "Las redes neuronales tradicionales no pueden hacer esto, lo parece una gran deficiencia. Por ejemplo, imagine que desea clasificar qué tipo de evento está ocurriendo en cada punto de una película. No está claro cómo una red neuronal tradicional podría usar su razonamiento sobre eventos anteriores en la película para informar a los posteriores.\n",
    "\n",
    "Las redes neuronales recurrentes (RNR) abordan este problema. Estas son redes con bucles que permiten que la información sea persistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RNN-rolled-sm.png\" width=\"200\" height=\"100\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Diagrama enrrollado de un trozo de una red neuronal recurrente</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diagrama anterior presenta un diagrama *enrrollado* de un trozo de una red neuronal. La neurona $A$, observa alguna entrada $x_t$ y genera un valor $h_t$. Un bucle permite que la información trasite de un paso de la red al siguiente.\n",
    "\n",
    "Estos bucles hacen que las redes neuronales recurrentes parezcan misteriosas. \n",
    "\n",
    "\n",
    "Sin embargo, si piensa un poco más, resulta que no son tan diferentes de una red neuronal normal. \n",
    "\n",
    "Una red neuronal recurrente puede considerarse como copias múltiples de la misma red, cada una de las cuales pasa un mensaje a un sucesor. Considere lo que sucede si desenrollamos el bucle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RNN-unrolled.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Diagrama desenrrollado  de un trozo de una red neuronal recurrente </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Esta naturaleza en cadena revela que las redes neuronales recurrentes están íntimamente relacionadas con secuencias. \n",
    "\n",
    "Son la arquitectura natural de  red neuronal para usar con datos de naturaleza secuencial.\n",
    "\n",
    "Desde el punto de vista estadístico son procesos estocásticos. Más precisamente, cadenas de Markov (ocultas).\n",
    "\n",
    "\n",
    "En los últimos años, ha habido un éxito increíble aplicando RNR a una variedad de problemas: \n",
    "\n",
    "1. reconocimiento de voz,\n",
    "2. modelamiento de idiomas, \n",
    "3. traducción, \n",
    "4. subtitulado de imágenes\n",
    "5. ... La lista continúa. \n",
    "\n",
    "\n",
    "Dejaré la discusión de las increíbles hazañas que uno puede lograr con los RNN en la excelente publicación de blog de Andrej Karpathy, La irrazonable efectividad de las redes neuronales recurrentes http://karpathy.github.io/2015/05/21/rnn-effectiveness/ . Pero realmente son bastante asombrosos.\n",
    "\n",
    "\n",
    "Esencial para estos éxitos es el uso de redes de tipo **LSTM**, un tipo muy especial de red neuronal recurrente que funciona, para muchas tareas, mucho mejor que la versión estándar. Casi todos los resultados emocionantes basados en redes neuronales recurrentes se logran con ellas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de las dependencias a largo plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencias a corto plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los atractivos de las RNR es la idea de que podrían ser capaces de conectar información previa a la tarea actual, como el uso de fotogramas de video anteriores que podrían informar la comprensión del presente cuadro. \n",
    "\n",
    "\n",
    "Si las RNR pudieran hacer esto, serían extremadamente útiles. ¿Pero pueden hacerlo? \n",
    "\n",
    "Depende.\n",
    "\n",
    "A veces, solo necesitamos mirar información reciente para realizar la tarea actual. Por ejemplo, considere un modelo de lenguaje que intenta predecir la siguiente palabra en función de las anteriores. \n",
    "\n",
    "\n",
    "Si estamos tratando de predecir la última palabra en \n",
    "\n",
    "**las nubes están en el...**, \n",
    "\n",
    "no necesitamos más contexto; es bastante obvio que la siguiente palabra será \n",
    "\n",
    "**cielo**. \n",
    "\n",
    "\n",
    "En tales casos, en donde la brecha entre la información relevante y el lugar que se necesita es pequeña, las RNR pueden aprender a usar la información pasada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RNN-shorttermdepdencies.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">RNR: dependencias de corto plazo. $h_3$ depende de $x_0, x_1,x_3,x_4$ </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero también hay casos en los que necesitamos más contexto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto a más largo plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el problema de  tratar de predecir la última palabra en el texto \n",
    "\n",
    "**\"Crecí en Francia ... hablo francés con fluidez\"**.\n",
    "\n",
    "\n",
    "La información reciente sugiere que la siguiente palabra es probablemente el nombre de un idioma, pero si queremos limitar qué idioma, necesitamos el contexto de Francia, desde más atrás. \n",
    "\n",
    "\n",
    "Es completamente posible que la brecha entre la información relevante y el punto donde se necesita sea muy grande.\n",
    "\n",
    "Desafortunadamente, a medida que crece esa brecha, los RNN no pueden aprender a conectar la información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RNN-longtermdependencies.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">RNR: dependencias de largo plazo. $h_{t+1}$ depende de $x_0, x_1$ </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales luchan con dependencias a largo plazo. En teoría, los RNR son absolutamente capaces de manejar tales \"dependencias a largo plazo\". \n",
    "\n",
    "\n",
    "Un humano podría elegir cuidadosamente los parámetros para resolver problemas de juguetes de esta forma.\n",
    "\n",
    "Lamentablemente, en la práctica, los RNR no parecen poder aprenderlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema del gradiente que se desvanece o explota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El problema fue explorado en profundidad por Hochreiter (1991) y Bengio, et al. (1994), quienes encontraron algunas razones fundamentales por las cuales podría ser difícil.\n",
    "\n",
    "El problema fundamental mostrado por Hochreiter es que el **gradiente tiende a desaparecer o explotar** cuando en la RNR se calcula las transformaciones afines y se aplica la función de activación (el sigmoide) sobre las mismas matrices y vectores. Ver referencia 2 para más detalles.\n",
    "\n",
    "\n",
    "\n",
    "¡Afortunadamente, los LSTM no tienen este problema!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Las redes de memoria a corto y largo plazo, generalmente llamadas \"LSTM\", son un tipo especial de RNR, capaces de aprender\n",
    "dependencias a largo plazo. \n",
    "\n",
    "\n",
    "Fueron introducidos por [Hochreiter y Schmidhuber (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf), y fueron refinadas y popularizadas por muchas personas en trabajos posteriores. \n",
    "\n",
    "Funcionan tremendamente bien en una gran variedad de problemas, y ahora son ampliamente utilizadas.\n",
    "\n",
    "Las redes LSTM están diseñadas explícitamente para resolver el problema de dependencia a largo plazo.\n",
    "\n",
    "\n",
    "- **Recordar información durante largos períodos de tiempo es prácticamente su comportamiento predeterminado, ¡no es algo que les cuesta aprender!**\n",
    "\n",
    "Todas las redes neuronales recurrentes tienen la forma de una cadena de módulos repetitivos de red neuronal. \n",
    "\n",
    "\n",
    "En los RNR estándar, este módulo repetitivo tendrá una estructura muy simple, como una sola capa de $\\tanh$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-SimpleRNN.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "El módulo de repetición en una RNR estándar con una sola capa. </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes LSTM también tienen esta estructura tipo cadena, pero el módulo de repetición tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro que interactúan de una manera muy especial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-chain.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "El módulo de repetición en una red LSTM  con cuatro capas interactuando. </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se preocupe por los detalles de lo que está sucediendo. Recorreremos el diagrama LSTM paso a paso más adelante. Por ahora, intentemos sentirnos cómodos con la notación que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM2-notation_1.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Nomenclatura de los objetos en las redes LSTM </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el diagrama anterior, cada línea transporta un vector completo, desde la salida de un nodo hasta las entradas de otros. Los círculos de color rosa representan operaciones puntuales, como la suma de vectores, mientras que los cuadros amarillos son capas de redes neuronales aprendidas. La fusión de líneas denota concatenación, mientras que una bifurcación de línea denota que su contenido se copia y las copias van a diferentes ubicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La idea central detras de las redes LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clave para las redes LSTM es el estado de la celda, la línea horizontal que pasa por la parte superior del diagrama.\n",
    "\n",
    "El estado de la celda es como una cinta transportadora. Corre directamente por toda la cadena, con solo algunas interacciones lineales menores. Es muy fácil que la información fluya sin cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-C-line.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Banda transportadora  en las redes LSTM </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red LSTM tiene la capacidad de eliminar o agregar información al estado de la célula, cuidadosamente regulada por estructuras llamadas puertas.\n",
    "\n",
    "Las puertas son una forma opcional de dejar pasar la información. Se componen de una capa de red neuronal con activación  sigmoide y una operación de multiplicación puntual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-gate.png\" width=\"200\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Puerta  en una red LSTM </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa sigmoide genera números entre cero y uno, que describe la cantidad de cada componente que debe dejarse pasar. \n",
    "\n",
    "- Un valor de cero significa **no dejar pasar nada**, \n",
    "- mientras que un valor de uno significa **dejar pasar todo**.\n",
    "\n",
    "Un red LSTM tiene tres de estas puertas, para proteger y controlar el estado de la neurona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caminando a lo largo de una red LSTM. Paso a paso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puerta de olvido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso en nuestra red LSTM es decidir qué información vamos a tirar del estado de la celda. Esta decisión la toma una capa sigmoidea llamada **capa de puerta de olvido**. Se recibe $ h_{t − 1} $ y $ x_t $, y se genera un número entre 0 y 1 para cada número en el estado de celda $ C_{t − 1} $. Un 1 representa **mantener completamente esto** mientras que un 0 representa **deshacerse completamente de esto**.\n",
    "\n",
    "Volvamos a nuestro ejemplo de un modelo de lenguaje que intenta predecir la siguiente palabra en función de todas las anteriores. \n",
    "\n",
    "\n",
    "En tal problema, el estado de la celda puede incluir el género del sujeto presente, de modo que se puedan usar los pronombres correctos. Cuando vemos un tema nuevo, queremos olvidar el género del tema anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-focus-f.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Puerta  de olvido </p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puerta de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es decidir qué nueva información vamos a almacenar en el estado de la celda. Esto tiene dos partes. \n",
    "\n",
    "- Primero, una capa sigmoidea llamada **puerta de entrada** decide qué valores actualizaremos. \n",
    "\n",
    "- Luego, una capa de $\\tanh$ crea un vector de nuevos valores candidatos, $\\tilde{C}_t$, que podrían agregarse al estado. En el siguiente paso, combinaremos estos dos para crear una actualización del estado.\n",
    "\n",
    "En el ejemplo de nuestro modelo de lenguaje, queremos agregar el género del nuevo sujeto al estado de la celda, para reemplazar el antiguo que estamos olvidando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-focus-i.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Puerta  de entrada</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualización del estado de la celda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es el momento de actualizar el estado de la celda anterior, $ C_{t − 1} $, en el nuevo estado de la celda $ C_t $. Los pasos anteriores ya decidieron qué hacer, solo tenemos que hacerlo.\n",
    "\n",
    "\n",
    "Multiplicamos el estado anterior por $ f_t $, olvidando las cosas que decidimos olvidar antes. Luego agregamos $ i_t \\times \\tilde{C}_t $. Estos son los nuevos valores candidatos, escalados según cuánto decidimos actualizar cada valor de estado.\n",
    "\n",
    "En el caso del modelo de lenguaje, aquí es donde realmente soltaríamos la información sobre el género del sujeto anterior y agreguemos la nueva información, como decidimos en los pasos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-focus-C.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Actualización del estado de la celda</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, tenemos que decidir qué vamos a generar. Esta salida se basará en nuestro estado de celda, pero será una versión filtrada. Primero, ejecutamos una capa sigmoidea que decide qué partes del estado de la celda vamos a generar. Luego, ponemos el estado de la celda a través de tanh (para empujar los valores a estar entre -1 y 1) y lo multiplicamos por la salida de la puerta sigmoidea, de modo que solo produzcamos las partes que decidimos.\n",
    "\n",
    "Para el ejemplo del modelo de lenguaje, dado que acaba de ver un tema, es posible que desee generar información relevante para un verbo, en caso de que eso sea lo que viene a continuación. Por ejemplo, podría generar si el sujeto es singular o plural, de modo que sepamos en qué forma se debe conjugar un verbo si eso es lo que sigue a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/LSTM3-focus-o.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">\n",
    "Salida</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Imagen tomada de [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura Matemática de una red LSTM  \n",
    "\n",
    "En una red neuronal LSTM, cada puerta es una RNA. Una capa LSTM tiene varios bloques y cada bloque tiene dos salidas: $ C_t $, el estado actualizado de la celda y $ h_t $, el estado oculto actualizado.\n",
    "\n",
    "$ C_t $ se actualiza de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma(W_f\\cdot [h_{t-1}, x_t] + b_f)\\\\\n",
    "i_t &= \\sigma(W_i\\cdot [h_{t-1}, x_t] + b_i)\\\\\n",
    "\\tilde{C}_t &= \\tanh(W_c\\cdot [h_{t-1}, x_t] + b_c)\\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t\\odot  \\tilde{C}_t,\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $ \\odot $ es el producto de Hadamard o producto puntual de vectores.\n",
    "\n",
    "Por otro lado, $ h_t $ se actualiza después de que $ C_t $ se haya actualizado de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "o_t &= \\sigma(W_o\\cdot [h_{t-1}, x_t] + b_o)\\\\\n",
    "h_t &=  o_t \\odot \\tanh(C_t).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Estas son las ecuaciones requeridas para calcular el gradiente de los parámetros de una capa LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo del número de parámetros de una capa LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que el tamaño de entrada es $n$ y que el tamaño de salida de la capa LSTM es *p*.\n",
    "\n",
    "Entonces $h$ que es la salida tiene tamaño *p* y $x$ tiene tamaño *n*. Por lo tanto para que los operaciones indicadas puedan hacerse se requiere que\n",
    "\n",
    "1. $W_f$, $W_c$, $W_i$ y $W_o$ tienen tamaño $p\\times p+n$. Osea en total $4(p*p  + p*n)$.\n",
    "2. Si se incluyen bias (que es lo común), son $4p$ parámetros adicionales, y en total se tiene que\n",
    "\n",
    "$$\n",
    "\\text{Número de parámetros capa LSTM } = 4(p^2+ pn +p)\n",
    "$$\n",
    "\n",
    "La siguiente imagen muestra la estructura típica de una compuerta en el modelo LSTM. Se ilustra la activación *sigmoide* y *tanh*, porque téngase en cuenta que en la compuerta de actualización la activación es *tanh*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/compuerta_LSTM.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagen: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la siguiente imagen revela el plano de una capa neuronal de tipo LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/plano_lstm.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagen: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
