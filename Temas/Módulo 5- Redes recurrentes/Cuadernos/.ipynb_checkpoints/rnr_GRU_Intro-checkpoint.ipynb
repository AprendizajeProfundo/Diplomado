{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Gated Recurrent Unit-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Funcionamiento de una red GRU](#Funcionamiento-de-una-red-GRU)\n",
    "* [Puerta de actualización-update](#Puerta-de-actualización-update)\n",
    "* [Puerta reinicio](#Puerta-reinicio)\n",
    "* [Activación candidata](#Activación-candidata)\n",
    "* [Actualización del estado recurrente](#Actualización-del-estado-recurrente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "1.  Cho et al. 2014 [On the Properties of Neural Machine Translation: Encoder–Decoder\n",
    "Approaches](https://arxiv.org/pdf/1409.1259.pdf)\n",
    "2. J. Chung, C. Gulcehre, K. Cho, Y. Bengio, [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence  Modeling](https://arxiv.org/pdf/1412.3555v1.pdf)\n",
    "3. Karpathy, [*The Unreasonable Effectiveness of Recurrent Neural Networks*](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una red neuronal recurrente (RNR) es una extensión de una red neuronal convencional,\n",
    "que es capaz de manejar una entrada de secuencia de longitud variable. La RNR maneja la longitud variable de las secuencias mediante un estado oculto recurrente cuya activación en cada momento depende del estado anterior. \n",
    "\n",
    "Más formalmente, dada una secuencia $\\mathbf{x}= (x_1, x_2, \\ldots, x_T)$, la RNR actualiza su estado oculto recurrente $h_t$ mediante \n",
    "\n",
    "\n",
    "$$\n",
    "h_t = \\begin{cases}\n",
    "0,  &\\text{ si } t=0,\\\\\n",
    "\\phi(h_{t-1},x_t), &\\text{en otro caso}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Si $g$ es una función de activación  suave, como un sigmoide o una tangente hiperbólica, es común definir\n",
    "\n",
    "$$\n",
    "h_t = g(Wh_{t-1} + Ux_t+ b)\n",
    "$$\n",
    "\n",
    "Una RNR generativa genera una distribución de probabilidad sobre el siguiente elemento de la secuencia, dado su estado actual $h_t$, y este modelo generativo puede capturar una distribución sobre secuencias de longitud variable mediante el uso de un símbolo de salida especial para representar el final de la secuencia. La secuencia la probabilidad se puede descomponer en\n",
    "\n",
    "$$\n",
    "p (x_1,\\ldots, x_T) = p (x_1) p (x_2 | x_1) p (x_3 | x_1, x_2) \\ldots p (x_T | x_1,\\ldots, x_{T − 1}), \n",
    "$$\n",
    "\n",
    "donde el último elemento es un valor especial de final de secuencia. Modelamos cada probabilidad condicional distribución con\n",
    "\n",
    "$$\n",
    "p (x_t | x_1,\\ldots, x_{t − 1}) = g (h_t)\n",
    "$$\n",
    "\n",
    "El problema con este modelo, es que el cálculo del gradiente tiende a volverse cero o a explotar. \n",
    "\n",
    "Dos líneas de trabajo impulsó esta situación. Por un lado se inició la búsqueda de nuevas técnicas para para el uso del gradiente en el proceso de optimización de  la función de costo y por el otro, el desarrollo de nuevos modelos de redes neuronales. \n",
    "\n",
    "\n",
    "La primera línea ha producido nuevas técnicas de optimización estocástica basados en el gradiente, que han sido usado exitosamente en redes generales. \n",
    "\n",
    "La segunda línea llevó al desarrollo de las redes [LSTM](Intro_LSTM.ipynb), en las cuales la función de activación consiste en una transformación afinada seguida por una simple no linealidad de elementos mediante el uso de unidades de compuerta, [Hochreiter y Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf). \n",
    "\n",
    "\n",
    "Más recientemente, otro tipo de unidad recurrente, a la que nos referimos como una unidad recurrente cerrada (GRU), fue propuesta por [Cho et al. 2014](https://arxiv.org/pdf/1409.1259.pdf). Puede consultar una comparación entre LSTM y GRU en [Junyoung Chung et al., 2014](https://arxiv.org/pdf/1412.3555v1.pdf). De estas unidades recurrentes se ha demostrado que funcionan bien en tareas que requieren captura de dependencias a largo plazo.  Esas tareas incluyen, pero no se limitan a reconocimiento de voz, música,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamiento de una red GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red de unidad recurrente cerrada (GRU) permite que cada unidad recurrente capture de forma adaptativa dependencias de diferentes escalas de tiempo. De manera similar a la unidad LSTM, la GRU tiene puertas que modulan el flujo de información dentro de la unidad.  Sin embargo, a diferencia de las redes LSTM no tiene celdas de memoria separadas.\n",
    "\n",
    "El diagrama presenta la estructura general de una compuerta en una red GRU. A la entrada de cada unidad se presenta, la información de memoria procedente de la unida anterior y la información en la unidad de tiempo $t$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/compuerta_GRU_after_false.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagen: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reset_after: GRU convention (whether to apply reset gate after or before matrix multiplication). False = \"before\", True = \"after\" (default and CuDNN compatible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/compuerta_GRU_after_true.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagen: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La notación que usamos es bastante estándar. \n",
    "\n",
    "1. $+$ : indica suma de vectores\n",
    "2. $\\sigma$ : representa a la función de activación sigmoide\n",
    "3. $\\tanh$ : representa a la función de activación tangente hiperbólica.\n",
    "4.$\\odot$ : es producto componente a componente (producto de Hamard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una GRU tien dos tipos de puerta: actualización (update) y reinicio (reset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puerta de actualización-update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Comenzamos calculando la puerta de actualización $z_t$ para el paso de tiempo $t$ usando la fórmula:\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(W_z x_t + U_zh_{t-1} + b_z)\n",
    "$$\n",
    "\n",
    "en donde $W_z$ y $U_z$ son pesos asociados $x_t$ (la nueva entrada) y $h_{t-1}$, (la  información procedente de la unidad anterior).\n",
    "\n",
    "\n",
    "La puerta de actualización ayuda al modelo a determinar qué cantidad de la información pasada (de los pasos de tiempo anteriores) debe transmitirse al futuro, combinandola  con la nueva información. \n",
    "\n",
    "\n",
    "Eso es realmente poderoso porque el modelo puede decidir copiar toda la información del pasado y eliminar el riesgo de desvanecer del gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puerta reinicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esencialmente, esta puerta se utiliza para que el modelo decida qué cantidad de información pasada debe olvidar. Para calcularla, utilizamos:\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(W_r x_t + U_rh_{t-1} + b_r),\n",
    "$$\n",
    "\n",
    "en donde $W_r$ y $U_r$ son pesos asociados $x_t$ (la nueva entrada) y $h_{t-1}$, (la  información procedente de la unidad anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activación candidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Veamos cómo afectarán exactamente las puertas al resultado final. Primero, comenzamos con el uso de la puerta de reinicio. Introducimos un nuevo contenido de memoria que utilizará la puerta de reinicio para almacenar la información relevante del pasado. Se calcula de la siguiente manera:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(Wx_t + U(r_t\\odot h_{t-1})+b),\n",
    "$$\n",
    "\n",
    "en donde $W$ y $U$ son pesos asociados a las  $x$'s y a las $h$'s respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización del estado recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La activación de la GRU en tiempo $t$ es una interpolación lineal entre la activación previa ${h}_{t-1}$ y la activación candidata $\\tilde{h}_t$ definida arriba. \n",
    "\n",
    "En símbolos tenemos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "h_t = (1-z_t)\\odot h_{t-1} + z_t \\odot \\tilde{h}_t \n",
    "$$\n",
    "\n",
    "La siguiente imagen ilustra la arquitectura de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/plano_gru.jpg\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagen: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
