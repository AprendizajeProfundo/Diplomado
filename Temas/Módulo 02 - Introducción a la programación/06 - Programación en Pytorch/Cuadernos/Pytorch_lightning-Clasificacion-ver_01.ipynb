{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850d8c65-cf49-4c62-8d3e-7991a941f2d4",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45bbdf-1e03-452a-9518-832bfb7d2c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Pytorch-lightning</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a5d93-2fd1-4f5c-b7d3-d37508e35d94",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Large_lightning_bolt.jpg\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: <a href=\"https://commons.wikimedia.org/wiki/File:Large_lightning_bolt.jpg\">Guilerms</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9470be15-7c36-49e1-9ec2-b8650e584f05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Coordinador</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c1e2f-b3b4-46d2-af72-72771bc310a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdaea4-8930-4dc0-9cc3-dc3d31f1542d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc098b33-843d-4f75-98d7-f70610004198",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Camilo José Torres Jiménez, Msc, cjtorresj@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1029c8-1e06-4ef5-b992-f36cae9d4caa",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8339d77-9304-4388-9a8c-ae264360c301",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com\n",
    "1. Jessica López Mejía, jelopezme@unal.edu.co\n",
    "1. Venus Celeste Puertas Gualtero, vpuertasg@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96f054-2fc2-4f85-b62c-c03d2c204b10",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Jefe Jurídica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6222dad-4270-466b-bfa5-c31e4a0f9006",
   "metadata": {},
   "source": [
    "6. Paula Andrea Guzmán, guzmancruz.paula@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714da2ce-e7ba-42e4-8925-434229ec3c1a",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Coordinador Jurídico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83341e02-6fd3-449e-8495-aeacd223bc3d",
   "metadata": {},
   "source": [
    "7. David Fuentes, fuentesd065@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f8b6f-193a-49dc-a08b-2f0c6abf7e47",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Desarrolladores Principales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c999d7-ec87-435d-812c-df5e05ac882f",
   "metadata": {},
   "source": [
    "8. Dairo Moreno, damoralesj@unal.edu.co\n",
    "9. Joan Castro, jocastroc@unal.edu.co\n",
    "10. Bryan Riveros, briveros@unal.edu.co\n",
    "11. Rosmer Vargas, rovargasc@unal.edu.co\n",
    "12. Venus Puertas, vpuertasg@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcf746-2063-499d-a4a5-87064001247b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Expertos en Bases de Datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ac84c-d227-4b6f-9efa-6de06723f473",
   "metadata": {
    "tags": []
   },
   "source": [
    "13. Giovvani Barrera, udgiovanni@gmail.com\n",
    "14. Camilo Chitivo, cchitivo@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dcf83-89c8-41a2-95d2-d359876a745d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f1544-61e0-41a9-bae4-480dd4fbe088",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2023](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Alvaro Montenegro, Daniel Montenegro y Oleg Jarma,  Inteligencia Artificial y Aprendizaje Profundo Avanzado, 2023](https://github.com/AprendizajeProfundo/Diplomado-Avanzado)\n",
    "1. [Tutoriales de Pytorch](https://pytorch.org/tutorials/)\n",
    "1. [Pytorchlightning.ai](https://www.pytorchlightning.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0961e2-f9a2-4a22-a215-15441aa3f0df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e76b5f-dfa0-4ef0-8298-7537151772b5",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Instalar Pytorch-lightning](#Instalar-Pytorch-lightning)\n",
    "* [Ejemplo de un módulo Lightning](#Ejemplo-de-un-módulo-Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8e8fd-b4d2-4d44-8511-718af854d4bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d026e31-2c95-4a27-9da0-45331b02c810",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/trainer.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab698e08-33bf-45c2-8b16-306084b6bf78",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02e2c2-0e22-4f37-8e7d-021c3bad1b04",
   "metadata": {},
   "source": [
    "En esta lección aprendemos como construir una modelo de clasificación de imágenes a color. Usaremos el framework [Pytorch-lightning](https://www.pytorchlightning.ai/) para construir el modelo y el conjunto de datos [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). La siguiente figura los tipos de datos que se usarán en esta lección. El contenido es una adaptación libre del tutorial [Image Classification using PyTorch Lightning](https://wandb.ai/wandb/wandb-lightning/reports/Image-Classification-using-PyTorch-Lightning--VmlldzoyODk1NzY) de [WandB](https://wandb.ai/site)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcd09b-7e30-47ed-b57d-d232193cb03f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/cifar10.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Universidad de Toronto-Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21048ee9-8cbe-49f6-b8f6-03261e092621",
   "metadata": {},
   "source": [
    "Para esta lección usaremos los datos CIFAR10 disponibles en los datasets de la librería [Torchvision](https://pytorch.org/vision/stable/index.html). Adicionalmente usaremos [Weights and bias-Wandb](https://wandb.ai/site), una plataforma moderna que puede apoyar para crear mejores modelos, más rápido con el seguimiento de experimentos, el control de versiones de conjuntos de datos y la gestión de modelos. WandB debe instalarse por separado, pero se incorpora a la librería `pytorch_lightning.loggers`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8198f82-960c-4665-b61d-e8715cdccaf9",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Descarga de los datos y creación de un dataset</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd232aa1-d6fd-40ee-9e2c-60b417ca31f1",
   "metadata": {},
   "source": [
    "Los datos pueden ser descargados directamente de la librería `Torchvision` como indicamos antes  y los datasets creados automaticamente  mediante el siguiente código. Descomente las las dos últimas líneas si desea crear el dataset directamente desde Torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c7c60-cbb7-425d-8f37-fbf88f133de7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carga el dataset CIFAR10\n",
    "import torchvision.datasets as datasets\n",
    "CIFAR10 = datasets.CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed5ce9-54cf-4cfe-88a1-797a5e55f156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Creación del dataset</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e175f-0345-4d82-841a-8a547685e061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_names = os.listdir(root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_name = self.class_names[idx]\n",
    "        class_dir = os.path.join(self.root_dir, class_name)\n",
    "        image_names = os.listdir(class_dir)\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for image_name in image_names:\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            label = idx  # Utiliza el índice de la clase como etiqueta\n",
    "\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1d056-307e-4201-8d4a-d7a3b02818b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Utiliza la clase CIFAR10Dataset \n",
    "#para crear un objeto de conjunto de datos personalizado\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Especifica la ruta de la carpeta donde se encuentran los datos\n",
    "data_root = \"../Datos/Cifar10\"\n",
    "\n",
    "# Transformaciones para normalizar y transformar las imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Asegura que todas las imágenes tengan el mismo tamaño\n",
    "    transforms.ToTensor(),  # Convierte las imágenes en tensores\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normaliza las imágenes\n",
    "])\n",
    "\n",
    "# Crea un objeto de conjunto de datos personalizado\n",
    "cifar10_dataset = CIFAR10Dataset(root_dir=data_root, transform=transform)\n",
    "\n",
    "# Crea un cargador de datos utilizando el conjunto de datos personalizado\n",
    "dataloader = DataLoader(cifar10_dataset, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14327a65-1c99-4832-b575-49ae84437201",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Preparación de los datos</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962b5c6-b5b5-49fb-b7da-2ab6656c1ac0",
   "metadata": {},
   "source": [
    "Los `DataModule` son una forma de desacoplar enlaces relacionados con datos del LightningModule para que pueda desarrollar modelos agnósticos de conjuntos de datos. En otra palabras, con DataModule puede preparar los datos por fuera del módulo de entrenamiento, de tal manera que peude cambiar sus datos sin tocar el módulo de entrenamiento.\n",
    "\n",
    "Con `DataModule` podemos organiza la canalización de datos en una clase compartible y reutilizable. Un módulo de datos encapsula los cinco pasos involucrados en el procesamiento de datos en PyTorch:\n",
    "\n",
    "* Descargar/tokenizar/procesar.\n",
    "* Limpiar y (tal vez) guardar en el disco.\n",
    "* Cargar dentro del conjunto de datos.\n",
    "* Aplicar transformaciones (rotar, tokenizar, etc.).\n",
    "* Envolver dentro de un DataLoader.\n",
    "\n",
    "\n",
    "Obtenga más información sobre los módulos de datos [aquí](https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html). Construyamos un módulo de datos para el conjunto de datos Cifar-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de02b2c-3da4-429e-bd12-7b20b0640afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Carga las librerías requeridas</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fa436-86f2-4db4-92a2-60250534ce69",
   "metadata": {},
   "source": [
    "Instale la libreria wandb con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c02d38-9fcb-49d6-ad2a-1d5d66c1f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea02acc2-4c38-4778-9914-f27a297fdb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Carga DataLoader para crear los dataloaders\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Librería Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# métricas\n",
    "import torchmetrics \n",
    "\n",
    "# reproductibilidad\n",
    "from pytorch_lightning import seed_everything # reproducibilidad\n",
    "\n",
    "# Carga WandBLogger para hacer seguimiento (tracking) del entrenamiento\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# carga el dataset CIFAR10\n",
    "import torchvision.datasets as datasets\n",
    "CIFAR10 = datasets.CIFAR10\n",
    "\n",
    "# callbacks\n",
    "from pytorch_lightning.callbacks import Callback \n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c9bc5-bf5d-41bc-a8b6-7d15ee9c1e24",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Canalización (pipeline) de los datos con DataModule</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d42586-b397-470c-8ccb-41a58581ba35",
   "metadata": {},
   "source": [
    "Construimos una clase derivada de `LightningDataModule` específica para CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e554bb-eeb6-47b2-9820-883b2751fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size, data_dir: str = './', num_workers=4):\n",
    "        \"\"\"\n",
    "        Pasaremos los hiperparámetros necesarios para nuestra canalización de datos\n",
    "        También definiremos la canalización de transformación de datos aquí.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        self.dims = (3, 32, 32)\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Aquí es donde definiremos la lógica para descargar nuestro conjunto de datos. \n",
    "        Estamos utilizando la clase de conjunto de datos CIFAR10 de torchvision para descargar.\n",
    "        Use este método para hacer cosas que podrían escribirse en el disco \n",
    "        o que deben hacerse solo desde una única GPU en configuraciones distribuidas. \n",
    "        No haga ninguna asignación de estado en esta función \n",
    "        (es decir, self.alguna_cosa = ...).\n",
    "        \"\"\"\n",
    "        # descarga\n",
    "        CIFAR10(self.data_dir, train=True, download=True)\n",
    "        CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Aquí es donde cargaremos los datos del archivo y prepararemos \n",
    "        los conjuntos de datos del tensor PyTorch para cada división de los datos. \n",
    "        La división de datos es por lo tanto reproducible. \n",
    "        Este método espera un argumento de etapa (stage) que se utiliza para separar \n",
    "        la lógica de 'entrenamiento' y de 'prueba'. \n",
    "        Esto es útil si no queremos cargar todo el conjunto de datos a la vez. \n",
    "        Las operaciones de datos que queremos realizar en cada GPU se definen aquí. \n",
    "        Esto incluye aplicar la transformación al conjunto de datos del tensor de PyTorch.\n",
    "        \"\"\"        \n",
    "        # Asigna datos a los datasets de entrenamiento/validación  \n",
    "        # para uso en los dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000, 5000])\n",
    "\n",
    "        # Asigna dataset de prueba para uso en los  dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    # Métodos para crear los dataloaders        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cifar_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cifar_test, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ded3e1-3cf2-4f1e-88a4-def6bbf41d45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Devolución de llamadas (Callbacks)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfeab5-2add-46f5-913e-e56b3c479883",
   "metadata": {},
   "source": [
    "Una devolución de llamada o `callback` es un programa autónomo que se puede reutilizar en todos los proyectos. PyTorch Lightning viene con algunos callbacks integrados que se usan regularmente.\n",
    "Obtenga más información sobre callbacks en PyTorch Lightning [aquí](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454fe0c-98d7-4d59-b0ab-b590c20b6344",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Devoluciones  de llamada integrados</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a219f-6c4d-457e-b826-304cb432d4c1",
   "metadata": {},
   "source": [
    "En este tutorial, utilizaremos las devoluciones de llamada integradas de parada anticipada `Early Stopping` y punto de control `checkpoint` de modelo. Estos callback se pueden pasar al entrenador (`Trainer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c264f-39f7-4c9e-bdfb-4c5054d4b5b4",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Devoluciones de llamada personalizados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d48f6-0cce-4817-b1d4-d2a79bf88936",
   "metadata": {},
   "source": [
    "Si está familiarizado con la devolución de llamada personalizada de `Keras`, la capacidad de hacer lo mismo en su canalización de PyTorch es solo una cereza del pastel.\n",
    "\n",
    "Dado que estamos realizando una clasificación de imágenes, la capacidad de visualizar las predicciones del modelo en algunas muestras de imágenes puede resultar útil. Esto en forma de callback puede ayudar a depurar el modelo en una etapa temprana. Así que vamos a implementar un callback personalizado para tal fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab2ec98-0a0c-4601-bbd0-b4bd0478f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictionLogger(Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        \"\"\"\n",
    "        ImagePredictionLogger es una subclase de la clase Callback de PyTorch Lightning. \n",
    "        params:\n",
    "        val_samples: es una tupla de imágenes y etiquetas. \n",
    "        num_samples: es el número de imágenes que se registrarán en el panel de control de W&B.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Este método es llamado cuando finaliza la época de validación. \n",
    "        Se necesitan dos argumentos: \n",
    "        trainer: entrenador del modelo\n",
    "        pl_module: módulo del modelo\n",
    "        Ambos parámetros son pasados automáticamente por el trainer.\n",
    "        Al usar trainer.logger.experimental podemos usar todas las funciones disponibles por Pesos y sesgos.\n",
    "        \"\"\"\n",
    "        # Trasladar los tensors a CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Obtener la predicción del modelo\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "        # Pasa las imágenes al logger como  wandb Image\n",
    "        trainer.logger.experiment.log({\n",
    "            \"ejemplos\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
    "                                                 preds[:self.num_samples], \n",
    "                                                 val_labels[:self.num_samples])]\n",
    "            })\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc7bd8-0cec-488f-b0f8-20eb795744c7",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Define el modelo: Backbone</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919af584-f434-4df5-8fc3-a8a1c4baea9c",
   "metadata": {},
   "source": [
    "Esta clase define el modelo de red neuronal que usaremos para nuestra investigación. Aunque en ocasiones el modelo hacer parte del sistema que definimos en la siguiente sección, desacoplar el código de esta forma lo hace más agnóstico y fácil de reutilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7f6bdef-6a5c-44c7-a1e1-325837a2002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(torch.nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        Modelo neuronal de clasificación paar los datos CIFAR10.\n",
    "        parámetros:\n",
    "        input_shape: tamaño de los tensores de entrada\n",
    "        num_classes: número de clases en el problema\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # log hyperparameters\n",
    "        #self.save_hyperparameters()\n",
    "        #self.learning_rate = learning_rate\n",
    "        \n",
    "        # bloque convolucional: cuerpo de la red\n",
    "        \"\"\"\n",
    "        CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
    "        \"\"\"\n",
    "        CLASS torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, \n",
    "        return_indices=False, ceil_mode=False)\n",
    "        \"\"\"\n",
    "        self.pool1 = torch.nn.MaxPool2d(2)\n",
    "        self.pool2 = torch.nn.MaxPool2d(2)\n",
    "        \n",
    "        # Calcula el tamaño de salida del bloque convolucional\n",
    "        n_sizes = self._get_conv_output(input_shape)\n",
    "        \n",
    "        # bloque Lineal: cabeza de la red\n",
    "        self.fc1 = nn.Linear(n_sizes, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    # métodos auxiliares para calcular el tamaño de salida\n",
    "    # del bloque convolucion. Se requiere ara poder configurar \n",
    "    # totalmente la red. No requrido en Keras\n",
    "    def _get_conv_output(self, shape):\n",
    "        \"\"\"\n",
    "        devuelve el tamaño del tensor de salida del bloque conv.\n",
    "        \"\"\"\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self._forward_features(input) \n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"\n",
    "        retorna el tensor de características de la salida el bloque conv\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool2(F.relu(self.conv4(x)))\n",
    "        return x\n",
    "    \n",
    "    # modelo\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Cálculo del modelo. Corre cada vez que se llama al modelo\n",
    "        \"\"\"\n",
    "        # calcula el bloque convolucional\n",
    "        x = self._forward_features(x)\n",
    "        # aplana el tensor de salida del bloque convolucional\n",
    "        x = x.view(x.size(0), -1) # x.size(0) es el tamaño del lote de datos\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1) # log softmax es más estable que softmax\n",
    "       \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3942a-b103-4171-ac33-809695545fb7",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">LightningModule - Definición del sistema</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea586c-1687-4d56-8d45-1e7057049bd3",
   "metadata": {},
   "source": [
    "`LightningModule` define un sistema y no un modelo. Aquí, un sistema agrupa todo el código de investigación en una sola clase para que sea autónomo. LightningModule organiza su código PyTorch en 5 secciones:\n",
    "\n",
    "* Bucle de entranamiento (training_step)\n",
    "* Bucle de validación (validation_step)\n",
    "* Bucle de prueba (test_step)\n",
    "* Optimizadores (configure_optimizers)\n",
    "\n",
    "\n",
    "Por lo tanto, se puede construir un modelo agnóstico del conjunto de datos que se puede compartir fácilmente. Construyamos un sistema para la clasificación Cifar-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c8d5a6b-1c04-47e1-952a-ac6378e428dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self,  backbone,  learning_rate=2e-4, ):\n",
    "        super().__init__()\n",
    "        # activa log para almacenar los hiperparámetros\n",
    "        self.save_hyperparameters()\n",
    "        # modelo\n",
    "        self.backbone = backbone\n",
    "        # rata  de aprendizaje para el optimizador\n",
    "        self.learning_rate = learning_rate\n",
    "        # métricas\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning automatiza la mayor parte del entrenamiento para nosotros, \n",
    "        la época y las iteraciones por lotes, todo lo que necesitamos mantener \n",
    "        es la lógica del paso de entrenamiento. El método training_step requiere argumentos \n",
    "        batch y batch_idx que el Entrenador pasa automáticamente. \n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = F.nll_loss(logits, y) # entropía cruzada: negative log likelihood\n",
    "        \n",
    "        # training metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "        #self.log('perdida_entrenamiento', loss )\n",
    "        #self.log('precision_entrenamiento', acc)\n",
    "        wandb.log({\"acc_train\": acc, \"loss_train\": loss})\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        el ciclo de validación se puede implementar sobrescribiendo este método \n",
    "        de LightningModule\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.valid_acc(preds, y)\n",
    "        #self.log('perdida_validacion', loss, prog_bar=True)\n",
    "        #self.log('precision_validacion', acc, prog_bar=True)\n",
    "        wandb.log({\"acc_test\": acc, \"loss_test\": loss})\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Este método similar al ciclo de validación. \n",
    "        La única diferencia es que en prueba solo se llama \n",
    "        cuando se usa trainer.test(). \n",
    "        Las métricas se registran automáticamente por épocas.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.test_acc(preds, y)\n",
    "        #self.log('perdida_test', loss, prog_bar=True)\n",
    "        #self.log('precision_test', acc, prog_bar=True)\n",
    "        wandb.log({\"acc_test\": acc, \"loss_test\": loss})\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Podemos definir nuestro optimizador y programadores \n",
    "        de tasa de aprendizaje usando el método \n",
    "        configure_optimizer. \n",
    "        Incluso se pueden definir múltiples optimizadores \n",
    "        como en el caso de las GAN.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.backbone.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff167f-641b-4454-9769-5e95be9af68a",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Entrenamiento y Evaluación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fc9a4-d3e4-48d6-968c-f111578fe4ae",
   "metadata": {},
   "source": [
    "Ahora que organizamos nuestra canalización de datos con `DataModule` y modelamos la arquitectura del modelo con `nn.Module` y el ciclo de entrenamiento con `LightningModule`, PyTorch Lightning `Trainer` automatiza todo lo demás por nosotros.\n",
    "\n",
    "El Entrenador automatiza:\n",
    "\n",
    "* Iteración de época y lote\n",
    "* Llamada de `optimizer.step()`, `backward`, `zero_grad()`\n",
    "* Llamada de `.eval()`, habilitación/deshabilitación de gradientes\n",
    "* Guardar y cargar pesos\n",
    "* Registro de pesos y sesgos\n",
    "* Soporte de entrenamiento multi-GPU\n",
    "* Soporte de TPU\n",
    "* soporte de entrenamiento de 16 bits\n",
    "\n",
    "\n",
    "Primero inicializaremos nuestra canalización de datos. El Entrenador solo necesita un  DataLoader de PyTorch para los datos de entrenamiento/val/prueba. \n",
    "\n",
    "Podemos pasar directamente el objeto `dm` que hemos creado al Entrenador. Pero dado que necesitamos algunas muestras para nuestro `ImagePredictionLogger`, llamaremos manualmente a los métodos *prepare_data* y *setup*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575df58-12a7-4c57-a0bb-99735900c002",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Instancia los datos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af0b5830-b278-4409-a04a-f0f00ff51ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa la canalización de los datos\n",
    "dm = CIFAR10DataModule(batch_size=32)\n",
    "# Para acceder al x_dataloader necesitamos llamar a prepare_data y setup.\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f223c4-5ad7-46b8-9f88-f984d9904502",
   "metadata": {},
   "source": [
    "Solo necesitamos inicializar el modelo y nuestro logger favorito. Tenga en cuenta que hemos pasado checkpoint_callback por separado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e05fba-eca3-4590-a344-ffff7994da9c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Instancia el modelo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "812ed2f8-fec5-4855-b01d-ec282a1c3123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backbone(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instancia el modelo\n",
    "inputshape = (3, 32, 32)\n",
    "numclases = 10\n",
    "\n",
    "backbone = Backbone(inputshape , numclases)\n",
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05710d66-1223-453d-83b8-42b7ff49c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34536afb-b003-4ed7-b866-dee50105f6b0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Instancia el trainer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fd79f21-ec21-4ccc-ac61-b7c057e5d9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instancia el trainer\n",
    "#trainer = pl.Trainer(gpus=4, precisión=16, limit_train_batches=0.5)\n",
    "max_epochs = 2 # 10\n",
    "acelerator = 'cpu'\n",
    "progress_bar_refresh_rate = 1\n",
    "\n",
    "# ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"../\", save_top_k=2, monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                     enable_progress_bar=True,\n",
    "                     accelerator=acelerator,                     \n",
    "                     callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\"),\n",
    "                                ImagePredictionLogger(val_samples)],\n",
    "                     enable_checkpointing=True,\n",
    "                     deterministic=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f73c8d7-5fb4-405c-86c3-d8913cc8c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mammontenegrod\u001b[0m (\u001b[33maprendizaje-profundo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alvaro/Documents/AprendizajeProfundo/Diplomado_UNAL/Diplomado-master_sep_2022/Temas/Módulo 15- Pytorch/Cuadernos/wandb/run-20230512_161104-wdv7ffmq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aprendizaje-profundo/CIFAR10/runs/wdv7ffmq' target=\"_blank\">dauntless-wind-3</a></strong> to <a href='https://wandb.ai/aprendizaje-profundo/CIFAR10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aprendizaje-profundo/CIFAR10' target=\"_blank\">https://wandb.ai/aprendizaje-profundo/CIFAR10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aprendizaje-profundo/CIFAR10/runs/wdv7ffmq' target=\"_blank\">https://wandb.ai/aprendizaje-profundo/CIFAR10/runs/wdv7ffmq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aprendizaje-profundo/CIFAR10/runs/wdv7ffmq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7b8f60f410>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configura el logger wandb\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='CIFAR10', \n",
    "           config={\n",
    "           \"learning_rate\": 0.02,\n",
    "           \"architecture\": \"CNN\",\n",
    "           \"dataset\": \"CIFAR-100\",\n",
    "           \"epochs\": 2,\n",
    "            }\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f850465-4a7a-4da6-b98d-8bf178293907",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrena el modelo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f41ea-411f-4afb-9ccc-1b94a2e34474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrena el modelo\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40b98b-522c-4d6d-895c-eb65c394862c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Evalua el modelo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6781480-fdd4-483d-adcf-4c3b01ba34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalua el modelo en el conjunto de prueba retenido\n",
    "trainer.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28055c6-1523-4252-949e-6d6b506ecf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
