{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diplomado en Inteligencia Artificial y Aprendizaje Profundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Lineal de Clasificación  con JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Función de Predicción](#Función-de-predicción)\n",
    "* [Función de Pérdida. Entropía cruzada](#Función-de-pérdida.-Entropía-cruzada)\n",
    "* [Ejemplo. Datos de Juguete](#Ejemplo.-Datos-de-Juguete)\n",
    "* [Gradiente](#Gradiente)\n",
    "* [Entrenamiento del modelo](#Entrenamiento-del-modelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con su versión actualizada de [Autograd](https://github.com/hips/autograd), [JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) puede diferenciar automáticamente el código nativo de Python y NumPy. Puede derivarse través de un gran subconjunto de características de Python, incluidos bucles, ifs, recursión y clousures, e incluso puede tomar derivadas de derivadas de derivadas. Admite la diferenciación tanto en modo inverso como en modo directo, y los dos pueden componerse arbitrariamente en cualquier orden.\n",
    "\n",
    "Lo nuevo es que JAX usa [XLA](https://www.tensorflow.org/xla) para compilar y ejecutar su código NumPy en aceleradores, como GPU y TPU. La compilación ocurre de forma predeterminada, con las llamadas de la biblioteca compiladas y ejecutadas justo a tiempo. Pero JAX incluso le permite compilar justo a tiempo sus propias funciones de Python en núcleos optimizados para XLA utilizando una API de una función. La compilación y la diferenciación automática se pueden componer de forma arbitraria, por lo que puede expresar algoritmos sofisticados y obtener el máximo rendimiento sin tener que abandonar Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jax jaxlib \n",
    "\n",
    "from __future__ import print_function\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "import itertools\n",
    "\n",
    "\n",
    "#import random\n",
    "#import jax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x/2)+1)\n",
    "# more stable than  1.0/(1+np.exp(-x))\n",
    "\n",
    "# outputs probability of a label being true\n",
    "def predict(W,b,inputs):\n",
    "    return sigmoid(np.dot(inputs,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida. Entropía cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training loss: -log likelihood of trainig examples\n",
    "def loss(W,b,x,y):\n",
    "    preds = predict(W,b,x)\n",
    "    label_probs = preds*y + (1-preds)*(1-y)\n",
    "    return -np.sum(np.log(label_probs))\n",
    "\n",
    "# initialize coefficients\n",
    "key, W_key, b_key = random.split(key,3)\n",
    "W = random.normal(key, (3,))\n",
    "b = random.normal(key,())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo. Datos de Juguete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a toy dataset\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la funcion *grad* con sus argumentos  para diferenciar la función con respecto a sus parámetros ṕosicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile with jit\n",
    "# argsnums define positional params to derive with respect to\n",
    "grad_loss = jit(grad(loss,argnums=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_grad, b_grad = grad_loss(W,b,inputs, targets)\n",
    "print(\"W_grad = \", W_grad)\n",
    "print(\"b_grad = \", b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(W,b,x,y, lr= 0.12):\n",
    "    gradient = grad_loss(W,b,inputs,targets) \n",
    "    W_grad, b_grad = grad_loss(W,b,inputs,targets)\n",
    "    W -= W_grad*lr\n",
    "    b -= b_grad*lr\n",
    "    return(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    \n",
    "weights, biases = [], []\n",
    "train_loss= []\n",
    "epochs = 20\n",
    "\n",
    "train_loss.append(loss(W,b,inputs,targets))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    W,b = train(W,b,inputs, targets)\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    losss = loss(W,b,inputs,targets)\n",
    "    train_loss.append(losss)\n",
    "    print(f\"Epoch {epoch}: train loss {losss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights')\n",
    "for weight in weights:\n",
    "    print(weight)\n",
    "print('biases')\n",
    "for bias in biases:\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(loss)(W,b,inputs,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando el valor de la función y el gradiente con value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "loss_val, Wb_grad = value_and_grad(loss,(0,1))(W,b,inputs, targets)\n",
    "print('loss value: ', loss_val)\n",
    "print('gradient value: ', Wb_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
