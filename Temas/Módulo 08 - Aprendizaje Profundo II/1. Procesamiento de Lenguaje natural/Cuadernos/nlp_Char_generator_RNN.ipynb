{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto usando caracteres y redes recurrentes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tensorflow, [Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "2. Ralf C. Staudemeyer and Eric Rothstein Morris,[Understanding LSTM a tutorial into Long Short-Term Memory Recurrent Neural Networks*, arxiv, September 2019](https://arxiv.org/pdf/1909.09586.pdf)\n",
    "3. karpathy, [The Unreasonable Effectiveness of Recurrent Neural Networks](  http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Importa módulos requeridos](#Importa-módulos-requeridos)\n",
    "* [Descarga los datos](#Descarga-los-datos)\n",
    "* [Crea  el alfabeto](#Crea-el-alfabeto)\n",
    "* [Crea los diccionarios ](#Crea-los-diccionarios )\n",
    "* [Transforma el texto en un arreglo de caracteres](#Transforma-el-texto-en-un-arreglo-de-caracteres)\n",
    "* [Creación de datos de entrenamiento y etiquetas](#Creación-de-datos-de-entrenamiento-y-etiquetas)\n",
    "* [Construcción del Modelo](#Construcción-del-Modelo)\n",
    "* [Prueba del modelo ](#Prueba-del-modelo)\n",
    "* [Entrenamiento](#Entrenamiento)\n",
    "* [Generación de texto](#Generación-de-texto)\n",
    "* [Entrenamiento personalizado](#Entrenamiento-personalizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno es una adaptación del tutorial de tensorflow [Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "Se muestra cómo generar texto usando un RNR basado en caracteres. Trabajaremos con un conjunto de datos de los escritos de Shakespeare preparados por Andrej Karpathy en su blog sobre [La irrazonable efectividad de las redes neuronales recurrente]( http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "\n",
    "Dada una secuencia de caracteres a partir de estos datos (\"Shakespear\"), se entrena un modelo para predecir el siguiente carácter en la secuencia (\"e\"). \n",
    "\n",
    "\n",
    "Se pueden generar secuencias de texto más largas llamando al modelo repetidamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Tensorflow:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Versión de Tensorflow: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtiene el path en donde están los datos y los descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lee los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una mirada a los primeros 250  caracters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea  el alfabeto\n",
    "\n",
    "Este alfabeto es una lista que contiene las letras mayúscula y munúsculas, y algunos caracteres especiales, incluyendo el salto de línea '\\n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea los diccionarios "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- caracter a índice\n",
    "- índice a caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
       "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma el texto en un arreglo de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, ..., 45,  8,  0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostración del uso de los diccionarios\n",
    "\n",
    "La función *repr* convierte el argumento en una expresión imprimible(cuando es posible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La tarea de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dado un caracter, o una secuencia de caracteres, ¿cuál es el próximo caracter más probable? Esta es la tarea en la que vamos a entrenar al modelo. \n",
    "\n",
    "\n",
    "La entrada al modelo será una secuencia de caracteres, y entrenamos al modelo para predecir la salida, el siguiente carácter en cada paso de tiempo.\n",
    "\n",
    "Dado que los RNR mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres calculados hasta este momento, ¿cuál es el siguiente carácter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de datos de entrenamiento y etiquetas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto se divide  en secuencias de entrenamiento. Cada secuencia de entrada contendrá  una longitud *seq_length* de caracteres del texto.\n",
    "\n",
    "Para cada secuencia de entrada, las etiquetas correspondientes contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
    "\n",
    "Así que primero se divide el texto en trozos de *seq_length + 1*. Por ejemplo, digamos que *seq_length* es 3 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia de destino \"ola\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se usa la función *tf.data.Dataset.from_tensor_slices* para convertir el vector de texto en una secuencia de índices de caracteres. Un tensor de enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método por lotes nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (101,), types: tf.int64>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n',\n",
      "       ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p',\n",
      "       'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u',\n",
      "       'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm',\n",
      "       'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'A', 'l', 'l',\n",
      "       ':', '\\n', 'S', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a',\n",
      "       'k', '.', '\\n', '\\n', 'F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't',\n",
      "       'i', 'z', 'e', 'n', ':', '\\n', 'Y', 'o', 'u', ' '], dtype='<U1')\n",
      "array(['a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l',\n",
      "       'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o',\n",
      "       ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ',\n",
      "       'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'A', 'l', 'l', ':',\n",
      "       '\\n', 'R', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e',\n",
      "       's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'F', 'i', 'r', 's',\n",
      "       't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'F', 'i',\n",
      "       'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k'], dtype='<U1')\n",
      "array(['n', 'o', 'w', ' ', 'C', 'a', 'i', 'u', 's', ' ', 'M', 'a', 'r',\n",
      "       'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f',\n",
      "       ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e',\n",
      "       ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'A', 'l', 'l',\n",
      "       ':', '\\n', 'W', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ',\n",
      "       'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'F',\n",
      "       'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':',\n",
      "       '\\n', 'L', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i'], dtype='<U1')\n",
      "array(['l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w',\n",
      "       'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r',\n",
      "       'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ',\n",
      "       'p', 'r', 'i', 'c', 'e', '.', '\\n', 'I', 's', \"'\", 't', ' ', 'a',\n",
      "       ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'A', 'l',\n",
      "       'l', ':', '\\n', 'N', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a',\n",
      "       'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l',\n",
      "       'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd'], dtype='<U1')\n",
      "array(['o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w',\n",
      "       'a', 'y', '!', '\\n', '\\n', 'S', 'e', 'c', 'o', 'n', 'd', ' ', 'C',\n",
      "       'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'O', 'n', 'e', ' ', 'w',\n",
      "       'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't',\n",
      "       'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'F', 'i', 'r', 's', 't',\n",
      "       ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'W', 'e', ' ',\n",
      "       'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd',\n",
      "       ' ', 'p', 'o', 'o', 'r', ' ', 'c', 'i', 't', 'i'], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(5):\n",
    "  print(repr(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para cada secuencia, duplíquela y cámbiela para formar el texto de entrada y de destino utilizando el método  map para aplicar una función simple a cada lote. Es  similar a la función apply de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
      "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
      "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
      "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
      "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
      "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "       53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
      "       63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51,\n",
      "       43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54,\n",
      "       43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57,\n",
      "       58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1])>)\n",
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([39, 56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1,\n",
      "       56, 39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39,\n",
      "       52,  1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50,\n",
      "       10,  0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50,\n",
      "       60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64,\n",
      "       43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59,  1])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n",
      "       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n",
      "        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n",
      "        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n",
      "       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n",
      "       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59,  1, 49])>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de lotes de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, necesitamos mezclar los datos y empaquetarlos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use `tf.keras.Sequential` para definir el modelo. Para este sencillo ejemplo, se utilizan tres capas para definir nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/text_generation_training.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de generación de texto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora ejecute el modelo para ver que se comporta como se esperaba.\n",
    "\n",
    "Primero verifique la forma de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, la longitud de secuencia de la entrada es `100` pero el modelo puede ejecutarse en entradas de cualquier longitud.\n",
    "\n",
    "El dataset de entrenamiento que tiene lotes de tamaño 64 * 100, los cuales tiene orden aleatorio (ver celda 249 se extrae El primer lote actual y se pasa por el modelo.\n",
    "\n",
    "El modelo tiene de momento los pesos inciales. El lote de datos pasa por el modelo y devuelve un tensor de tamaño (64, 100, 65).\n",
    "\n",
    "Como el vocabulario tien 65 caracteres y cada sequencia tiene tamaño 100, el modelo predice el siguiente caractrer para cada caracter en la entrada.\n",
    "\n",
    "La predicción funciona así:\n",
    "\n",
    "Por cada caracter de entrada, se predice el siguiente caractaer así: el modelo asigna un valor numérico a cada elemento en el vocabulario. Si se eligiera el máximo el caracter seleccionado sería simplemente el carater con el valor mas grande.\n",
    "\n",
    "Ya por fuera del modelo.\n",
    "\n",
    "El siguiente es el vector de predicciones del siguiente caracter para todos los caracteres de la primera secuencia en el primer bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener predicciones reales del modelo, necesitamos tomar muestras de la distribución de salida, para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de los caracteres.\n",
    "\n",
    "Nota: Es mejor muestrear  esta distribución,  que tomar el *argmax* para evitar  que el modelo quede atascado en un bucle.\n",
    "\n",
    "Probemos para el primer ejemplo en el lote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 65), dtype=float32, numpy=\n",
       "array([[ 0.0098305 ,  0.00348802, -0.00275722, ...,  0.00632911,\n",
       "        -0.00936237, -0.00554749],\n",
       "       [ 0.01394573,  0.00106732, -0.01497596, ...,  0.00198129,\n",
       "        -0.0045165 ,  0.00952342],\n",
       "       [ 0.01688722,  0.00932702, -0.00521472, ...,  0.00950518,\n",
       "         0.00519089, -0.00446027],\n",
       "       ...,\n",
       "       [ 0.00468424, -0.0112329 , -0.01078923, ..., -0.00687163,\n",
       "        -0.00174603, -0.00093643],\n",
       "       [ 0.01254865, -0.00709624, -0.00906933, ...,  0.00107603,\n",
       "        -0.00896124, -0.00605359],\n",
       "       [ 0.01664659, -0.00633432,  0.0001511 , ...,  0.01343223,\n",
       "        -0.01652833, -0.00117041]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto valores se transforman con logits para obtener probabilidades. Entonces se normaliza a la una distribución categórica, es decir se usa softmax. Con esto, se genera una muestra de la variable categórica para seleccionar el caracter.\n",
    "\n",
    "El siguiente es el vector de predicciones del sigiente caracter para todos los caracteres de  la primera secuencia en el primer bloque.\n",
    "\n",
    "Vamos a predecir los caracteres como indicamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1), dtype=int64, numpy=\n",
       "array([[59],\n",
       "       [32],\n",
       "       [20],\n",
       "       [ 7],\n",
       "       [40],\n",
       "       [17],\n",
       "       [23],\n",
       "       [30],\n",
       "       [ 6],\n",
       "       [22],\n",
       "       [53],\n",
       "       [ 2],\n",
       "       [14],\n",
       "       [33],\n",
       "       [10],\n",
       "       [54],\n",
       "       [59],\n",
       "       [20],\n",
       "       [63],\n",
       "       [40],\n",
       "       [46],\n",
       "       [ 5],\n",
       "       [28],\n",
       "       [60],\n",
       "       [14],\n",
       "       [24],\n",
       "       [57],\n",
       "       [ 4],\n",
       "       [49],\n",
       "       [17],\n",
       "       [15],\n",
       "       [55],\n",
       "       [ 3],\n",
       "       [20],\n",
       "       [13],\n",
       "       [48],\n",
       "       [50],\n",
       "       [35],\n",
       "       [ 1],\n",
       "       [50],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [64],\n",
       "       [61],\n",
       "       [49],\n",
       "       [18],\n",
       "       [ 1],\n",
       "       [23],\n",
       "       [12],\n",
       "       [20],\n",
       "       [32],\n",
       "       [16],\n",
       "       [33],\n",
       "       [51],\n",
       "       [25],\n",
       "       [44],\n",
       "       [52],\n",
       "       [ 3],\n",
       "       [47],\n",
       "       [32],\n",
       "       [13],\n",
       "       [44],\n",
       "       [47],\n",
       "       [39],\n",
       "       [47],\n",
       "       [49],\n",
       "       [42],\n",
       "       [33],\n",
       "       [42],\n",
       "       [ 9],\n",
       "       [64],\n",
       "       [35],\n",
       "       [10],\n",
       "       [53],\n",
       "       [15],\n",
       "       [63],\n",
       "       [15],\n",
       "       [42],\n",
       "       [17],\n",
       "       [64],\n",
       "       [11],\n",
       "       [52],\n",
       "       [33],\n",
       "       [11],\n",
       "       [11],\n",
       "       [ 9],\n",
       "       [55],\n",
       "       [23],\n",
       "       [10],\n",
       "       [25],\n",
       "       [21],\n",
       "       [17],\n",
       "       [26],\n",
       "       [38],\n",
       "       [44],\n",
       "       [62],\n",
       "       [14],\n",
       "       [21],\n",
       "       [53],\n",
       "       [12]])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 59, 49, 20, 30, 28, 44, 57, 56,  8, 23, 40, 11, 59, 16, 43,  0,\n",
       "       18,  8, 48, 25, 27, 48, 14,  7,  3, 49, 23, 43, 44, 42,  6, 47, 53,\n",
       "        2, 21,  3, 56,  5, 63,  2, 22, 35, 17, 11, 28,  1, 48, 25, 36,  0,\n",
       "       35, 42, 28, 38, 18, 11, 63, 32, 63, 57,  1, 43,  7, 49, 20, 45,  1,\n",
       "       29,  0, 34, 18, 54,  0, 54, 44, 38,  6,  8, 24, 11, 24, 58, 14, 62,\n",
       "       44, 16, 25, 36, 20, 34, 37,  6,  0, 42, 23,  9, 48, 43, 39])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da, en cada paso de tiempo, una predicción del siguiente índice de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 59, 49, 20, 30, 28, 44, 57, 56,  8, 23, 40, 11, 59, 16, 43,  0,\n",
       "       18,  8, 48, 25, 27, 48, 14,  7,  3, 49, 23, 43, 44, 42,  6, 47, 53,\n",
       "        2, 21,  3, 56,  5, 63,  2, 22, 35, 17, 11, 28,  1, 48, 25, 36,  0,\n",
       "       35, 42, 28, 38, 18, 11, 63, 32, 63, 57,  1, 43,  7, 49, 20, 45,  1,\n",
       "       29,  0, 34, 18, 54,  0, 54, 44, 38,  6,  8, 24, 11, 24, 58, 14, 62,\n",
       "       44, 16, 25, 36, 20, 34, 37,  6,  0, 42, 23,  9, 48, 43, 39])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta salida significa que el caracter predicho como el siguiente para el primer caracter es el caracter 58 y asi...\n",
    "\n",
    "Los textos lucen así actualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodifíquelos para ver el texto predicho por este modelo no entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " ' so my care\\nTo have you royally appointed as if\\nThe scene you play were mine. For instance, sir,\\nTha'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"tukHRPfsr.Kb;uDe\\nF.jMOjB-$kKefd,io!I$r'y!JWE;P jMX\\nWdPZF;yTys e-kHg Q\\nVFp\\npfZ,.L;LtBxfDMXHVY,\\ndK3jea\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, el problema puede tratarse como un problema de clasificación estándar. Dado el estado RNN anterior, y la entrada en este paso de tiempo, predice la clase del siguiente carácter.\n",
    "\n",
    "Se define una función de pérdida adecuada para este caso. Esta función recibe los targets (labels) y las predicciones La función calcula la distribución a partir de las predicciones y genera para cada caracter un valor predicho. Esto es lo que se necesita para calcular la entropía cruzada en este caso (promedio). Este es el valor retornado. En el ejemplo, como entra una secuencia y se predicen 100 caracteres hay 100 distribuciones para el siguiente caracter. Una por cada caracter en la entrada. En total hay un lote de 64 secuencias. Luego la función de pérdida se basa en el promedio de  64∗100=6400  entropías cruzadas dispersas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjuntar un optimizador y una función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
    "\n",
    "Debido a que nuestro modelo devuelve logits, debemos establecer el indicador `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1738386\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Configure el procedimiento de entrenamiento utilizando el método `tf.keras.Model.compile`. Usaremos `tf.keras.optimizers.Adam` con argumentos predeterminados y la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice un `tf.keras.callbacks.ModelCheckpoint` para asegurarse de que los puntos de control se guarden durante el entrenamiento. Se crea un directorio en el cual se guardará checkpoints con el estado del modelos. Los pesos son guardados allí para uso posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecuta el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En history queda al información de loss, y acurracy para revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaura el último  checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vamos a tomar un pre-entranamiento. \n",
    "\n",
    "Usaremos los pesos almacenados, pero ahora vamos a cambiar el modelo para recibir lotes de tamaño 1. Es decir vamos a recibir una secuencia para hacer la predicción a partir de esa secuencia.\n",
    "\n",
    "\n",
    "Debido a la forma en que se pasa el estado RNN de un paso a otro, el modelo solo acepta un tamaño de lote fijo una vez construido.\n",
    "\n",
    "Para ejecutar el modelo con un tamaño de lote diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el punto de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El bucle de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código genera el texto:\n",
    "\n",
    "* Comienza eligiendo una cadena de inicio, inicializando el estado de la  RNR y configurando el número de caracteres a generar.\n",
    "\n",
    "* Obtenga la distribución de predicción del siguiente carácter utilizando la cadena de inicio y el estado de la  RNR.\n",
    "\n",
    "* Luego, use una distribución categórica para calcular el índice del carácter predicho. Use este caracter predicho como nuestra próxima entrada al modelo.\n",
    "\n",
    "* El estado RNN devuelto por el modelo se retroalimenta al modelo para que ahora tenga más contexto, en lugar de una sola palabra. Después de predecir la siguiente palabra, los estados RNN modificados se retroalimentan nuevamente en el modelo, que es cómo aprende a medida que obtiene más contexto de las palabras predichas previamente.\n",
    "\n",
    "La siguiente es la linea que define la capa GRU arriba en la definición del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.layers.GRU(rnn_units,  return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro *stateful* determina si los valores de estado recurrente deben matenerse (True) o no, cuando se pasa al siguiente caracter. En este caso, al mantenerse esos valores, se mantiene la memoria de la secuencia inicial y de los caracteres que van siendo generados. \n",
    "\n",
    "Por eso, solamente se pasa en el primer paso la secuencia de entrada completa. Desde el segundo paso solamente se pasa el nuevo caracter (que es el predicho). Con esta predicción y la memoria del resto de la secuencia anterior mantenida en estado recurrente se hace la siguiente predicción y así sucesivamente tomado la última secuencia pasada por la capa.\n",
    "\n",
    "Para pasar la última secuencia tratada en la capa GRU, se usa el parámetro *return_sequences*. Por defecto es falso, es decir se omite en la salida la última secuencia pasada por la capa GRU. Aquí hemos colocado *return_sequences=True*, para pasar únicamente el último carater predicho.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Al observar el texto generado, verá que el modelo sabe cuándo colocar mayúsculas, hacer párrafos e imita un vocabulario de escritura similar a Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar oraciones coherentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/text_generation_sampling.png\" width=\"800\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Muestreo  de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función implementa el ciclo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states() # reset the recurrent units \n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas.\n",
    "1. *reset_states* borra solo los estados recurrentes de la red. Vale la pena mencionar que dependiendo de si la opción *stateful = True* se configuró en la red, el comportamiento de esta función podría ser diferente. Si no está configurado, todos los estados se restablecen automáticamente después de cada cálculo por lotes en su red (por ejemplo, después de llamar a *fit, predict y evaluate* también). Si no es así, debe llamar a reset_states cada vez que desee que las llamadas de modelo consecutivas sean independientes.\n",
    "2. *tf.expand_dims* devuelve un tensor con una dimensión adicional insertada en el eje de índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: they are toward up comfort,\n",
      "As when thou comest: you show'd mementing,\n",
      "She did unpeening\n",
      "too:\n",
      "Their lastless gay of my wife, as he be togs and pide\n",
      "And not become our son.\n",
      "\n",
      "Provost:\n",
      "I brought thee. How our dath have to\n",
      "Forguit as pinisly wanting fair Nadua,\n",
      "Or honourable garther of my request of your affection\n",
      "Thy general and strange of a goadensfleshy enough their instrument:\n",
      "Consent, and nothing craves them; for my consumate me\n",
      "To the day a shrewful boy:\n",
      "Eacle! pardonly; for, upure rume that cannot be\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "This one that should confluct thee here the time\n",
      "From what my treeching frames in my\n",
      "soltue garlands. But what make hassing\n",
      "That rather content a wife, queen. War it, make toon\n",
      "To nou quaken.\n",
      "\n",
      "KANGARIT:\n",
      "Ay, so Gentle queen,\n",
      "Or any other dismay ent-hatred be the first blood icud tale!\n",
      "\n",
      "CLEON MARAARANA:\n",
      "Good night! phe undonefully: Ill periel.\n",
      "\n",
      "LEONTES:\n",
      "No!\n",
      "There was thy daughter?\n",
      "\n",
      "First Senator:\n",
      "What trefts it were, for honour, thou wilt\n",
      "Flatter Kate, and hither so he th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más fácil que puedes hacer para mejorar los resultados es entrenarlo por más tiempo (prueba `EPOCHS = 30`).\n",
    "\n",
    "También puede experimentar con una cadena de inicio diferente, o intentar agregar otra capa RNN para mejorar la precisión del modelo, o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El procedimiento de entrenamiento anterior es simple, pero no le da mucho control.\n",
    "\n",
    "Entonces, ahora que ha visto cómo ejecutar el modelo manualmente, descomprimimos el ciclo de entrenamiento e implementémoslo nosotros mismos. Esto proporciona un punto de partida si, por ejemplo, se implementa _ aprendizaje curricular_ para ayudar a estabilizar la salida de bucle abierto del modelo.\n",
    "\n",
    "Usaremos `tf.GradientTape` para rastrear los gradientes. Puede obtener más información sobre este enfoque leyendo el [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "El procedimiento funciona de la siguiente manera:\n",
    "\n",
    "* Primero, inicialice el estado RNN. Hacemos esto llamando al método `tf.keras.Model.reset_states`.\n",
    "\n",
    "* Luego, repita el conjunto de datos (lote por lote) y calcule las * predicciones * asociadas con cada una.\n",
    "\n",
    "* Abra un `tf.GradientTape`, y calcule las predicciones y pérdidas en ese contexto.\n",
    "\n",
    "* Calcular los gradientes de la pérdida con respecto a las variables del modelo utilizando el método `tf.GradientTape.grads`.\n",
    "\n",
    "* Finalmente, dé un paso hacia abajo utilizando el método `tf.train.Optimizer.apply_gradients` del optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # initializing the hidden state at the start of every epoch\n",
    "  # initally hidden is None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Regresar al inicio](Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
