{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diplomado en Inteligencia Artificial y Aprendizaje Profundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos de Optimización "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Métodos basados en el gradiente](#Métodos-basados-en-el-gradiente)\n",
    "* [Gradiente descendiente en lote](#Gradiente-descendiente-en-lote)\n",
    "* [Gradiente descendiente estocástico](#Gradiente-descendiente-estocástico)\n",
    "* [Gradiente descendiente por mini-lotes](#Gradiente-descendiente-por-mini-lotes)\n",
    "* [Método del momento](#Método-del-momento)\n",
    "* [RMSprop](#RMSprop)\n",
    "* [Algoritmo Adam](#Algoritmo-Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los algoritmos de aprendizaje profundo implican optimización de algún tipo. La optimización se refiere a la tarea de minimizar o maximizar alguna función $ f (x) $ alterando $ x $. Por lo general, expresamos la mayoría de los problemas de optimización en términos de minimizar $ f (x) $.\n",
    "\n",
    "\n",
    "La función que queremos minimizar  se llama función o criterio **objetivo (objetivo)**. \n",
    "\n",
    "Cuando estamos minimizando, también podemos llamarla función de costo, **función de pérdida** o función de error.\n",
    "\n",
    "\n",
    "\n",
    "A menudo denotamos el valor que minimiza una función con un superíndice ∗. \n",
    "\n",
    "\n",
    "Por ejemplo, podríamos decir\n",
    "\n",
    "$$x^∗ = argmin f(x).$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos basados en el gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A menudo minimizamos las funciones que tienen múltiples entradas: $ f: \\mathbb{R}^n \\to \\mathbb {R} $. \n",
    "\n",
    "Para que el concepto de \"minimización\" para tener sentido, debe haber una sola salida (función escalar).\n",
    "\n",
    "\n",
    "Para funciones con múltiples entradas, se hace uso del concepto de derivadas parciales. La derivada parcial $\\frac{\\partial}{\\partial x_i} f(x)$  mide como cambia $f$  cuando la\n",
    "variable $x_i$ crece o decrece desde el punto  $x$. \n",
    "\n",
    "\n",
    "El gradiente generaliza la noción de derivada al caso en que la derivada es con respecto a un vector: el gradiente de $ f $ es el vector que contiene todas las derivadas parciales, denotado $ \\nabla_xf (x) $. El elemento $ i $ del gradiente es la derivada parcial de f con respecto a $ x_i $.\n",
    "\n",
    "\n",
    "En múltiples dimensiones, los puntos críticos son puntos donde cada elemento del gradiente es igual a cero.\n",
    "\n",
    "\n",
    "Por otro lado, se puede verificar que el gradiente $ \\nabla_xf (x) $  es el vector geométrico que con base en el punto $X$ apunta en la direección en la cual crece más rápidamente la función. En consecuencia, $ -\\nabla_xf (x) $ apunta en la dirección contraria, es decir en la dirección hacia la cual la función decrece más rápido, desde el punto $x$. Esta es la base de los métodos de optimización basados en el gradiente. \n",
    "\n",
    "El término **gradiente descediente** indica que se usará $ -\\nabla_xf (x) $ para moverse a un siguiente punto en busca de un minimo local.\n",
    "\n",
    "El método general se escribe como:\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} − \\eta_{k} \\nabla_x f(x^{(k)})\n",
    "$$\n",
    "\n",
    "\n",
    "Los  valores  $\\eta_k$ se denominan genéricamente  **tasa de aprendizaje**.\n",
    "\n",
    "\n",
    "La razón de incorporar la tasa de aprendizaje es controlar el tamaño de paso. Si no hace esta corrección podemos alejarnos en lugar de acercarnos al mínimo que se está buscando.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente descendiente en lote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el descenso de gradiente  descendiente vainilla (vainilla se refiere al ejemplo básico), también conocido como descenso de gradiente por lotes, calcula el gradiente de la función de pérdida con respecto a los parámetros $\\boldsymbol{\\theta}$ para el **conjunto de datos de entrenamiento completo** $(\\mathbf{x}_{train},\\mathbf{y}_{train})$. Si $\\mathfrak{L}$ es la función de pérdida del problema, entonces se tiene que \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{k+1} =  \\boldsymbol{\\theta}_k - \\eta_k \\nabla_{\\boldsymbol{\\theta}} \\mathfrak{L}(\\mathbf{x}_{train},\\mathbf{y}_{train},\\boldsymbol{\\theta}_k),\n",
    "$$\n",
    "\n",
    "\n",
    "El principal problema a resolver con los métodos de gradiente descendiente es cómo definir y actualizar en cada paso la tasa de aprendizaje $\\eta_k $.\n",
    "\n",
    "Un fragmento de código puede lucir como sigue. Supongamos que al comenzar $0<\\eta_0<1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(theta, x_train, y_train, loss_func, epochs):\n",
    "    for i in range (epochs):\n",
    "        gradient = evaluate_gradient(loss_func, x_train, y_train, theta)\n",
    "        theta -=  eta * gradient\n",
    "        eta   *= eta\n",
    "    return theta, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente descendiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El descenso de gradiente estocástico (SGD), por el contrario, realiza una actualización de parámetros para cada ejemplo de entrenamiento $x_{train}^{(i)} $ y etiqueta $ y_{train}^ {(i)} $, seleccionados al azar en cada época.\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{k+1} =  \\boldsymbol{\\theta}_k - \\eta_k \\nabla_{\\boldsymbol{\\theta}} \\mathfrak{L}({x}_{train}^{(i)},{y}_{train}^{(i)},\\boldsymbol{\\theta}_k),\n",
    "$$\n",
    "\n",
    "\n",
    "En el artículo original de [Robbins and Monro (1951)](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586) $\\eta$ cambia en cada iteración como acabamos de mostrar y se asume que  $\\{\\eta_k\\}$ es una sucesión tal que $\\sum_k \\eta_k = \\infty$, and $\\sum_k \\eta_k^2 < \\infty$. Por ejemplo $\\eta_k = 1/k$.\n",
    "\n",
    "Un fragmento de código luce como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(theta, data_train, loss_func, epochs):\n",
    "    for i in range (epochs):\n",
    "        np.random.shuffle (data)\n",
    "        for example in data:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func,x, y, theta )\n",
    "            theta = theta - eta * gradient\n",
    "            eta *= eta\n",
    "    return theta, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente descendiente por mini-lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El descenso de gradiente por mini-lotes finalmente toma lo mejor de los dos mundos anteriores y realiza una actualización para cada mini-lote de $n$ ejemplos de entrenamiento:\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{k+1} =  \\boldsymbol{\\theta}_k - \\eta_k \\nabla_{\\boldsymbol{\\theta}} \\mathfrak{L}(\\mathbf{x}_{train}^{(i:i+n)},\\mathbf{y}_{train}^{(i:i+n)},\\boldsymbol{\\theta}_k),\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Desde este punto de la lección, asumiremos que **tomamos mini-lotes**, por lo que omitimos super-índices en los datos $(\\mathbf{x}_{train}^{(i:i+n)},\\mathbf{y}_{train}^{(i:i+n)})$ en todas las expresiones.\n",
    "\n",
    "un fragmento de código para este método puede verse como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mini_batch(theta, data_train, loss_func, epochs, batch_size):\n",
    "    for i in range (epochs):\n",
    "        np.random.shuffle (data)\n",
    "        for batch in get_batches(data , batch_size = batch_size):\n",
    "            x, y = batch\n",
    "            gradient = evaluate_gradient(loss_func,x, y, theta )\n",
    "            theta = theta - eta * gradient\n",
    "            eta *= eta\n",
    "    return theta, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño de los mini-lotes depende del problema y puede ser 32, 64, 128, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método vainilla del descenso de gradiente  no garantiza una buena convergencia, y ofrece algunos desafíos que deben abordarse:\n",
    "\n",
    "1. Elegir un ritmo de aprendizaje adecuado puede resultar complicado. Una tasa de aprendizaje demasiado pequeña conduce a una convergencia dolorosamente lenta, mientras que una tasa de aprendizaje demasiado grande puede dificultar la convergencia y hacer que la función de pérdida fluctúe alrededor del mínimo o incluso diverja.\n",
    "2. Los horarios de actualización de la tasa de aprendizaje intentan ajustar la tasa de aprendizaje durante la entrenamiento, es decir, reducir la tasa de aprendizaje de acuerdo con un programa predefinido o cuando el cambio función de pérdida entre epochs cae por debajo de un umbral. Sin embargo, estos horarios y umbrales deben definirse con anticipación y, por lo tanto, no pueden adaptarse a las características de un conjunto de datos.\n",
    "3. Además, la misma tasa de aprendizaje se aplica a todas las actualizaciones de parámetros. Si nuestros datos son escasos y nuestras características tienen frecuencias muy diferentes, es posible que no queramos actualizarlas todas en la misma medida, sino realizar una actualización más grande para las características que ocurren con poca frecuencia.\n",
    "4. Otro desafío clave al minimizar las funciones de error altamente no convexas comunes para las redes neuronales es evitar quedar atrapado en sus numerosos mínimos locales subóptimos. Algunos autores argumentan que, de hecho, la dificultad no surge de los mínimos locales sino de los puntos de silla, es decir, puntos donde una dimensión se inclina hacia arriba y otra hacia abajo. Estos puntos de silla suelen estar rodeados por una meseta del mismo error, lo que dificulta notablemente el escape de SGD, ya que el gradiente es cercano a cero en todas las dimensiones.\n",
    "\n",
    "Para una revisón contemporáneas de los algoritmos de optimización modernos puede consultar [An overview of gradient descent optimization\n",
    "algorithms](https://arxiv.org/pdf/1609.04747.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método del momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SGD tiene problemas para navegar por los barrancos, es decir, áreas donde la superficie se curva mucho más abruptamente en una dimensión que en otra, que son comunes en los óptimos locales. En estos escenarios, SGD oscila a lo largo de las pendientes del barranco mientras solo avanza vacilante por el fondo hacia el óptimo local.\n",
    "\n",
    " Este es un método que ayuda a acelerar SGD en la dirección relevante y amortigua oscilaciones. Lo hace sumando una fracción $\\lambda$ del vector de actualización del paso anterior al vector de actualización actual. \n",
    " \n",
    "El método  se esquematiza como sigue\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{v}_k &= \\lambda \\mathbf{v}_{k-1} +  \\eta \\nabla_{\\boldsymbol{\\theta}} \\mathfrak{L}({x}_{train}^{(i)},{y}_{train}^{(i)},\\boldsymbol{\\theta}_k)\\\\\n",
    "\\theta_{k+1} &= \\theta_{k} - v_k,\n",
    "\\end{align}\n",
    "$$\n",
    "$\\lambda<1$. Ussually, $\\lambda= 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/moment_method.png\" width=\"300\" height=\"200\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">SGD.Momento</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "[Imagen tomada de](https://www.google.com/search?q=momentum+method+gradient+descent&source=lnms&tbm=isch&sa=X&ved=2ahUKEwi22sKXn4DsAhVHk1kKHaYyB2wQ_AUoAXoECA4QAw&biw=1366&bih=540)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollado por Goeff Hinton, no publicado. Se basa en dividir la tasa de aprendizaje en cada caso por un promedio del cuadrado de las componentes del gradiente anteriore. Por cada componente $\\theta$ del vector de parámetros $\\boldsymbol{\\theta}$, sea $g$ la respectiva componente del gradiente asociada a $\\theta$, entonces el métodos es como sigue:\n",
    "\n",
    "1. $E[g^2]_t= \\lambda E[g^2]_{t-1} + (1-\\lambda)g_t^2$\n",
    "2. $\\theta_{t+1} = \\theta_t - \\tfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}}g_t$\n",
    "\n",
    "$\\epsilon >0$ es para evitar divisioens por cero. \n",
    "\n",
    "- $\\lambda$ es el parámetro de decaimiento. Típicamente $\\lambda = 0.9$.\n",
    "- $\\eta$ es la tasa de aprendizaje. Típicamente el valor por defecto es 0.001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo [Adam a method for Stochastic optimization](https://arxiv.org/pdf/1412.6980.pdf) de Kingma y Lei es actualmente el método más utilizado. El siguiente es el algoritmo.\n",
    "\n",
    "El símbolo  $g^2_t$ indica los elementos del producto de Hadamard (componente por componente)  $g_t\\bigodot g_t$. Según los autores, los mejores resultados han sido obtenidos para los valores de los hiperparámetros  $\\alpha = 0.001$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$ y $\\epsilon = 10−8$. Todas operaciones entre vectores son hechas componente por componente (producto de Hadamard). con $\\beta_1^t$ and $\\beta_2^t$ se denota la potencia $t$-ésima.\n",
    "1. Require: $\\alpha$: Stepsize\n",
    "2. Require: $\\beta_1^t$ and $\\beta_2^t \\in [0, 1)$. Exponential decay rates for the moment estimates\n",
    "3. Require: $f(\\theta)$: Stochastic objective function with parameters $\\theta$\n",
    "4. Require: $\\theta_0$: Initial parameter vector\n",
    "5.  $m_0  = 0$ (Initialize 1st moment vector)\n",
    "6. $v_0 =  0$ (Initialize 2nd moment vector)\n",
    "7. $t =  0$ (Initialize timestep)\n",
    "8. while $\\theta_t$ not converged do\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "t  &= t + 1 \\\\\n",
    "g_t &=  \\nabla f_t(\\theta_{t-1}) \\text{ (Get gradients w.r.t. stochastic objective at timestep t)}\\\\\n",
    "m_t  &= \\beta_1 m_{t−1} + (1 − \\beta_1) · g_t \\text{ (Update biased first raw moment estimate)}\\\\\n",
    "v_t  &= \\beta_2 v_{t−1} + (1 − \\beta_2) · g_t^2 \\text{ (Update second raw moment estimate)}\\\\\n",
    "\\hat{m}_t  &= \\frac{m_t}{1 − \\beta_1^t}   \\text{ (Compute bias-corrected first moment estimate)}\\\\ \n",
    "\\hat{v}_t  &=  \\frac{v_t}{1 − \\beta_2^t}  \\text{ (Compute bias-corrected second moment estimate)}\\\\ \n",
    "\\theta_t &=   \\theta_{t-1}  - \\alpha  \\frac{\\hat{m}_t}{\\hat{v}_t + epsilon} \\text{ (Update parameters)}\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "end while\n",
    "\n",
    "return $\\theta_t$ (Resulting parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
