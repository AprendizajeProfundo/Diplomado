{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diplomado en Inteligencia Artificial y Aprendizaje Profundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de clasificación para detección de fraude en tarjetas de crédito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Importa módulos](#Importa-módulos)\n",
    "* [Los datos](#Los-datos)\n",
    "* [Lee los datos](#Lee-los-datos)\n",
    "* [Funciones de activación](#Funciones-de-activación)\n",
    "* [Regularizadores ](#Regularizadores)\n",
    "* [El conjunto de datos cáncer de seno Wisconsin](#El-conjunto-de-datos-cáncer-de-seno-Wisconsin)\n",
    "* [Lectura de datos](#Lectura-de-datos)\n",
    "* [Preprocesamiento](#Preprocesamiento)\n",
    "* [Analiza el desbalance en los targets](#Analiza-el-desbalance-en-los-targets)\n",
    "* [Crea el modelo](#Crea-el-modelo)\n",
    "* [Define métricas y compila](#Define-métricas-y-compila)\n",
    "* [Entrena, tiene en cuenta los pesos de las transacciones](#Entrena-tiene-en-cuenta-los-pesos-de-las-transacciones)\n",
    "* [Predicciones](#Predicciones)\n",
    "* [Resultados-Calcula métricas](#Resultados-Calcula-métricas)\n",
    "* [Evaluación del modelo](#Evaluación-del-modelo)\n",
    "* [Conclusiones](#Conclusiones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección hacemos ejemplo de clasificación para detectar fraude en el uso de trajeta de crédito. El ejemplo es adaptado de  [fchollet](https://github.com/keras-team/keras-io/blob/master/examples/structured_data/imbalanced_classification.py). Los datos están disponible en [Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/).\n",
    "\n",
    "El propósito del ejemplo es ilustrar el caso de datos categóricos altamente desbalanceados. Un  problema muy común en la práctica. Usaremos varias métricas para evaluar el modelo.\n",
    "\n",
    "La técnica se puede extender a la detección de datos anómalos en grades conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#\n",
    "#import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "#\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "#\n",
    "from tensorflow.keras.layers import Dense, Input, Activation, Dropout\n",
    "#\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#\n",
    "from tensorflow.keras. metrics import FalseNegatives, FalsePositives, TrueNegatives\n",
    "from tensorflow.keras. metrics import TruePositives, Precision, Recall\n",
    "#\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#\n",
    "from tensorflow.keras import callbacks\n",
    "#\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excepto por la transacción y el monto, no sabemos cuáles son las otras columnas (por razones de privacidad). Lo único que sabemos es que esas columnas que se desconocen ya se han escalado.\n",
    "\n",
    "- *El monto de la transacción* es relativamente pequeño. La media de todas las monturas realizadas es de aproximadamente USD 88.\n",
    "\n",
    "- *No hay valores faltantes*\n",
    "\n",
    "- *La mayoría de las transacciones fueron no fraudulentas* (99,82%) del tiempo, mientras que las transacciones fraudulentas ocurren (018%) del tiempo en el marco de datos.\n",
    "\n",
    "- *Transformación PCA*: La descripción de los datos dice que todas las características pasaron por una transformación PCA (técnica de reducción de dimensionalidad) (excepto por tiempo y cantidad).\n",
    "\n",
    "- *Escalado*: tenga en cuenta que para implementar una transformación de PCA es necesario escalar previamente las características. (En este caso, todas las características de V se han escalado o al menos eso es lo que suponemos que hicieron las personas que desarrollan el conjunto de datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lee los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../Datos/fraude/creditcard.csv'\n",
    "training = pd.read_csv(fname)\n",
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construye tensores para entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa features y targest\n",
    "targets = training.pop('Class')\n",
    "targets = np.array(targets, dtype = 'uint8')\n",
    "targets.reshape((targets.shape[0],1))\n",
    "#\n",
    "features = np.array(training,dtype = 'float32' )\n",
    "#\n",
    "print(\"tamaño de los targets: \",targets.shape)\n",
    "print(\"tamaño de los features: \",features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepara datos de entrenamiento y de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea el objeto StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajusta los parámetros del scaler\n",
    "scaler.fit(train_features)\n",
    "\n",
    "# escala training y test\n",
    "train_features = scaler.transform(train_features)\n",
    "val_features = scaler.transform(val_features )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza el desbalance en los targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta las frecuencias de los datos\n",
    "counts = np.bincount(train_targets)\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Crea los pesos para el entrenamiento. Más peso a los menos frecuentes (1)\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(train_features.shape[1],),name='capa_entrada')\n",
    "\n",
    "# vamos construyendo capa por capa\n",
    "x = Activation('relu')(inputs)\n",
    "x = Dense(256, activation='relu',name='primera_capa_oculta')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu',name='segunda_capa_oculta')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(1, activation='sigmoid', name='capa_salida')(x)\n",
    "\n",
    "# Creamos ahora el modelo\n",
    "model = Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file='../Imagenes/fraude_model.png', \n",
    "           show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define métricas y compila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **accuracy_score**: En la clasificación con múltiples etiquetas, esta función calcula la precisión del subconjunto: el conjunto de etiquetas predichas para una muestra que coincide exactamente con el conjunto de etiquetas correspondiente en y_true.\n",
    "- **precision_score**: es la razón $\\frac{tp }{tp + fp}$ en donde $tp$ es el número de positivos verdadero y $fp$ el número de falsos positivos. El mejor valor es 1 y el peor valor es 0.\n",
    "- **recall_score**:  es la relación $\\frac{tp }{tp + fn}$ donde $tp$ es el número de verdaderos positivos y $fn$ el número de falsos negativos. El recuerdo es intuitivamente la capacidad del clasificador para encontrar todas las muestras positivas. El mejor valor es 1 y el peor valor es 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# métricas\n",
    "metrics = [\n",
    "    FalseNegatives(name=\"fn\"),\n",
    "    FalsePositives(name=\"fp\"),\n",
    "    TrueNegatives(name=\"tn\"),\n",
    "    TruePositives(name=\"tp\"),\n",
    "    Precision(name=\"precision\"),\n",
    "    Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "model.compile(optimizer=Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrena-tiene en cuenta los pesos de las transacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callbacks.ModelCheckpoint(\"../Datos/fraude/fraud_model_at_epoch_{epoch}.h5\")]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "history = model.fit(\n",
    "    train_features,\n",
    "    train_targets,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(val_features, val_targets),\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_val = model.predict(val_features)\n",
    "y_pred_val[y_pred_val > 0.5] = 1\n",
    "y_pred_val[y_pred_val <=0.5] = 0\n",
    "y_pred_val.reshape((y_pred_val.shape[0]))\n",
    "# predict the training set\n",
    "y_pred_train = model.predict(train_features)\n",
    "y_pred_train[y_pred_train > 0.5] = 1\n",
    "y_pred_train[y_pred_train <=0.5] = 0\n",
    "y_pred_train.reshape((y_pred_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados-Calcula métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# elimina dimensiones sobrantes\n",
    "val_targets = np.squeeze(val_targets)\n",
    "y_pred_val = np.squeeze(y_pred_val)\n",
    "train_targets = np.squeeze(train_targets)\n",
    "y_pred_train = np.squeeze(y_pred_train)\n",
    "#\n",
    "# falsos negativos validación\n",
    "fn_val = FalseNegatives()\n",
    "fn_val.update_state(val_targets, y_pred_val)\n",
    "fn_val = fn_val.result().numpy()\n",
    "#\n",
    "# falsos negativos entrenamiento\n",
    "fn_train = FalseNegatives()\n",
    "fn_train.update_state(train_targets, y_pred_train)\n",
    "fn_train = fn_train.result().numpy()\n",
    "# \n",
    "# falsos positivos validación\n",
    "fp_val = FalsePositives()\n",
    "fp_val.update_state(val_targets, y_pred_val)\n",
    "fp_val = fp_val.result().numpy()\n",
    "#\n",
    "# falsos positivos entrenamiento\n",
    "fp_train = FalsePositives()\n",
    "fp_train.update_state(train_targets, y_pred_train)\n",
    "fp_train = fp_train.result().numpy()\n",
    "# \n",
    "# Precision validación\n",
    "pre_val = Precision()\n",
    "pre_val.update_state(val_targets, y_pred_val)\n",
    "pre_val = pre_val.result().numpy()\n",
    "#\n",
    "# falsos negativos entrenamiento\n",
    "pre_train = Precision()\n",
    "pre_train.update_state(train_targets, y_pred_train)\n",
    "pre_train = pre_train.result().numpy()\n",
    "# \n",
    "# recall validación\n",
    "re_val = Recall()\n",
    "re_val.update_state(val_targets, y_pred_val)\n",
    "re_val = re_val.result().numpy()\n",
    "#\n",
    "# recall entrenamiento\n",
    "re_train = Recall()\n",
    "re_train.update_state(train_targets, y_pred_train)\n",
    "re_train = re_train.result().numpy()\n",
    "# \n",
    "\n",
    "# diccionario\n",
    "metricas = {'Falsos_positivos_train':fp_train, 'Falsos_positivos_val':fp_val,\n",
    "           '%Falsos_positivos': np.round((fp_train+fp_val)/ len(training)*100,4),\n",
    "           'Falsos_negativos_train':fn_train, 'Falsos_negativos_val':fn_val,\n",
    "           '%Falsos_negativos': np.round((fn_train+fn_val)/ len(training)*100,4),\n",
    "           'Precision_train': pre_train, 'Precision_val': pre_val,\n",
    "           'Recall_train': re_train, 'Recall_val':re_val }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Falsos_positivos_train': 5318.0,\n",
       " 'Falsos_positivos_val': 545.0,\n",
       " '%Falsos_positivos': 2.0586,\n",
       " 'Falsos_negativos_train': 0.0,\n",
       " 'Falsos_negativos_val': 11.0,\n",
       " '%Falsos_negativos': 0.0039,\n",
       " 'Precision_train': 0.07271142,\n",
       " 'Precision_val': 0.10509031,\n",
       " 'Recall_train': 1.0,\n",
       " 'Recall_val': 0.85333335}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Entrenamiento y validación '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final del entrenamiento, de 56,961 transacciones de validación, se obtuvo:\n",
    "\n",
    "- Identificar correctamente a 66 de ellos como fraudulentos\n",
    "- Faltan 9 transacciones fraudulentas\n",
    "- A costa de marcar incorrectamente 441 transacciones legítimas\n",
    "\n",
    "En el mundo real, se le daría un peso aún mayor a la clase 1,\n",
    "para reflejar que los falsos negativos son más costosos que los falsos positivos.\n",
    "\n",
    "**La próxima vez que su tarjeta de crédito sea rechazada en una compra en línea, esta es la razón.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
