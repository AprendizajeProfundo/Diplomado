{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema del bandido multibrazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El problema del bandido multibrazo](#El-problema-del-bandido-multibrazo)\n",
    "* [Política ε-voraz](#Política-ε-voraz)\n",
    "* [Implementación](#Implementación)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adaptado de Markel Sanz, [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-1-el-problema-del-bandido-multibrazo-afe05c0c372e)\n",
    "2. Sutton, R. S., & Barto, A. G. (2018). [Reinforcement learning: An introduction. MIT press](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje reforzado (Reinforcement Learning).\n",
    "\n",
    "Ésta es una técnica de inteligencia artificial (IA) en la que un agente inteligente (**agent**) tiene que interactuar con un entorno (**environment**), escogiendo una de las acciones (**action**) que el entorno ofrece en cada uno de los posibles estados (**state**), e intentar conseguir la mayor recompensa (**reward**) posible a través de esas acciones.\n",
    "\n",
    "\n",
    "Al principio, el agente no conoce nada sobre el entorno, por lo que tomará acciones de forma aleatoria. \n",
    "\n",
    "Si una acción trae una recompensa positiva, el agente deberá aprender a escoger esa acción más frecuentemente, mientras que si una acción trae una recompensa negativa, el agente deberá aprender a escoger esa acción menos frecuentemente. \n",
    "\n",
    "Así, el *agente aprenderá a escoger las acciones que maximicen la suma de recompensas recibidas*, también conocida como el retorno (**return**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema del bandido multibrazo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandit problem.\n",
    "\n",
    "El problema del bandido multibrazo se puede ver como un problema de máquinas tragamonedas en un casino. \n",
    "\n",
    "Si tenemos un número $N$  de tragamonedas una nos da una recompensa positiva con probabilidad $p$, y ninguna recompensa con probabilidad $(1-p)$, ¿podemos crear un agente que maximice las recompensas escogiendo jugar siempre en la tragamonedas que más beneficio nos vaya a proporcionar? \n",
    "\n",
    "\n",
    "Pues la idea es la misma; tenemos un bandido con $N$ brazos, y cada brazo tiene una probabilidad distinta de darnos una recompensa positiva. El objetivo es crear un agente que maximice esas recompensas.\n",
    "\n",
    "\n",
    "Para esta lección , usaremos un bandido de $N=5$ brazos (5-armed bandit). \n",
    "\n",
    "Éstas serán las probabilidades de cada brazo de dar una recompensa positiva: $[0.1, 0.3, 0.05, 0.55, 0.4]$. \n",
    "\n",
    "\n",
    "Como podemos ver, la mejor acción entre estas cinco es tirar del cuarto brazo. Sin embargo, el agente no dispone de esta información. \n",
    "\n",
    "Por lo tanto, deberá probar a tirar de todos los brazos varias veces, e ir aprendiendo cuál de todos es el mejor. Cuando vaya acumulando más información, empezará a tomar mejores decisiones, y a recibir mejores recompensas más frecuentemente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Tragamonedas.jpeg\" width=\"200\" height=\"200\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-1-el-problema-del-bandido-multibrazo-afe05c0c372e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Política ε-voraz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ε-greedy policy**\n",
    "\n",
    "Ésta será la política (*policy*) que decidirá qué acciones toma nuestro agente. La política es una función de probabilidad asociada a la siguiente acción que tomará el agente dado que se encuentra en el estado actual $s$.\n",
    "\n",
    "La **política ε-voraz** consiste en que el agente casi siempre tomará la mejor acción posible dada la información que posee. \n",
    "\n",
    "Sin embargo, de vez en cuando, con una **probabilidad de ε, el agente tomará una acción completamente al azar**. \n",
    "\n",
    "De esta forma, si tras la primera acción el agente ha obtenido una recompensa positiva, no se quedará atascado escogiendo esa misma acción todo el rato.\n",
    "\n",
    "Con probabilidad ε el agente explorará otras opciones.\n",
    "\n",
    "Este valor ε lo decidiremos nosotros, y será la forma que tengamos de equilibrar el problema de exploración y explotación (exploration vs. exploitation). \n",
    "\n",
    "La **exploración** consiste en explorar todas las acciones posibles varias veces para ver cuál es la mejor, a pesar de que durante esa exploración no obtengamos recompensas muy buenas. \n",
    "\n",
    "La **explotación** consiste en maximizar las recompensas, por lo que el agente escogerá la mejor de las acciones cada vez. \n",
    "\n",
    "Por ello, es importante **equilibrar la exploración y la explotación**: si solo exploramos dos de las cinco acciones posibles, no sabremos si las acciones que nunca hemos probado nos traerán mayores recompensas, por lo que la exploración es necesaria; y sin embargo, si nos pasamos todo el rato explorando todas las opciones una y otra vez, nunca utilizaremos ese conocimiento para poder escoger la mejor acción y conseguir la mayor recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un agente que resuelva el problema del bandido multibrazo. \n",
    "\n",
    "Empecemos con el código. Vamos a escribir una función que nos permita escoger uno de los cinco brazos, y nos devuelva una recompensa, dependiendo de la probabilidad de dicho brazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pull_bandit_arm(bandits, bandit_number):\n",
    "  \"\"\"Pull arm in position bandit_number and return the obtained reward.\"\"\"\n",
    "  result = np.random.uniform()\n",
    "  return int(result <= bandits[bandit_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, crearemos la función que decide qué acción debe tomar el agente. Con probabilidad *epsilon* tomará una acción aleatoria; y si no, tomará la acción con mejor media de recompensas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_epsilon_greedy_action(epsilon, average_rewards):\n",
    "  \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "  result = np.random.uniform()\n",
    "  if result < epsilon:\n",
    "    return np.random.randint(0, len(average_rewards)) # Random action\n",
    "  else:\n",
    "    return np.argmax(average_rewards) # Greedy action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, definamos las probabilidades de los brazos como hemos mencionado anteriormente, y definamos los parámetros epsilon y la cantidad de iteraciones o acciones que vamos a tomar. \n",
    "\n",
    "Definamos también tres listas donde guardaremos información sobre las acciones ejecutadas hasta este momento y las recompensas conseguidas para cada brazo.\n",
    "\n",
    "\n",
    "Ejecutamos el algoritmo llamando a las funciones, y guardamos las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for bandits in iteration 0 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 100 is ['0.14', '0.23', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 200 is ['0.15', '0.27', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 300 is ['0.18', '0.29', '0.00', '0.25', '0.27']\n",
      "Average reward for bandits in iteration 400 is ['0.17', '0.29', '0.12', '0.53', '0.29']\n",
      "Average reward for bandits in iteration 500 is ['0.16', '0.29', '0.10', '0.58', '0.32']\n",
      "Average reward for bandits in iteration 600 is ['0.15', '0.29', '0.07', '0.55', '0.30']\n",
      "Average reward for bandits in iteration 700 is ['0.14', '0.29', '0.07', '0.55', '0.26']\n",
      "Average reward for bandits in iteration 800 is ['0.14', '0.29', '0.06', '0.57', '0.28']\n",
      "Average reward for bandits in iteration 900 is ['0.14', '0.29', '0.05', '0.57', '0.27']\n",
      "Average reward for bandits in iteration 1000 is ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
      "\n",
      "Best bandit is 3 with an average observed reward of 0.5588\n",
      "Total observed reward in the 1000 episodes has been 436\n"
     ]
    }
   ],
   "source": [
    "# Probability of success of each bandit\n",
    "bandits = [0.1, 0.3, 0.05, 0.55, 0.4]\n",
    "num_iterations = 1000\n",
    "epsilon = 0.1\n",
    "\n",
    "# seed for reproductibility\n",
    "np.random.seed(600)\n",
    "\n",
    "# Store info to know which one is the best action in each moment\n",
    "total_rewards = [0 for _ in range(len(bandits))]\n",
    "total_attempts = [0 for _ in range(len(bandits))]\n",
    "average_rewards = [0.0 for _ in range(len(bandits))]\n",
    "\n",
    "for iteration in range(num_iterations+1):\n",
    "    action = take_epsilon_greedy_action(epsilon, average_rewards)\n",
    "    reward = pull_bandit_arm(bandits, action)\n",
    "  \n",
    "    # Store result\n",
    "    total_rewards[action] += reward\n",
    "    total_attempts[action] += 1\n",
    "    average_rewards[action] = total_rewards[action] / float(total_attempts[action])\n",
    "  \n",
    "    if iteration % 100 == 0:\n",
    "        print('Average reward for bandits in iteration {} is {}'.format(iteration,\n",
    "                                  ['{:.2f}'.format(elem) for elem in average_rewards]))\n",
    "\n",
    "# Print results\n",
    "best_bandit = np.argmax(average_rewards)\n",
    "print('\\nBest bandit is {} with an average observed reward of {:.4f}'\n",
    "      .format(best_bandit, average_rewards[best_bandit]))\n",
    "print('Total observed reward in the {} episodes has been {}'\n",
    "      .format(num_iterations, sum(total_rewards)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final, vemos que el algoritmo escribe esto:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Average reward for bandits is ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
    "Best bandit is 3 with an average observed reward of 0.5588\n",
    "Total observed reward in the 1000 episodes has been 436\n",
    "\n",
    "Recuerde que los parámetros reales son:\n",
    "\n",
    "bandits = [0.1, 0.3, 0.05, 0.55, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos la lista con la media de recompensas producidas por cada brazo, la cual se parece bastante a las probabilidades que hemos definido para cada uno de ellos. \n",
    "\n",
    "Es decir, con 1000 iteraciones hemos encontrado la probabilidad de éxito de cada brazo, y ahora sabemos que el cuarto brazo es el mejor (índice 3 en la lista). \n",
    "\n",
    "En este proceso, hemos obtenido 436 recompensas positivas. Es un número muy alto, teniendo en cuenta que escogiendo el mejor brazo desde el principio hubiéramos obtenido 550 recompensas positivas (porque tiene una probabilidad de 0.55). Podemos cambiar el valor de epsilon y la cantidad de iteraciones, para ver cómo estos valores afectan a la recompensa total recibida.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
