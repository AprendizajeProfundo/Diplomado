{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning con redes neuronales, algoritmo Double DQN y Dueling DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Algortimos basados en el modelo](#Algortimos-basados-en-el-modelo)\n",
    "* [Algortimos basados en la política](#Algortimos-basados-en-la-política)\n",
    "* [Double DQN](#Double-DQN)\n",
    "* [Dueling DQN](#Dueling-DQN)\n",
    "* [El código](#El-código)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adaptado de Markel Sanz, [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)\n",
    "2. Sutton, R. S., & Barto, A. G. (2018). [Reinforcement learning: An introduction. MIT press](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "3.  Van Hasselt, Hado, Arthur Guez, and David Silver. [Deep reinforcement learning with double q-learning.](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389/11847) Thirtieth AAAI conference on artificial intelligence. 2016.\n",
    "4.  Wang, Ziyu, et al. [Dueling network architectures for deep reinforcement learning](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección DQN-Learning hemos visto cómo funciona el algoritmo DQN, y cómo éste puede aprender a solucionar problemas complejos.\n",
    "\n",
    "En esta lección veremos dos nuevos algoritmos que suponen mejoras respecto a DQN, son *Double DQN* y *Dueling DQN*. Pero antes, introduzcamos algunos términos que hemos pasado por alto.\n",
    "\n",
    "\n",
    "Los algoritmos de aprendizaje reforzado se pueden clasificar en varias familias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algortimos basados en el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera de estas familias depende de si el algoritmo aprende cómo funciona el entorno de manera explícita o no. \n",
    "\n",
    "Si el algoritmo utiliza la dinámica del entorno (también conocido como modelo) durante la toma de decisiones, entonces el algoritmo es **basado en el modelo** (model based), y si no lo hace será **libre de modelo** (model free). Un algoritmo basado en el modelo tiene que aprender (o tener acceso a) todas las probabilidades de transición de un estado a otro. \n",
    "\n",
    "Como muchos entornos son estocásticos (probabilísticos) y sus dinámicas desconocidas, el algoritmo debe aprender el modelo detrás de estas transiciones probabilísticas. \n",
    "\n",
    "Una vez aprendidas, utilizará esa información para tomar mejores decisiones. Por ejemplo, si tomar la acción 1 te llevará al estado A con probabilidad 0.9 y recibirás una recompensa de -10, o  llevará al estado B con una probabilidad de 0.1 y recibirás recompensa de +10; pero tomar la acción 2 te llevará al estado C con probabilidad de 1.0 y obtendrás una recompensa de +5, la mejor decisión es tomar la acción 2, ya que aunque la posible recompensa del estado B es muy grande, la probabilidad de terminar en ese estado es muy baja. \n",
    "\n",
    "Por lo tanto, no solo es importante usar las recompensas en la toma de decisiones, sino también el modelo. En el futuro hablaremos más de los algoritmos basados en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos basados en la política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda familia en la que se pueden clasificar los algoritmos son **fuera-de-la-política** (off policy) y **dentro-de-la-política** (on-policy).\n",
    "\n",
    "Los algoritmos fuera-de-la-política aprenden la función de valor (value function), sin importar qué acciones tome el agente. \n",
    "\n",
    "Es decir, que la política de comportamiento (behavior policy) y la política objetivo (target policy) pueden ser distintas. \n",
    "\n",
    "La primera es la que utiliza el agente para explorar el entorno y recoger datos, mientras que la segunda es la que el agente intenta aprender y mejorar. \n",
    "\n",
    "Ésto significa que el agente puede explorar el entorno de forma completamente aleatoria con la política de comportamiento, y usar esos datos para aprender una política objetivo que sea capaz de obtener una recompensa muy alta en el futuro. En los algoritmos dentro-de-la-política, la política de comportamiento y la objetivo deben ser las mismas. \n",
    "\n",
    "Los algoritmos aprenden de los datos que deben recibir tras seguir la misma política que están aprendiendo.\n",
    "\n",
    "A partir de ahora, clasificaremos los algoritmos en estas dos familias. \n",
    "\n",
    "Por ejemplo, tanto el algoritmo *Q-Learning* como *DQN* son algoritmos libre de modelo y *fuera-de-la-política*. \n",
    "\n",
    "Los dos algoritmos que veremos en esta parte, *Double DQN* y *Dueling DQN*, también son algoritmos libre de modelo y fuera-de-la-política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con el algoritmo DQN es que **sobreestima las recompensas reales**; es decir, *los valores-Q*\n",
    "que aprende piensan que van a obtener una recompensa mayor de lo que en realidad obtendrá. Para solucionarlo, los autores del algoritmo **Double DQN [1]** proponen un sencillo truco: separar la selección y evaluación de la acción en dos pasos diferentes. En lugar de usar la ecuación de Bellman del algoritmo DQN, este algoritmo la cambia y se convierte en:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a; \\theta) = r + \\lambda Q(\\arg \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta);\\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero la red neuronal principal $\\theta$ decide cuál es la mejor acción entre todas las posibles, y luego la red objetivo evalúa esa acción para conocer su *valor-Q*. \n",
    "\n",
    "Este simple cambio ha demostrado **reducir las sobreestimaciones** y resultar en mejores políticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo divide los *valores-Q* en dos partes distintas, la función de valor (value function) $V(s)$ y la función de ventaja (advantage function) $A(s, a)$.\n",
    "\n",
    "La función de valor $V(s)$ nos dice cuánta recompensa obtendremos desde el estado *s*. La función de ventaja A$(s, a)$ nos dice cuánto mejor es una acción respecto a las demás. Combinando el valor *V* y la ventaja *A* de cada acción, obtenemos los *valores-Q*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) + A(s,a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que propone el **algoritmo Dueling DQN es que la misma red neuronal divida su capa final en dos, una para estimar el valor del estado** *s* $(V(s))$ y otra para estimar la ventaja de cada acción *a* ($A(s, a)$), y al final juntar ambas partes en una sola, la cual estimará los *valores-Q*. \n",
    "\n",
    "Este cambio ayuda en algunos casos, porque a veces no es necesario saber exactamente al valor de cada acción, por lo que aprender el valor del estado puede ser suficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/red_Dueling.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Red neronal asociada al algortimo Dueling DNQ</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-4-double-dqn-y-dueling-dqn-b24ac0a5a46c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, entrenar la red neuronal de esta simple manera, sumando el valor y la ventaja en la capa final, no es posible. \n",
    "\n",
    "Si se tiene $Q=V+A$, dada la función $Q$, no podemos determinar $V$ y $A$, es decir, el problema es **no-identificable**. \n",
    "\n",
    "Para solucionarlo, el artículo propone un truco: forzar al *valor-Q* más alto a ser igual al valor *V*, haciendo que el valor más alto en la función de ventaja sea cero, y los demás valores sean negativos. \n",
    "\n",
    "Esto  dirá exactamente cuál es el valor de $V$, y se pueden calcular los valores de la función de ventaja desde ahí, solucionando el problema. Así es como se entrenaría el algoritmo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) +( A(s,a)- \\max_{a'\\in|A|} A(s,a)) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, el artículo sugiere un pequeño cambio a estas ecuaciones. En lugar de usar el máximo, usaremos la media de las ventajas, así que eso será lo que hagamos (vea el artículo para más detalles). Así es como entrenaremos el algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) +( A(s,a)- \\frac{1}{|A|} \\sum_{a'} A(s,a)) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solucionaremos uno del los juegos de la saga Atari 2600 usando OpenAI Gym. \n",
    "\n",
    "El juego que he seleccionado es el de Pong, ya que es fácil de visualizar y entender, y es uno de los juegos más sencillos de solventar con aprendizaje por refuerzo profundo. \n",
    "\n",
    "Usaremos el código de la clase *DQN learning* como referencia. Pero esta vez cambiaremos la arquitectura de la red neuronal para dividir la capa final en dos, *las funciones de valor y de ventaja* (*V* y *A* en el código), y luego combinarlas para formar los *valores-Q*. \n",
    "\n",
    "También cambiaremos los tipos de capas de la red neuronal. Como el agente aprenderá directamente de los pixeles, las capas densas (dense o fully connected) no son la mejor opción. \n",
    "\n",
    "Usaremos capas convolucionales. El número de unidades en cada capa y los parámetros será el mismo que en el artículo *Dueling DQN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atari.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo del juego Atari</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(tf.keras.Model):\n",
    "  \"\"\"Convolutional neural network for the Atari games.\"\"\"\n",
    "  def __init__(self, num_actions):\n",
    "    super(DuelingDQN, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=8, strides=4, activation=\"relu\",\n",
    "    )\n",
    "    self.conv2 = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=4, strides=2, activation=\"relu\",\n",
    "    )\n",
    "    self.conv3 = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=1, activation=\"relu\",\n",
    "    )\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=512, activation=\"relu\")\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    self.A = tf.kears.layers.Dense(num_actions)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, states):\n",
    "    \"\"\"Forward pass of the neural network with some inputs.\"\"\"\n",
    "    x = self.conv1(states)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    V = self.V(x)\n",
    "    A = self.A(x)\n",
    "    Q = V + tf.subtract(A, tf.reduce_mean(A, axis=1, keepdims=True))\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, cambiaremos la función que ejecuta un paso de entrenamiento usando el descenso de gradiente. La modificaremos para implementar el paso de entrenamiento del algoritmo *Double DQN* en lugar del *DQN normal*. \n",
    "\n",
    "Esto significa que la ecuación de Bellman será la descrita anteriormente en este artículo, que es ligeramente diferente a la descrita en la lección *DQN-Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data.\"\"\"\n",
    "    # Select best next action using main_nn.\n",
    "    next_qs_main = main_nn(next_states)\n",
    "    next_qs_argmax = tf.argmax(next_qs_main, axis=-1)\n",
    "    next_action_mask = tf.one_hot(next_qs_argmax, num_actions)\n",
    "  \n",
    "    # Evaluate that best action using target_nn to know its Q-value.\n",
    "    next_qs_target = target_nn(next_states)\n",
    "    masked_next_qs = tf.reduce_sum(next_action_mask * next_qs_target, axis=-1)\n",
    "  \n",
    "    # Create target using the reward and the discounted next Q-value.\n",
    "    target = rewards + (1. - dones) * discount * masked_next_qs\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Q-values for the current state.\n",
    "        qs = main_nn(states)\n",
    "        action_mask = tf.one_hot(actions, num_actions)\n",
    "        masked_qs = tf.reduce_sum(action_mask * qs, axis=-1)\n",
    "        loss = loss_fn(target, masked_qs)\n",
    "    \n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutaremos el bucle principal de entrenamiento de la misma forma que en las partes anteriories, recogiendo datos y guardándolos en el buffer para usarlos más tarde. \n",
    "\n",
    "Tras entrenar el algoritmo durante 1000 episodios, esto será lo que escriba. El retorno indica cuantos goles ha marcado cada jugador."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
