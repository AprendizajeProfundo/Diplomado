{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Modelo del bandido multibrazo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Conceptos básicos de aprendizaje reforzado</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El problema del bandido multibrazo](#El-problema-del-bandido-multibrazo)\n",
    "* [Política ε-voraz](#Política-ε-voraz)\n",
    "* [Implementación](#Implementación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizaje reforzado (Reinforcement Learning).\n",
    "\n",
    "Ésta es una técnica de inteligencia artificial (IA) en la que un agente inteligente (**agent**) tiene que interactuar con un entorno (**environment**), escogiendo una de las acciones (**action**) que el entorno ofrece en cada uno de los posibles estados (**state**), e intentar conseguir la mayor recompensa (**reward**) posible a través de esas acciones.\n",
    "\n",
    "\n",
    "Al principio, el agente no conoce nada sobre el entorno, por lo que tomará acciones de forma aleatoria. \n",
    "\n",
    "Si una acción trae una recompensa positiva, el agente deberá aprender a escoger esa acción más frecuentemente, mientras que si una acción trae una recompensa negativa, el agente deberá aprender a escoger esa acción menos frecuentemente. \n",
    "\n",
    "Así, el *agente aprenderá a escoger las acciones que maximicen la suma de recompensas recibidas*, también conocida como el retorno (**return**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El problema del bandido multibrazo </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandit problem.\n",
    "\n",
    "El problema del bandido multibrazo se puede ver como un problema de máquinas tragamonedas en un casino. \n",
    "\n",
    "Si tenemos un número $N$  de tragamonedas una nos da una recompensa positiva con probabilidad $p$, y ninguna recompensa con probabilidad $(1-p)$, ¿podemos crear un agente que maximice las recompensas escogiendo jugar siempre en la tragamonedas que más beneficio nos vaya a proporcionar? \n",
    "\n",
    "\n",
    "Pues la idea es la misma; tenemos un bandido con $N$ brazos, y cada brazo tiene una probabilidad distinta de darnos una recompensa positiva. El objetivo es crear un agente que maximice esas recompensas.\n",
    "\n",
    "\n",
    "Para esta lección , usaremos un bandido de $N=5$ brazos (5-armed bandit). \n",
    "\n",
    "Éstas serán las probabilidades de cada brazo de dar una recompensa positiva: $[0.1, 0.3, 0.05, 0.55, 0.4]$. \n",
    "\n",
    "\n",
    "Como podemos ver, la mejor acción entre estas cinco es tirar del cuarto brazo. Sin embargo, el agente no dispone de esta información. \n",
    "\n",
    "Por lo tanto, deberá probar a tirar de todos los brazos varias veces, e ir aprendiendo cuál de todos es el mejor. Cuando vaya acumulando más información, empezará a tomar mejores decisiones, y a recibir mejores recompensas más frecuentemente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Tragamonedas.jpeg\" width=\"200\" height=\"200\" align=\"center\"/>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-1-el-problema-del-bandido-multibrazo-afe05c0c372e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Política ε-voraz  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ε-greedy policy**\n",
    "\n",
    "Ésta será la política (*policy*) que decidirá qué acciones toma nuestro agente. La política es una función de probabilidad asociada a la siguiente acción que tomará el agente dado que se encuentra en el estado actual $s$.\n",
    "\n",
    "La **política ε-voraz** consiste en que el agente casi siempre tomará la mejor acción posible dada la información que posee. \n",
    "\n",
    "Sin embargo, de vez en cuando, con una **probabilidad de ε, el agente tomará una acción completamente al azar**. \n",
    "\n",
    "De esta forma, si tras la primera acción el agente ha obtenido una recompensa positiva, no se quedará atascado escogiendo esa misma acción todo el rato.\n",
    "\n",
    "Con probabilidad ε el agente explorará otras opciones.\n",
    "\n",
    "Este valor ε lo decidiremos nosotros, y será la forma que tengamos de equilibrar el problema de exploración y explotación (exploration vs. exploitation). \n",
    "\n",
    "La **exploración** consiste en explorar todas las acciones posibles varias veces para ver cuál es la mejor, a pesar de que durante esa exploración no obtengamos recompensas muy buenas. \n",
    "\n",
    "La **explotación** consiste en maximizar las recompensas, por lo que el agente escogerá la mejor de las acciones cada vez. \n",
    "\n",
    "Por ello, es importante **equilibrar la exploración y la explotación**: si solo exploramos dos de las cinco acciones posibles, no sabremos si las acciones que nunca hemos probado nos traerán mayores recompensas, por lo que la exploración es necesaria; y sin embargo, si nos pasamos todo el rato explorando todas las opciones una y otra vez, nunca utilizaremos ese conocimiento para poder escoger la mejor acción y conseguir la mayor recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un agente que resuelva el problema del bandido multibrazo. \n",
    "\n",
    "Empecemos con el código. Vamos a escribir una función que nos permita escoger uno de los cinco brazos, y nos devuelva una recompensa, dependiendo de la probabilidad de dicho brazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pull_bandit_arm(bandits, bandit_number):\n",
    "    \"\"\"Pull arm in position bandit_number and return the obtained reward.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    # return a binary signal: (1 = reward; 0 = no reward)\n",
    "    # with probability bandits[bandit_number] obtain a reward.\n",
    "    return int(result <= bandits[bandit_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, crearemos la función que decide qué acción debe tomar el agente. Con probabilidad *epsilon* tomará una acción aleatoria; y si no, tomará la acción con mejor media de recompensas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_epsilon_greedy_action(epsilon, average_rewards):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    # exploration\n",
    "    if result < epsilon:\n",
    "        return np.random.randint(0, len(average_rewards)) # Random action\n",
    "    # explotation\n",
    "    else:\n",
    "        return np.argmax(average_rewards) # Greedy action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, definamos las probabilidades de los brazos como hemos mencionado anteriormente, y definamos los parámetros epsilon y la cantidad de iteraciones o acciones que vamos a tomar. \n",
    "\n",
    "Definamos también tres listas donde guardaremos información sobre las acciones ejecutadas hasta este momento y las recompensas conseguidas para cada brazo.\n",
    "\n",
    "\n",
    "Ejecutamos el algoritmo llamando a las funciones, y guardamos las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for bandits in iteration 0 is ['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 100 is ['0.14', '0.23', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 200 is ['0.15', '0.27', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 300 is ['0.18', '0.29', '0.00', '0.25', '0.27']\n",
      "Average reward for bandits in iteration 400 is ['0.17', '0.29', '0.12', '0.53', '0.29']\n",
      "Average reward for bandits in iteration 500 is ['0.16', '0.29', '0.10', '0.58', '0.32']\n",
      "Average reward for bandits in iteration 600 is ['0.15', '0.29', '0.07', '0.55', '0.30']\n",
      "Average reward for bandits in iteration 700 is ['0.14', '0.29', '0.07', '0.55', '0.26']\n",
      "Average reward for bandits in iteration 800 is ['0.14', '0.29', '0.06', '0.57', '0.28']\n",
      "Average reward for bandits in iteration 900 is ['0.14', '0.29', '0.05', '0.57', '0.27']\n",
      "Average reward for bandits in iteration 1000 is ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
      "\n",
      "Best bandit is 3 with an average observed reward of 0.5588\n",
      "Total observed reward in the 1000 episodes has been 436\n"
     ]
    }
   ],
   "source": [
    "# probbilidad de exito de cada bandido\n",
    "bandits = [0.1, 0.3, 0.05, 0.55, 0.4]\n",
    "num_iterations = 1000 # episodes\n",
    "epsilon = 0.1\n",
    "# cada episodio termina cuando el bandido empuja la palanca\n",
    "# y recibe una recompensa\n",
    "\n",
    "# seed para reproducibilidad\n",
    "np.random.seed(600)\n",
    "\n",
    "# SAlmacens información para sabe en cada momento que es mejor\n",
    "# Cuatro listas de tamaño len(bandits)\n",
    "total_rewards = [0 for _ in range(len(bandits))]\n",
    "total_attempts = [0 for _ in range(len(bandits))]\n",
    "average_rewards = [0.0 for _ in range(len(bandits))]\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # select an action: select an arm\n",
    "    action = take_epsilon_greedy_action(epsilon, average_rewards)\n",
    "    # inform the action to the environment and receive a reward\n",
    "    reward = pull_bandit_arm(bandits, action)\n",
    "  \n",
    "    # Store result\n",
    "    # acumulate the reward for this arm\n",
    "    total_rewards[action] += reward \n",
    "    # acumulate the attemps of this arm\n",
    "    total_attempts[action] += 1  # for this arm\n",
    "    # compute the current average reward \n",
    "    average_rewards[action] = total_rewards[action] / float(total_attempts[action])\n",
    "  \n",
    "    if iteration % 100 == 0:\n",
    "        print('Average reward for bandits in iteration {} is {}'.format(iteration+1,\n",
    "                                  ['{:.2f}'.format(elem) for elem in average_rewards]))\n",
    "\n",
    "# Print results\n",
    "best_bandit = np.argmax(average_rewards)\n",
    "print('\\nBest bandit is {} with an average observed reward of {:.4f}'\n",
    "      .format(best_bandit, average_rewards[best_bandit]))\n",
    "print('Total observed reward in the {} episodes has been {}'\n",
    "      .format(num_iterations, sum(total_rewards)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final, vemos que el algoritmo escribe esto:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Average reward for bandits is ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
    "Best bandit is 3 with an average observed reward of 0.5588\n",
    "Total observed reward in the 1000 episodes has been 436\n",
    "\n",
    "Recuerde que los parámetros reales son:\n",
    "\n",
    "bandits = [0.1, 0.3, 0.05, 0.55, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos la lista con la media de recompensas producidas por cada brazo, la cual se parece bastante a las probabilidades que hemos definido para cada uno de ellos. \n",
    "\n",
    "Es decir, con 1000 iteraciones hemos encontrado la probabilidad de éxito de cada brazo, y ahora sabemos que el cuarto brazo es el mejor (índice 3 en la lista). \n",
    "\n",
    "En este proceso, hemos obtenido 436 recompensas positivas. Es un número muy alto, teniendo en cuenta que escogiendo el mejor brazo desde el principio hubiéramos obtenido 550 recompensas positivas (porque tiene una probabilidad de 0.55). Podemos cambiar el valor de epsilon y la cantidad de iteraciones, para ver cómo estos valores afectan a la recompensa total recibida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación formal, usando clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, bandits, num_iterations):\n",
    "        self.bandid_prob = bandits\n",
    "        self.num_iterations = num_iterations\n",
    "        self.iteration = 0\n",
    "            \n",
    "    def reset(self):\n",
    "        self.iteration = 0\n",
    "        #return np.random.choice(len(bandits))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def set_bandits(self, bandits):\n",
    "        self.bandits = bandits\n",
    "        \n",
    "    def is_done(self):\n",
    "        return self.iteration >= self.num_iterations\n",
    "        \n",
    "    def step(self, arm):\n",
    "        \"\"\"Take the action , compute the reward  and\n",
    "        verify if is_done\"\"\"\n",
    "        result = np.random.uniform()\n",
    "        # return a binary signal: (1 = reward; 0 = no reward)\n",
    "        # with probability bandits[bandit_number] obtain a reward.\n",
    "        reward = int(result <= bandits[arm])\n",
    "        # increment itrations of the episode\n",
    "        self.iteration += 1\n",
    "        # return [next_obs, reward, is_done, info]\n",
    "        return [None, reward, self.is_done(), None]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.total_rewards = [0 for _ in range(len(bandits))]\n",
    "        self.total_attempts = [0 for _ in range(len(bandits))]\n",
    "        self.average_rewards = [0.0 for _ in range(len(bandits))]\n",
    "        \n",
    "        def set_epsilon(self, epsilon):\n",
    "            self.epsilon = epsilon\n",
    "    \n",
    "    def take_epsilon_greedy_action(self):\n",
    "        \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "        result = np.random.uniform()\n",
    "        # exploration\n",
    "        if result < self.epsilon:\n",
    "            return np.random.randint(0, len(self.average_rewards)) # Random action\n",
    "        # explotation\n",
    "        else:\n",
    "            return np.argmax(self.average_rewards) # Greedy action\n",
    "    \n",
    "    def update(self, reward, action):\n",
    "        # acumulate the reward for this arm\n",
    "        self.total_rewards[action] += reward \n",
    "        # acumulate the attemps of this arm\n",
    "        self.total_attempts[action] += 1  # for this arm\n",
    "        # compute the current average reward \n",
    "        self.average_rewards[action] = self.total_rewards[action] / float(self.total_attempts[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_rewards = [0 for _ in range(len(bandits))]\n",
    "        self.total_attempts = [0 for _ in range(len(bandits))]\n",
    "        self.average_rewards = [0.0 for _ in range(len(bandits))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, bandits, epsilon=0.1, num_iterations=1000):\n",
    "                \n",
    "           \n",
    "        self.agent = Agent(epsilon)\n",
    "        self.env = Env(bandits, num_iterations)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent.reset()\n",
    "        self.env.reset()\n",
    "\n",
    "    def play(self, verbose=True):\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            # select an action: select an arm\n",
    "            action = self.agent.take_epsilon_greedy_action()\n",
    "            # inform the action to the environment and receive a reward\n",
    "            # return [next_obs, reward, is_done, info]\n",
    "            _, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "            # Store result\n",
    "            # acumulate the reward for this arm\n",
    "            self.agent.update(reward, action)\n",
    "            \n",
    "            # print intermediate results\n",
    "            if verbose and (self.env.iteration % 100 == 0):\n",
    "                print('Average reward for bandits in iteration {} is {}'.format(self.env.iteration,\n",
    "                                      ['{:.2f}'.format(elem) for elem in self.agent.average_rewards]))\n",
    "\n",
    "    def print_results(self):\n",
    "        best_bandit = np.argmax(self.agent.average_rewards)\n",
    "        print('\\nBest bandit is {} with an average observed reward of {:.4f}'\n",
    "          .format(best_bandit, self.agent.average_rewards[best_bandit]))\n",
    "        print('Total observed reward in the {} episodes has been {}'\n",
    "          .format(self.env.iteration, sum(self.agent.total_rewards)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for bandits in iteration 100 is ['0.14', '0.24', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 200 is ['0.14', '0.27', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 300 is ['0.18', '0.29', '0.00', '0.25', '0.27']\n",
      "Average reward for bandits in iteration 400 is ['0.17', '0.29', '0.12', '0.52', '0.29']\n",
      "Average reward for bandits in iteration 500 is ['0.16', '0.29', '0.10', '0.58', '0.32']\n",
      "Average reward for bandits in iteration 600 is ['0.15', '0.29', '0.07', '0.55', '0.30']\n",
      "Average reward for bandits in iteration 700 is ['0.14', '0.29', '0.07', '0.55', '0.26']\n",
      "Average reward for bandits in iteration 800 is ['0.14', '0.29', '0.06', '0.57', '0.28']\n",
      "Average reward for bandits in iteration 900 is ['0.14', '0.29', '0.05', '0.57', '0.27']\n",
      "Average reward for bandits in iteration 1000 is ['0.13', '0.29', '0.05', '0.56', '0.26']\n",
      "\n",
      "Best bandit is 3 with an average observed reward of 0.5581\n",
      "Total observed reward in the 1000 episodes has been 435\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "bandits = [0.1, 0.3, 0.05, 0.55, 0.4]\n",
    "num_iterations = 1000 # episodes\n",
    "epsilon = 0.1\n",
    "# cada episodio termina cuando el bandido empuja la palanca\n",
    "# y recibe una recompensa\n",
    "\n",
    "# seed para reproducibilidad\n",
    "np.random.seed(600)\n",
    "\n",
    "# instance a game\n",
    "bandit = Bandit(bandits, epsilon, num_iterations)\n",
    "\n",
    "# play\n",
    "bandit.play()\n",
    "\n",
    "# print the final results\n",
    "bandit.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for bandits in iteration 100 is ['0.18', '0.35', '0.00', '0.00', '0.00']\n",
      "Average reward for bandits in iteration 200 is ['0.18', '0.27', '0.00', '0.53', '0.50']\n",
      "Average reward for bandits in iteration 300 is ['0.18', '0.28', '0.00', '0.56', '0.50']\n",
      "Average reward for bandits in iteration 400 is ['0.19', '0.29', '0.10', '0.57', '0.56']\n",
      "Average reward for bandits in iteration 500 is ['0.19', '0.30', '0.09', '0.58', '0.56']\n",
      "Average reward for bandits in iteration 600 is ['0.19', '0.29', '0.08', '0.59', '0.53']\n",
      "Average reward for bandits in iteration 700 is ['0.20', '0.30', '0.07', '0.57', '0.52']\n",
      "Average reward for bandits in iteration 800 is ['0.19', '0.29', '0.12', '0.57', '0.46']\n",
      "Average reward for bandits in iteration 900 is ['0.19', '0.29', '0.12', '0.57', '0.44']\n",
      "Average reward for bandits in iteration 1000 is ['0.19', '0.31', '0.12', '0.56', '0.39']\n",
      "\n",
      "Best bandit is 3 with an average observed reward of 0.5606\n",
      "Total observed reward in the 1000 episodes has been 497\n"
     ]
    }
   ],
   "source": [
    "bandit.reset()\n",
    "bandit.play()\n",
    "bandit.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
