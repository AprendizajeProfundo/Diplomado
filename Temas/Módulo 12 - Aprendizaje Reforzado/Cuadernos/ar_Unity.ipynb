{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR para videojuegos en Unity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Github ML Agents, [Machine Learning Agents Toolkit](https://github.com/Unity-Technologies/ml-agents)\n",
    "2. Github [ML Dungeon](https://github.com/TheJarmanitor/ML_Dungeon) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el desarrollo de la inteligencia artificial en los juegos de video comienza casi a la par con la misma creación de estos, pero no era una prioridad ya que los juegos eran inicialmente para 2 personas. En lo que aparecían juegos diseñados para un jugador, el interés por la IA incrementó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<right>\n",
    "<img src=\"../Imagenes/Space_invaders.gif\" width=\"400\" height=\"300\" align=\"right\"/>\n",
    "</right>\n",
    "</figure>\n",
    "<figure>\n",
    "<left>\n",
    "<img src=\"../Imagenes/Pong.gif\" width=\"400\" height=\"300\" align=\"left\"/>\n",
    "</left>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en sus inicio y  hasta hace relativamente poco, el termino \"Game AI\" se ha mantenido distante de lo que se ha estudiado como verdadera inteligencia artificial, ya que sus métodos solían verse como \"un montón de if else\" sin enfocarse en la toma de decisiones que viene con métodos de Machine Learning. Esto por supuesto ha cambiado a medida que los desarrolladores de videojuegos se comienzan a interesar por la parte academica de la inteligencia artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos tradicionales de Game AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arboles de Decisión\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Arboles_de_decision.jpg\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Hands-on Game AI Development](https://www.youtube.com/watch?v=bmP4ppe_-cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se piensan en posibles situaciones en las que el agente puede tener contacto y se programan directamente las acciones que hace este mismo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- el método más usado y más \"sencillo\" de implementar\n",
    "\n",
    "- Intuitivo\n",
    "\n",
    "- Simple, tal vez demasiado simple\n",
    "\n",
    "- Demasiado trabajo manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Máquinas de estado finito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/finite_state_machine.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [gamedevelopment.tutsplus.com](https://gamedevelopment.tutsplus.com/tutorials/finite-state-machines-theory-and-implementation--gamedev-11867)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se programa una serie de estados las cuales el agente puede tener si se cumple una condición. Solo se puede tener un estado de manera activa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Más flexible que arboles de decisión\n",
    "\n",
    "- Menos planeación Manual\n",
    "\n",
    "- Mayor esfuerzo a la hora de programar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos métodos tienen un par de problemas en común\n",
    "\n",
    "**Sensación durante el juego:** En su forma más básica, la implementación de estos algoritmos puede llevar a que la IA se sienta demasiado mecánica, lo cual puede romper la sensación de juego. Esto por supuesto, puede remediarse con algoritmos más complejos, pero esto genera mayor trabajo\n",
    "\n",
    "**Hard Coding:** la cantidad de trabajo que implican estos métodos ya bien sea en la planeación y durante la programación puede causar una enorme cantidad de trabajo para obtener resultados de calidad. Llegando a problemáticas como el crunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAMPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos casos, para dar la ilusión de un agente altamente competente, los desarrolladores activamente hacen trampa. Dando información que no debería poder obtener en un momento, o cambiando sus capacidades de manera espontanea para ajustarse a la situación. Esto, en ciertas situaciones, no es necesariamente malo, pero puede causar molestia o frustraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<left>\n",
    "<img src=\"../Imagenes/rubber_banding.gif\" width=\"400\" height=\"300\" align=\"left\"/>\n",
    "</left>\n",
    "</figure>\n",
    "<figure>\n",
    "<right>\n",
    "<img src=\"../Imagenes/cheating_ai.gif\" width=\"400\" height=\"300\" align=\"right\"/>\n",
    "</right>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El aprendizaje Reforzado como alternativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente el aprendizaje reforzado se usaba casi con exclusividad para desarrollar agentes capaces de jugar y superar niveles. Fue hasta hace poco que se extiende su uso a una implementación directa, pero de igual forma se hace más como una prueba de concepto, no se han visto muchos casos de productos con un uso verdadero de aprendizaje reforzado. Algunas ventajas y desventajas que se tienen al cambiar a AR en el diseño de juegos inteligentes serían los siguientes\n",
    "\n",
    "- **sensación de juego:** A veces no se necesita un agente perfecto, pero un agente creible. Utilizando AR es posible conseguir enemigos que si se sientan reales en lugar de máquinas.\n",
    "\n",
    "- **Soft Coding:** Con las herramientas de python y tensorflow, es posible desarrollar agentes inteligentes sin necesidad de programarlo en su totalidad, reduciendo el tiempo empleado.\n",
    "\n",
    "- **entrenamiento pasivo** Una gran parte del proceso de entrenamiento va a ser de manera pasiva, permitiendo poder enfocar energías en otras partes del proyecto.\n",
    "\n",
    "- **Barrera de entrada alta:** En el inicio  será un proceso complicado. No solo es necesario aprender sobre la implementación de AR, pero el poder desarrollar un videojuego como producto, por más simple que sea. Por supuesto esta desventaja no es permanente.\n",
    "\n",
    "- **Poca cantidad de información:** Ya que este método y sus herramientas son algo relativamente nuevo, no hay mucha gente haciendo este tipo de proyectos, por lo cual conseguir información específica puede ser complicado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a Unity y ML Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity\n",
    "\n",
    "Unity es una compañía y un motor de juego creado en 2005 con la intención de brindar herramientas de creación de videojuegos a una mayor cantidad de personas. Utiliza C# como su lenguaje de programación, lo cual hace que sea \"menos complicado\" al comenzar a trabajar. Tiene capacidad para gráficos en 2 y 3 dimensiones y adiciones en realidad virtual y realidad aumentada. Viene también con sus propias herramientas de Maquinas de estado finito y pathfinding. Actualmente está entre los motores más utilizados en muchos sectores de la industra del entretenimiento, no solo los videojuegos. \n",
    "\n",
    "### Machine Learning Agents Toolkit\n",
    "\n",
    "ML Agents es un proyecto libre desarrollado por el mismo Unity, basado en python y tensorflow. Permite la implementación de redes neuronales y aprendizaje Reforzado en proyectos hechos con el motor de juego. Utiliza archivos YAML, lo que hace más sencillo el proceso de seleccionar hiperparamétros, pero es posible utilizar una API de python para construir las redes neuronales que se quieren utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML agents cuenta con una gran cantidad de métodos de entrenamiento, entre ellas\n",
    "\n",
    "- CNNs y uso de una o múltiples cámaras para obtener imágenes de observación\n",
    "\n",
    "- RNN/LSTM para crear agentes con memoria\n",
    "\n",
    "- GAILs(Aprendizaje por imitación Generativa Adversaria) Uso de demostraciones para replicar acciones junto con una red neuronal para discriminar entre demostración y acción de Agente\n",
    "\n",
    "- Curriculum Learning, aumentando la dificultad de los problemas a medidas que se continua al proceso de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<left>\n",
    "<img src=\"../Imagenes/RNN_Unity.gif\" width=\"400\" height=\"300\" align=\"left\"/>\n",
    "</left>\n",
    "</figure>\n",
    "<figure>\n",
    "<right>\n",
    "<img src=\"../Imagenes/GAIL_Unity.gif\" width=\"400\" height=\"300\" align=\"right\"/>\n",
    "</right>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/CNN_Unity.gif\" width=\"427\" height=\"307\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos Nativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Agents usa directamente dos algoritmos de aprendizaje reforzado: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proximal Policy Optimization:** Algoritmo On-Policy Desarrollado por OpenAI con la intención de ser lo más eficiente a nivel de  ajuste de hiperparametros y tamaño de muestra(Aunque todavía sufre un poco en este aspecto), tratando de mantenerse multi proposito. Es el algoritmo estándar dentro de ML Agents.\n",
    "\n",
    "Paper detallando el algoritmo, [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Actor Critic:** Algoritmo Off-Policy Desarrollado en la Universidad de Berkeley. Intenta resolver la debilidad de sensibilidad que tienen estos algoritmos al cambio de parametros, necesitando muchas menos muestras que PPO. Es recomendable utilizar SAC sobre PPO cuando se piensa en un ambiente donde se puede hacer episodios lentos\n",
    "\n",
    "Paper detallando el algoritmo, [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de tener estos algoritmos nativamente, es posible escribir algoritmos propios de Tensorflow a partir del API, pero sin el factor visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Dungeon  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Dungeon_train.gif\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proyecto diseñado con la intención de crear un proyecto de aprendizaje reforzado desde 0. Este tiene las siguientes metas:\n",
    "\n",
    "- Diseñar un terreno de Aprendizaje\n",
    "    - Diseñar uno o más terrenos para el agente haga su entrenamiento.\n",
    "    - Diseñar un agente Enemigo con un sistema de vida, control de ataque y movimiento.\n",
    "    - Diseñar un objetivo con control de vida y ataque.\n",
    "- El entrenamiento del agente Enemigo\n",
    "    - Lograr que el agente busque un objetivo y le haga daño\n",
    "    - Lograr que el agente huya del objetivo en caso de que se encuentre en baja vida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de cualquier intento de implementación de Aprendizaje Reforzado, es necesario trabajar en el juego base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Unity_Screenshot.png\" width=\"1600\" height=\"900\" align=\"center\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada Area de entrenamieno consta de 3 componentes\n",
    "\n",
    "- **Grid:** El campo de entrenamiento. Se usa para limitar el movimiento del Agente\n",
    "\n",
    "- **Target:** La representación del jugador. Tiene un sistema de vida y la capacidad de atacar cuando tiene baja vida\n",
    "\n",
    "- **Slime Agent:** El agente que va a ser entrenado. Tiene un sistema de vida, puede moverse y atacar un vez por determinado tiempo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<left>\n",
    "<img src=\"../Imagenes/Agent_Scripts.png\" width=\"300\" height=\"200\" align=\"left\"/>\n",
    "</left>\n",
    "    \n",
    "El script \"Slime Agent\" Reune el resto de código y hace el manejo del proceso de entrenamiento. Además es posible definir desde aquí el número de pasos por episodios\n",
    "\n",
    "    \n",
    "\n",
    "En Behavior Parameters hay que escribir el tamaño del vector de observaciones y de acciones, el tipo de acciones que se tiene, el hardware con el que se va a hacer el entrenamiento y el tipo de comportamiento del agente al inciar la escena\n",
    "\n",
    "Decision Requester define el cada cuantos pasos se hace una decisión. Esto puede apresurar o alentar el entrenamiento o hacerlo más pesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del código del Agente obtenemos los estados básicos de cada episodio\n",
    "\n",
    "- **Initialize:** Se llama únicamente al inicio de la escena/nivel. Usualmente para conectar otros scripts y componentes \n",
    ">\n",
    "- **Episode Begin:** Se llama al inicio de cada episodio. Lo usamos para reiniciar variables y objetos\n",
    "\n",
    "- **Collect Observations:** Recolecta y guarda los datos para la red neuronal\n",
    "\n",
    "- **Action Received:** Define las acciones que puede tomar el agente y reparte las recompensas a partir de dichas acciones\n",
    "\n",
    "- **Heuristic:** Permite controlar las acciones del agente por el usuario. Importante para debuggear y comprender verdaderamente las acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 1\n",
    "\n",
    "\n",
    "Observaciones recolectadas: Posición del agente y del objetivo. Vector de tamaño 6\n",
    "\n",
    "Acciones: movimiento en X Y, y ataque. Vector de tamaño 3\n",
    "\n",
    "Recompensas:\n",
    "- si ataca al objetivo cuando tiene vida mayor a 30: recompensa de 0.33\n",
    "- si ataca al objetivo cuando tiene vida menor o igual a 30: recompensa negativa(castigo) de -0.2\n",
    "\n",
    "Resultado: Al agente se mueve de poco a nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 2\n",
    "\n",
    "<figure>\n",
    "<right>\n",
    "<img src=\"../Imagenes/Intento2_.png\" width=\"250\" height=\"150\" align=\"right\"/>\n",
    "</right>\n",
    "\n",
    "Observaciones recolectadas: Posición del agente y del objetivo. Vector de tamaño 6\n",
    "\n",
    "Acciones: movimiento en X Y, y ataque. Vector de tamaño 3\n",
    "\n",
    "Recompensas:\n",
    "- Estado de peligro\n",
    "    - si se acerca al objetivo: Recompensa de 0.01\n",
    "    - si ataca al objetivo: Recompensa de 0.1\n",
    "    - si se aleja del objetivo: Castigo de -0.001\n",
    "- No Estado de peligro\n",
    "    - si se aleja del objetivo: Recompensa de 0.02\n",
    "    - si se acerca al objetivo: Castigo de -0.001\n",
    "    - si ataca: Castigo de -0.005\n",
    "Resultado: El agente se mueve un poco, pero termina por intentar alejarse lo más posible del objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto sucede ya que el trabajo del agente no es únicamente máximizar la recompensa, también minimizar el castigo. En el momento que encuentra un comportamiento con el que obtenga un bajo nivel de castigo se quedará con él"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompensa escasa y exploración por curiosidad\n",
    "\n",
    "Es aquí cuando nos encontramos con un Talón de Aquiles del aprendizaje Reforzado: El problema de recompensa Escasa.\n",
    "\n",
    "Dentro de varios problemas de AR, es posible que el agente logre obtener recompensas extrinsecas de manera Aleatoria, consiguiendo rastros de cómo debe interactuar con el ambiente. Esto es mucho más complicado cuando las recompensas se consiguen en lapsos de tiempo largos o por situaciones específicas.\n",
    "\n",
    "Esto puede arreglarse brindándole al agente la necesidad de explorar a partir de su curiosidad. Adicionando una segunda red neuronal que va a dar recompensas intrinsecas basadas en qué tan bien el agente hace. Generando la necesidad de explorar y guardar información.\n",
    "\n",
    "Paper Curiosidad, [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/resources/icml17.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 3\n",
    "\n",
    "<figure>\n",
    "<right>\n",
    "<img src=\"../Imagenes/Intento3_.png\" width=\"250\" height=\"150\" align=\"right\"/>\n",
    "</right>\n",
    "\n",
    "Observaciones recolectadas: Posición del agente y del objetivo. Vector de tamaño 6\n",
    "\n",
    "Acciones: movimiento en X Y, y ataque. Vector de tamaño 3\n",
    "    \n",
    "Extras: Uso de recompensas intrinsecas a partir de Curiosidad\n",
    "\n",
    "Recompensas:\n",
    "- Estado de peligro\n",
    "    - si se acerca al objetivo: Recompensa de 0.01\n",
    "    - si ataca al objetivo: Recompensa de 0.1\n",
    "    - si se aleja del objetivo: Castigo de -0.001\n",
    "- No Estado de peligro\n",
    "    - si se aleja del objetivo: Recompensa de 0.02\n",
    "    - si se acerca al objetivo: Castigo de -0.001\n",
    "    - si ataca: Castigo de -0.005\n",
    "\n",
    "Si el Agente o el objetivo muere, se termina el episodio \n",
    "    \n",
    "Resultado: El agente crea una \"Estrategia\". Comienza a dar vueltas dentro del ambiente para intentar atacar al objetivo. Es un avance, pero no lo que se quiere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento Final\n",
    "\n",
    "Observaciones recolectadas: Posición del agente, del objetivo y si está en peligro. Vector de tamaño 6\n",
    "\n",
    "Acciones: movimiento en X Y, y ataque. Vector de tamaño 3\n",
    "    \n",
    "Extras: Uso de recompensas intrinsecas a partir de Curiosidad\n",
    "\n",
    "Recompensas:\n",
    "- Estado de peligro\n",
    "    - si se acerca al objetivo: Recompensa de 0.01\n",
    "    - si ataca al objetivo: Recompensa de 0.1\n",
    "    - si se aleja del objetivo: Castigo de -0.001\n",
    "- No Estado de peligro\n",
    "    - si se aleja del objetivo: Recompensa de 0.02\n",
    "    - si se acerca al objetivo: Castigo de -0.001\n",
    "    - si ataca: Castigo de -0.005\n",
    "\n",
    "Si el Agente o el objetivo muere, o el agente se aleja demasiado, se termina el episodio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivo YAML"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "behaviors:\n",
    "    Slime:\n",
    "        trainer_type: ppo\n",
    "        hyperparameters:\n",
    "          batch_size: 128\n",
    "          buffer_size: 2048\n",
    "          learning_rate: 0.0003\n",
    "          beta: 0.01\n",
    "          epsilon: 0.2\n",
    "          lambd: 0.95\n",
    "          num_epoch: 4\n",
    "          learning_rate_schedule: linear\n",
    "        network_settings:\n",
    "          normalize: false\n",
    "          hidden_units: 128\n",
    "          num_layers: 2\n",
    "          vis_encode_type: simple\n",
    "        reward_signals:\n",
    "          extrinsic:\n",
    "            gamma: 0.99\n",
    "            strength: 1.0\n",
    "          curiosity:\n",
    "            gamma: 0.99\n",
    "            strength: 0.02\n",
    "            encoding_size: 256\n",
    "            learning_rate: 0.0003\n",
    "        keep_checkpoints: 5\n",
    "        max_steps: 1000000\n",
    "        time_horizon: 128\n",
    "        summary_freq: 50000\n",
    "        threaded: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Demostración](https://www.youtube.com/watch?v=I7xTxLGFerk&feature=youtu.be)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
