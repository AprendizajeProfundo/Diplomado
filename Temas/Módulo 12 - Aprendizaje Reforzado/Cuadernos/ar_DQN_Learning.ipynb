{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-Learning con redes neuronales, algoritmo DQN.</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Deep Q-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Q-Learning profundo](#Q-Learning-profundo)\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [Experiencia por repetición](#Experiencia-por-repetición)\n",
    "* [Congelando la red objetivo](#Congelando-la-red-objetivo)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Ejemplo ambiente CartPole-v1 ](#Ejemplo-ambiente-CartPole-v1-en-Gym)\n",
    "* [Implementación del algoritmo DQN. CartPole](#Implementación-del-algoritmo-Q-learning.-CartPole)\n",
    "* [Video luego de entrenado el agente](#Video-luego-de-entrenado-el-agente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del algoritmo Q-Learning, este  funciona muy bien cuando el entorno es simple y la función $Q(s,a)$ se puede representar como una tabla o matriz de valores. \n",
    "\n",
    "Pero cuando hay miles de millones de estados diferentes y cientos de acciones distintas, la tabla se vuelve enorme, y no es viable su utilización. Por ello se introdujo el Q-learning profundo (DQN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Q-Learning profundo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnih et al. introdujeron el algoritmo Deep Q-Network o DQN, publicado en Nature en 2015 como [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). Este algoritmo combina el algoritmo *Q-learning* con redes neuronales profundas (Deep Neural Networks). \n",
    "\n",
    "Como es sabido en el campo de la IA, las redes neuronales son una fantástica manera de aproximar funciones no lineales. Este algoritmo usa una red neuronal para aproximar la función *Q*, evitando así utilizar una tabla para representar la misma. \n",
    "\n",
    "En realidad, utiliza dos redes neuronales para estabilizar el proceso de aprendizaje. \n",
    "\n",
    "1. La primera, la *red neuronal principal* (main Neural Network), representada por los parámetros $\\theta$, se utiliza para estimar los valores-Q del estado s y acción a actuales: $Q(s, a;\\theta)$. \n",
    "2. La segunda, la *red neuronal objetivo* (target Neural Network), parametrizada por $\\theta^{'}$, tendrá la misma arquitectura que la red principal pero se usará para aproximar los *valores-Q* del siguiente estado $s'$ y la siguiente acción $a'$. \n",
    "\n",
    "El aprendizaje ocurre en la red principal y no en la objetivo.\n",
    "\n",
    "La red objetivo se congela (sus parámetros no se cambian) durante varias iteraciones, normalmente alrededor de 10000. Luego de transcurrido ese número de iteraciones, los parámetros de la red principal se copian a la red objetivo, transmitiendo así el aprendizaje de una a otra, haciendo que las estimaciones calculadas por la red objetivo sean más precisas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos requerido para entrenar la Q-network provienen de  la experiencia del agente: $(s_0 a_0 r_1 s_1, s_1 a_1 r_2  s_2,\\ldots, s_{T-1} a_{T-1} r_T s_T)$. Cada muestra de entrenamiento es una unidad de experiencia, $s_t a_t r_{t+1} s_{t+1}$.\n",
    "\n",
    "En el tiempo $t$ se tiene el estado $s=s_t$.  La acción $a=a_t$ es determinada usando el algoritmo Q-learning, en donde la selección de la acción es de tipo epsilon-greedy., es decir, la siguiente acción, digamos $\\pi(a)$ se obtiene como:\n",
    "\n",
    "$$\n",
    "\\pi(a) = \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ecuación de Bellman en DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación de Bellman en el caso DQN se expresa como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a; \\theta) = r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar una red neuronal, necesitamos una función de pérdida o coste (loss or cost function), la cual definimos como el cuadrado de la diferencia entre ambos lados de la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "L(\\theta)= \\mathbb{E}[  r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})- Q(s,a; \\theta)]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta será la función que minimizaremos usando el algoritmo de descenso de gradiente (gradient descent), el cuál se ejecuta automáticamente si usamos una librería de diferenciación automática con redes neuronales, como TensorFlow o Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el punto de vista del algoritmo,  se usa la $Q$-network para predecir el valor $Q$ de cada posible siguiente acción, dado el estado siguiente y escogiendo el máximo entre ellos, como henos hecho antes. \n",
    "\n",
    "En el caso en el cual el estado es terminal se tiene que $\\max_{a^{'}}  Q(s^{'},a^{'})=0$.\n",
    "\n",
    "La siguiente imagen ilustra el procedimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/Q-Network.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</figure> \n",
    "\n",
    "Fuente:  [Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Experiencia por repetición</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general el entrenamiento de la Q-network es inestable. Hay dos causas de la instabilidad\n",
    "\n",
    "1. Alta correlación entre las muestras.\n",
    "2. El target no es estacionario.\n",
    "\n",
    "Para resolver el problema de la alta correlación, los datos de entrenamiento son seleccionados aleatorios de un buffer que creamos para tal fín. Este proceso se conoce como experience replay (experiencia por repetición)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Congelando la red objetivo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL problema de no estacionariedad de la red objetivo (target) es resuelto congelando sus pesos durante $C$ pasos de  entrenamiento. En realidad, esta es la razón de tener dos redes con idéntica arquitectura. Los parámetros de la Q-network target son copiados desde la Q-network principal cada $C$ pasos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo DQN </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo DQN es de tipo epsilon-greedy. \n",
    "\n",
    "**Primero se crean los objetos necesarios y se inicializan.**\n",
    "\n",
    "- Inicializar la memory  replay (buffer) *D* con capacidad *N*.\n",
    "- Inicializar la función de acción-valor $Q_{main}$ con pesos aleatorios $\\theta$\n",
    "- Inicializar la función acción-valor target $Q_{\\text{target}}$ con pesos con pesos $\\theta^{-}= \\theta$\n",
    "- Definir la rata de exploración inicial $\\epsilon$ y el factor de descuento, $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ciclo de aprendizaje**\n",
    "\n",
    "1. Para $\\text{ episodio } = 1,\\ldots, M$, do:\n",
    "2.        Dado el estado incial s\n",
    "3.        Para step = 1,...,T haga:\n",
    "4.            Escoja la acción (epsilon-greddy)\n",
    "$$\n",
    "a \\gets \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$\n",
    "5. Ejecuta la acción *a* observe la recompensa *r* y el siguiente estado *s'*\n",
    "6. Almacene la transición $(s,a,r,s')$ en el buffer D\n",
    "7. Actualice el estado $s=s'$\n",
    "\n",
    "**Experiencia por repetición**\n",
    "\n",
    "8.Tome un mini-lote de muestra de experiencias de episodios $(s_j,a_j, r_{j+1}, s_{j+1})$ desde el buffer *D*.\n",
    "\n",
    "9. \n",
    "$$\n",
    "Q_{\\text{max}} = \\begin{cases} r_{j+1} &\\text{ si el episodio termina en } j+1\\\\\n",
    "r_{j+1} + \\gamma \\max_{a_{j+1}} Q_{\\text{target}}(s_{j+1},a_{j+1};\\theta ^{-}) &\\text{ en otro caso }\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "10. Ejecuta un paso de gradiente descendiente para la función de pédida $\\left(Q_{\\text{max}}- Q(s_j,a_j;\\theta) \\right)^2$ con respecto a los parámetros\n",
    "\n",
    "**Actualización periódica de la red objetivo(target)**\n",
    "\n",
    "11. Cada $C$ pasos, $Q_{\\text{target}}=Q_{main}$, es decir $\\theta^{-} = \\theta$\n",
    "12. fin\n",
    "13. fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo ambiente CartPole-v0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenemos el entorno (environment) CartPole. Se ha utilizado la librería *OpenAI Gym* para visualizar y ejecutar este entorno. En este entorno, el objetivo es mover el carro hacia la derecha y la izquierda, con el objetivo de equilibrar el palo. Recuerde que  si el palo se tuerce más de 15 grados respecto al eje vertical, el episodio terminará y volveremos a empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación del algoritmo DQN. CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección implementamos el Algortimo DQN. Usaremos el ambiente CartPole-v1, como ejemplo, aunque el código es bastante genérico. Se presenta la implementación en Tensorflow y se deja como ejercio la implementación en Pytorch.\n",
    "\n",
    "El código puede ser un poco lento en su máquina local. Si eso ocurre ejecútelo en Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga módulos requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import io\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Configura el ambiente CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de variables del estado: 4\n",
      "Número de posibles acciones: 2\n"
     ]
    }
   ],
   "source": [
    "# Carga el ambiente gym environment y obtien información del \n",
    "# espacio de acciones y del espacio de estados\n",
    "env = gym.make('CartPole-v1')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Número de variables del estado: {}'.format(num_features))\n",
    "print('Número de posibles acciones: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el algoritmo DQN, empezaremos creando las dos redes neuronales, la principal (main_nn) y la objetivo (target_nn). Ésta última será una copia de la principal, pero con sus propios pesos. \n",
    "\n",
    "También necesitaremos un optimizador (optimizer) y una función de pérdida (loss function).\n",
    "\n",
    "Para este ejemplo usaremos un perceptron con dos capas ocultas de tamaño *hidden_dim = 32*. En ejemplos complejos re-escriba la clase para implemntar la red que usted require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32) # No activation\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "main_nn = DQN(hidden_dim=32)\n",
    "target_nn = DQN(hidden_dim=32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el buffer donde guardaremos la experiencia recogida en cada episodio para usarla después y entrenar la red neuronal.\n",
    "\n",
    "Usaremos *deque* — Cola de dos extremos.\n",
    "\n",
    "Los *deque* son una generalización de pilas y colas (el nombre se pronuncia \"deck\" y es la abreviatura de \"cola de dos extremos\"). Los deque's admiten agregados y estallidos (extracciones) seguros para subprocesos y son eficientes en el acceso a la memoria memoria desde cualquier lado del deque.\n",
    "\n",
    "Para ampliar la información sobre deque vaya [aquí](https://docs.python.org/3/library/collections.html#collections.deque). Los métodos  de ReplayBufffer son:\n",
    "\n",
    "* *\\_\\_init\\_\\_()*: Crea el buffer como un deque con un tamaño máximo (*size*).\n",
    "* *add()*: adiciona al buffer la experiencia de una iteración con el ambiente. Se agrega al buffer la tupla *(state, action, reward, next_state, done)*.\n",
    "*  *\\_len\\_()* Método privado que calcula la longitud actual del buffer, es decir, el número de tuplas almacenadas acualmente en le buffer.\n",
    "* *sample()*: extrae una muestra aleatoria del buffer de tamaño *num_samples*. El método consulta el buffer por índice. Esta consulta extrae los elemento del buffer, utilizando índices. La consulta entrega las tuplas de manera segura desde el deque.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], [] \n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "    \n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "    \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función elect_epsilon_greedy_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la función  para ejecutar la política ε-voraz. La función recibe el estado y epsilon, debido que este último parámetro va cambiando a lo largo del entrenamiento. Como ejercicio dejamo que haga la implementación de la política usando una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = tf.random.uniform((1,))\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() # Random action (left or right).\n",
    "    else:\n",
    "        return tf.argmax(main_nn(state)[0]).numpy() # Greedy action for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de implementación de una clase epsilon_greddy_accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsilon_greedy_action(object):\n",
    "    def __init__(self):\n",
    "        super(epsilon_greedy_action, self).__init__()\n",
    "        \n",
    "    def call(self, state, epsilon):\n",
    "        result = tf.random.uniform((1,))\n",
    "        if result < epsilon:\n",
    "            return env.action_space.sample() # Random action (left or right).\n",
    "        else:\n",
    "            return tf.argmax(main_nn(state)[0]).numpy() # Greedy action for state.\n",
    "\n",
    "#select_epsilon_greedy_action = epsilon_greedy_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso de entrenamiento de la red principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la implementación en Pytorch y permitir una mayor flexibilidad de la implementación para futuros usos se implementa el paso de entrenamiento de la red principal. El decorador *tf.funcion* complia la función creando un grafo tensorflow, que hace que la ejecución de la funcipon sea muy eficiente en tensorflow. Para Pytorch consulte [torch::jit::compile](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1jit_1a8660dc13a6b82336aadac667e6dccba1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Realiza una iteración de entrenamiento en un lote de datos\n",
    "    muestreados del buffer de experiencia  ReplayBuffer.\"\"\"\n",
    "    # Calcula  el valor de la siguiente acción para\n",
    "    # cada registro del batch de datos  de experiencia\n",
    "    # usando la red objetivo con los pesos actuales.\n",
    "    next_qs = target_nn(next_states)\n",
    "    # Calcula el máximo valor posible de la siguiente acción\n",
    "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
    "    # predice la recompensa futura, con el modelo target_nn\n",
    "    # si dones = 1.0, se ha alcanzado un estado terminal, \n",
    "    # en ese caso solamente se toma la recompensa inmediata del siguiente estado.\n",
    "    # Si dones=0.0 se calcula la recompensa futura completa, usando\n",
    "    # el factor de descuento\n",
    "    target = rewards + (1. - dones) * discount * max_next_qs\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        # calcula la función main_nn para los estados en el batch de datos\n",
    "        # la función calcula el valor actual de cada acción para cada estado\n",
    "        qs = main_nn(states)\n",
    "        # codificación one-hot para las acciones en el bath de datos\n",
    "        action_masks = tf.one_hot(actions, num_actions)\n",
    "        # calcula el valor para la acción escogida, en cada registro del batch\n",
    "        # de datos\n",
    "        # Esta es la predicción de la recompensa futura calculada\n",
    "        # con la red main_nn, de acuero con la acción seleccionada\n",
    "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
    "        # función de pérdida: loss(pred. modelo target_nn, pred. modelo main_nn)\n",
    "        loss = mse(target, masked_qs)\n",
    "  \n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define los hiperparámetros para realizar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros.\n",
    "num_episodes = 1000 # Total de episodio a correr\n",
    "epsilon = 1.0 # probabilidad incial de exploración\n",
    "min_epsilon = 0.05\n",
    "batch_size = 32 # Tamaño de los lotes a extraer del buffer\n",
    "discount = 0.99 # factor de descuento de recompensa futura\n",
    "buffer = ReplayBuffer(100000) # buffer para almacenar experiencia\n",
    "cur_frame = 0 # contador para determinar cuando actualizar pesos en target_nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos a entrenar al agente. Para ello, primero usaremos la **política ε-voraz** para interactuar con el ambiente y recoger experiencia para poder aprender de esos datos. \n",
    "\n",
    "Después de terminar un episodio jugado, llamaremos a la función que entrena la red neuronal. \n",
    "\n",
    "Cada 2000 pasos de entrenamiento, copiaremos los pesos de la red neuronal principal a la red neuronal objetivo. También reduciremos el valor de **epsilon (ε)** Empezamos  con un valor de exploración alto y va descendiendo poco a poco. \n",
    "\n",
    "Así, veremos cómo el agente empieza a aprender a interactuar con el ambiente, imprimiendo la recompensa obtenida por el agente, a medida que avanza el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000. Epsilon: 0.999. Reward in last 100 episodes: 12.000\n",
      "Episode 50/1000. Epsilon: 0.949. Reward in last 100 episodes: 21.039\n",
      "Episode 100/1000. Epsilon: 0.899. Reward in last 100 episodes: 20.080\n",
      "Episode 150/1000. Epsilon: 0.849. Reward in last 100 episodes: 19.820\n",
      "Episode 200/1000. Epsilon: 0.799. Reward in last 100 episodes: 19.040\n",
      "Episode 250/1000. Epsilon: 0.749. Reward in last 100 episodes: 19.460\n",
      "Episode 300/1000. Epsilon: 0.699. Reward in last 100 episodes: 22.780\n",
      "Episode 350/1000. Epsilon: 0.649. Reward in last 100 episodes: 32.070\n",
      "Episode 400/1000. Epsilon: 0.599. Reward in last 100 episodes: 46.540\n",
      "Episode 450/1000. Epsilon: 0.549. Reward in last 100 episodes: 74.260\n",
      "Episode 500/1000. Epsilon: 0.499. Reward in last 100 episodes: 96.980\n",
      "Episode 550/1000. Epsilon: 0.449. Reward in last 100 episodes: 117.010\n",
      "Episode 600/1000. Epsilon: 0.399. Reward in last 100 episodes: 147.740\n",
      "Episode 650/1000. Epsilon: 0.349. Reward in last 100 episodes: 167.790\n",
      "Episode 700/1000. Epsilon: 0.299. Reward in last 100 episodes: 183.110\n",
      "Episode 750/1000. Epsilon: 0.249. Reward in last 100 episodes: 191.360\n",
      "Episode 800/1000. Epsilon: 0.199. Reward in last 100 episodes: 193.920\n",
      "Episode 850/1000. Epsilon: 0.149. Reward in last 100 episodes: 194.330\n",
      "Episode 900/1000. Epsilon: 0.099. Reward in last 100 episodes: 194.840\n",
      "Episode 950/1000. Epsilon: 0.050. Reward in last 100 episodes: 197.300\n",
      "Episode 1000/1000. Epsilon: 0.050. Reward in last 100 episodes: 197.470\n"
     ]
    }
   ],
   "source": [
    "# Empieza el entrenamiento. \n",
    "# interactúa con el medio ambiente por un episodio. \n",
    "# En cada paso del episodio guarda el resultado de la experiencia\n",
    "# Entrena con un batch de los datos de entrenamiento\n",
    "\n",
    "# lista para mantener las últimas 100 recompensas\n",
    "last_100_ep_rewards = [] \n",
    "\n",
    "# ciclo de entrenamiento. Se ejecutaran num_episodes\n",
    "for episode in range(num_episodes+1):\n",
    "    state = env.reset()\n",
    "    ep_reward, done = 0, False\n",
    "    while not done:\n",
    "        state_in = tf.expand_dims(state, axis=0)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        # Save to experience replay.\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        # Copy main_nn weights to target_nn.\n",
    "        if cur_frame % 2000 == 0:\n",
    "            target_nn.set_weights(main_nn.get_weights())\n",
    "\n",
    "        # Paso de entrenamiento de la red neuronal principal.\n",
    "        if len(buffer) >= batch_size:\n",
    "            # extrae datos del buffer\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "  \n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
    "          f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
