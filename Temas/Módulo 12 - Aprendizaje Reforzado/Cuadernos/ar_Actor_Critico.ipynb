{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Método Actor-Crítico</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Actor-Critic</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Refuerzo con línea base](#Refuerzo-con-línea-base)\n",
    "* [Método Actor-crítico ](#Método-Actor-crítico )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección exploramos el `método actor-crítico` que es una variante de los métodos básicos de gradiente de política estudiados antes. Este método mejora de forma casi mágica la estabilidad y velocidad de convergencia.\n",
    "\n",
    "Recordemos primero el método Reinforce con línea base introducido en la lección de gradiente de política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Refuerzo con línea base </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ya mencionamos antes que nuestra aproximación del gradiente de política dada por $\\nabla J \\approx \\mathbb{E}[Q(s,a)\\nabla \\pi(s|a)]$ es proporcional a la recompensa descontada desde un estado dado. Esta recompensa depende por supeuesto del ambiente e introduce una fuente importante de variación. Una solución a este incoveniente substraer de la recompensa una contante. Posibles solucioness son restar de la recompensa\n",
    " \n",
    "+ La media de las recompensas descontadas.\n",
    "+ La media móvil de las recompensas descontadas.\n",
    "+ El valor del estado V(s).\n",
    "\n",
    "La última solución es introducida en la lección actor-crítico.\n",
    "\n",
    "Por otro lado para recortar los episodios y no hace cálculos innecesarios se puede determinar a parir de que momento el factor de decuento es tan pequeño que más pasos en el episodio no aportan mucho al cáculo de la recompensa dscontada. Por ejemplo, $0.9^{50}= 0.005$. Entonces si el factor de descuento es $\\gamma=0.9$, quizás sea suficiente parar después de 50 pasos en un episodio.\n",
    "\n",
    "\n",
    "La introducción de un constante que reste a la recompensa descontada se basa en el siguiente hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verificarse que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a_t|s_t) \\right) \\right]=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, bajo el supuesto que la integral y el gradiente pueden intercambiarse, se tiene que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a|s) \\right) \\right] =\\int \\pi_{\\theta}(a|s)\\nabla \\log \\pi_{\\theta}(a|s)d\\tau =  \\int \\nabla \\pi_{\\theta}(a|s)d\\tau= \\nabla\\int  \\pi_{\\theta}(a|s)d\\tau = \\nabla  1 = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La verificación meticulosa, se  deja como ejercicio al lector interesado. Sea $b$ una variable que no depende de la trayectoria $\\tau$.  Esta variable se denominará `línea base`. Debido al resultado anterior se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}(G_t-b)\\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right].\n",
    "$$\n",
    "\n",
    "Usando $b$ en teoría y en la práctica la varianza puede ser reducida, manteniendo el gradiente de la política insesgado. Un bune valor que puede usarse como línea base el valor del estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método actor-crítico </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reescritura de la recompensa total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que en la lección de extensiones de DQN, en particular en la extensión Dueling DQN verificamos que  la recompensa total puede escribirse en la forma\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "$$\n",
    "\n",
    "en donde $V(s)$ es el valor del estado $s$ y $A(a,s)$ es la ventaja de seleccionar la acción $a$.\n",
    "\n",
    "Entonces la pregunta que surge es: ¿Y si usamos como línea base $V(s)$?. Esto parece una idea interesante porque ahora el gradiente tendría un peso asociado directamente a la ventaja de selecciona la acción $a$.\n",
    "\n",
    "En realidad, esta es una gran idea. Pero tenemos un problema. No disponemos del valor $V(s)$.\n",
    "\n",
    "Para resolverlo introducimos una segunda red que estime el valor del estado.\n",
    "\n",
    "Entonces, tendremos dos redes, que pueden ser implementadas en una sola unidad neuronal. Esta redes son\n",
    "\n",
    "+ Red de la política (actor)\n",
    "+ Red del valor (crítico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/actor-critic_01.png\"  width=\"400\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea central detrás de este método es la siguiente. La red política (actor) dice que hacer. A su vez la red valor (crítico) dice que tan buenas fueron las acciones.\n",
    "\n",
    "En la práctica las dos redes se traslapan principalmente por razones de eficiencia y convergencia.  Ambas redes se implementan como dos cabeza de una red con un cuerpo común, tal como se muestra en la siguiente imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/actor-critico_2.png\"  width=\"400\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algortimo actor-crítico (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicializa los parámetros de la red con valores aleatorios.\n",
    "2. Corre N pasos en el ambiente usando la actual política $\\pi_{\\theta}$, y almacenando $(s_t, a_t, r_t)$.\n",
    "3. $R=0$ si el final del episodio es alcanzado. Si no $R = V_{\\theta}(s)$.\n",
    "4. for $i=t-1, \\ldots, t_{start}$ (los pasos son procesados hacia atrás)\n",
    "    + $R = r_i + \\gamma R$\n",
    "    + Acumula gradientes de política: $\\partial \\theta_{\\pi} = \\partial \\theta_{\\pi} + \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i|s_i)(R-V_{\\theta}(s_i))$\n",
    "    + Acumula gradientes de valor: $\\partial \\theta_{\\nu} = \\partial \\theta_{\\nu} + \\tfrac{\\partial(R- V_{\\theta}(s_i))}{\\partial \\theta_{\\nu}}$\n",
    "5. Actualiza los parámetros de la red usando los gradientes acumulados, moviendo en la dirección de los gradientes de la política, $\\partial \\theta_{\\pi}$ y en la dirección opuesta de los gradientes de valor $\\partial \\theta_{\\nu}$.\n",
    "6. Repite desde 2 hasta alcanzar convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota de implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Por lo general la función de pérdida incluye la pérdida de l a política, la pérdida de valor y una perdida entropía, la cual introduce el efecto de empujar al agente lejos de te tener certidumbre acerca de sus acciones. La entropía introducida es $$\n",
    "\\mathfrak{L}_H = \\beta\\sum_t \\pi_{\\theta}(s_i)\\log \\pi_{\\theta}(s_i)\n",
    "$$\n",
    "+ Para mejorar la estabilidad es usual trabajar con varios ambientes, los cuales entregan observaciones de manera concurrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo: A2C en Pong </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una implementación del entrenamiento de actor-crítico para el juego Pong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "REWARD_STEPS = 10\n",
    "BASELINE_STEPS = 1000000\n",
    "GRAD_L2_CLIP = 0.1\n",
    "\n",
    "ENV_COUNT = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MeanBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))\n",
    "\n",
    "\n",
    "class MeanBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.deque = collections.deque(maxlen=capacity)\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add(self, val):\n",
    "        if len(self.deque) == self.capacity:\n",
    "            self.sum -= self.deque[0]\n",
    "        self.deque.append(val)\n",
    "        self.sum += val\n",
    "\n",
    "    def mean(self):\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.sum / len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"-n\", '--name', required=True, help=\"Name of the run\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    envs = [make_env() for _ in range(ENV_COUNT)]\n",
    "    writer = SummaryWriter(comment=\"-pong-pg-\" + args.name)\n",
    "\n",
    "    net = common.AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    train_step_idx = 0\n",
    "    baseline_buf = MeanBuffer(BASELINE_STEPS)\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []\n",
    "    m_grad_max, m_grad_mean = [], []\n",
    "    sum_reward = 0.0\n",
    "\n",
    "    with common.RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        for step_idx, exp in enumerate(exp_source):\n",
    "            baseline_buf.add(exp.reward)\n",
    "            baseline = baseline_buf.mean()\n",
    "            batch_states.append(np.array(exp.state, copy=False))\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle new rewards\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if tracker.reward(new_rewards[0], step_idx):\n",
    "                    break\n",
    "\n",
    "            if len(batch_states) < BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            train_step_idx += 1\n",
    "            states_v = torch.FloatTensor(np.array(batch_states, copy=False)).to(device)\n",
    "            batch_actions_t = torch.LongTensor(batch_actions).to(device)\n",
    "\n",
    "            scale_std = np.std(batch_scales)\n",
    "            batch_scale_v = torch.FloatTensor(batch_scales).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_v = net(states_v)\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "            log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "            loss_v = loss_policy_v + entropy_loss_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(net.parameters(), GRAD_L2_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc KL-div\n",
    "            new_logits_v = net(states_v)\n",
    "            new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "            kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "            writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "            grad_max = 0.0\n",
    "            grad_means = 0.0\n",
    "            grad_count = 0\n",
    "            for p in net.parameters():\n",
    "                grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "                grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "                grad_count += 1\n",
    "\n",
    "            writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            writer.add_scalar(\"batch_scales_std\", scale_std, step_idx)\n",
    "            writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
