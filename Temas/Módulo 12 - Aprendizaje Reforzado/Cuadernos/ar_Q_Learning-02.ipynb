{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-Learning. Algortimo completo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Q-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Q-Learning general](#Q-Learning-general)\n",
    "* [Q-Learning profundo](#Q-Learning-profundo)\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [Experiencia por repetición](#Experiencia-por-repetición)\n",
    "* [Congelando la red objetivo](#Congelando-la-red-objetivo)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Ejemplo ambiente CartPole-v0 en GymImporta módulos ](#Ejemplo-ambiente-CartPole-v0-en-Gym-Importa-módulos )\n",
    "* [Crea la clase DQNAgent](#Crea-la-clase-DQNAgent)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Clase-DDQNA](#Clase-DDQNA)\n",
    "* [Video luego de entrenado el agente](#Video-luego-de-entrenado-el-agente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del algoritmo Q-Learning, este  funciona muy bien cuando el entorno es simple y la función $Q(s,a)$ se puede representar como una tabla o matriz de valores. \n",
    "\n",
    "Pero cuando hay miles de millones de estados diferentes y cientos de acciones distintas, la tabla se vuelve enorme, y no es viable su utilización. Por ello se introdujo el Q-learning profundo (DQN).\n",
    "\n",
    "Antes de introducir los conceptos de DQN vamos a dar un paso adelante, generalizando los métodos de iteración de valores e iteración de  acciones presentados en la lección [Q-learning](ar_QLearning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Q-Learning general</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de iteración de valores e iteración de  acciones  introducidos en la lección [Q-learning](ar_QLearning.ipynb), reqieren en cada paso recorrer toda la tabla de la función $Q(s,a)$ en cada iteración. En ambientes realistas, puede ocurrir que algunos estados no son mostrados frecuentemente por el ambiente. Entonces, deberiamos tener cuidado con esa situación.\n",
    "\n",
    "Vamoa a hacer la siguiente modificación que es conocida genéricamente como `Q-learning`. Los métodos de iteración de valores e iteración de  acciones  introducidos en la lección [Q-learning](ar_QLearning.ipynb) son casos particulares, tomando $\\alpha=1$ en lo que sigue.\n",
    "\n",
    "Recordemos que el método iteración de acciones usa el mapeo explicito estado $\\to$ valor y sigue lo siguientes pasos.\n",
    " \n",
    "1. Empieza con una tabla vacía, que mapea estados en valors de las acciones.\n",
    "1. Al interactuar con el ambiente obtiene la tupla $(s, a, r, s')$ es decir (estado, acción, recompensa, nuevo estado). En este paso  se es necesario decidir cuál acción tomar y realmente no existe un camino simple para tomar la decisión.  Una solución que dimos  en la lección del [bandido multibrazo](ar_Bandido_Multi_brazo.ipynb) fue usar la técnica epsilon-greedy. que retomaremos en esta lección.\n",
    "1. Una vez se selecciona una acción $a$, se actualiza $Q(s,a)$ usando la aproximación de Bellman \n",
    "$$\n",
    "Q(s,a) \\gets r + \\gamma \\max_{a' \\in A} Q(s', a').\n",
    "$$\n",
    "1. Repita desde el paso 2.\n",
    "\n",
    "En la iteración de valor, la condición final puede ser algún umbral de la actualización, o podemos realizar episodios de prueba para estimar la recompensa esperada de la póliza, como hicimos. Otra cosa a tener en cuenta aquí es cómo actualizar los valores $Q$. Mientras tomamos muestras\n",
    "del entorno, generalmente es una mala idea simplemente asignar nuevos valores a la tabla $Q$ cada vez. En problemas complicados esto vuelve inestable el proceso.\n",
    "\n",
    "En el algoritmo general Q-learning se introduce un suavizamiento del proceso, heredado de las técnica de apredizaje general. Es decir, el valor actual $Q(s,a)$ es modicado adicionandole solamente un porcentaje $\\alpha$ de la aproximación de Bellman para la más reciente acción. En símbolos, si $\\alpha$ es un valor entre 0 y 1, la actualización de $Q$ ahora ocurre de la siguiente forma:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\gets  (1-\\alpha) Q(s,a) + \\alpha \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Así el algoritmo Q-learnig completo es\n",
    "\n",
    "1. Empieza con una tabla vacía, para $Q(s,a)$\n",
    "1. Obtiene la tupla $(s, a, r, s')$ del ambiente.\n",
    "1. Se actualiza $Q(s,a)$ usando la aproximación de Bellman, mediante \n",
    "$$\n",
    "Q(s,a) \\gets  (1-\\alpha) Q(s,a) + \\alpha \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)\n",
    "$$\n",
    "1. Revisa la convergencia, Si no se ha alcanzado repita desde  el paso 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor $\\alpha$ se conoce técnicamente como rata de aprendizaje.\n",
    "\n",
    "Vamos a revisar la implementación para el caso del lago congelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Frozen-Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comienzo del código se definen las variables globales, de las cuales destacamos ALPHA = 0.2 y TEST_EPISODES = 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase *Agent* que contiene las tablas que usaremos y los métodos que realizaran las tareas. El constructor de la clase crea las tablas, el ambiente y la variable que contendrá el estado actual. La inicialización es  más simple que antes, porque ahora no vamos contar las trancisiones de un estado a otro, ni la historia de las recompensas. Esto hace menos demandante de memoria a nuestro nuevo algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método sample_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la función que genera datos de experiencia para el agente. Simplemente recibe una muestra aleatoria del ambiente. Reinicia el ambiente si un episodio ha terminado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método best_value_and_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método determina la mejor accion y el mejor valor para un estado dado, con base en lo que encuentra en la tabla actual de valores (Q). Si alguna pareja (estado, acción) no está aún en la tabla, se recibe cero para ese caso, en la busqueda del mejor valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método value_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método recibe la tupla (estado, acción, recompensa, nuevo valor) y hace la actualización de la tabla de valores, de acuerdo con el paso 3 de nuestro algoritmo arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método *play_episodio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ejecuta un episodio completo usando el correspondiente ambiente. La acción en cada paso se toma utilizando nuestra tabla de valor actual de\n",
    "Valores Q.  Este método se utiliza para evaluar nuestra política actual para verificar el progreso\n",
    "de aprendizaje. Este método no altera nuestra tabla de valores. Solo usa la tabla para encontrar la mejor acción a realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto del código es el ciclo de entrenamiento. Asegúrese de entenderlo completamente. Observe que al comienzo del ciclo el agente toma una acción de muestra, la somete al ambiente y actualiza la tabla de valores $Q$. Además, *play_episode* se usa para determinar si el agente ha encontrado ya la política optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actualizada la mejor recompensa 0.000 -> 0.050\n",
      "Actualizada la mejor recompensa 0.050 -> 0.100\n",
      "Actualizada la mejor recompensa 0.100 -> 0.150\n",
      "Actualizada la mejor recompensa 0.150 -> 0.200\n",
      "Actualizada la mejor recompensa 0.200 -> 0.250\n",
      "Actualizada la mejor recompensa 0.250 -> 0.350\n",
      "Actualizada la mejor recompensa 0.350 -> 0.450\n",
      "Actualizada la mejor recompensa 0.450 -> 0.650\n",
      "Actualizada la mejor recompensa 0.650 -> 0.700\n",
      "Actualizada la mejor recompensa 0.700 -> 0.850\n",
      "Resuelto in 7625 iteraciones!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return old_state, action, reward, new_state\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, r, next_ss, a, ):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_v = r + GAMMA * best_v\n",
    "        old_v = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"recompensa\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Actualizada la mejor recompensa %.3f -> %.3f\" % (\n",
    "                best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Resuelto in %d iteraciones!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
