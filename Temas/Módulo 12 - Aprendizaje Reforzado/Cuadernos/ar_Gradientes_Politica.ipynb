{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Gradientes de la Política</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Policy gradients</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Gradientes de política](#Gradientes-de-política)\n",
    "* [Algoritmo Refuerzo (Reinforce)](#Algoritmo-Refuerzo-(Reinforce)\n",
    "* [Ejemplo CartPole](#Ejemplo-CartPole )\n",
    "* [Refuerzo con línea base](#Refuerzo-con-línea-base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las lecciones previas hemos hablado de la política de un agente, pero no hemos desarrollado ningún procedimiento que directamente la incluya como parte central de los algoritmos.\n",
    "\n",
    "Por otro lado, hasta el momento hemos trabajado con modelos que tienen pocas acciones posibles, por lo que el método de entropía cruzada y el método Q-learning y el método DQN, los cuales están basados en redes neuronales de clasificación aplican bien al tipo de problemas abordado. Pero, ¿Qué sucede si el número de posibles acciones es muy grande o incluso infinito o no contable?\n",
    "\n",
    "Piense por un momento en el problema de conducción automática. Un acción posible es girar el timón para cambiar ligeramente la dirección para por ejemplo evitar un obstáculo. El ángulo de giro que corresponde a la respectiva acción es un número, posiblemente un número real.\n",
    "\n",
    "\n",
    "De otro lado, nos hemos enfocado en los valores de los estados o de las acciones dados los estados (Q-valores). Estos son números que aunque están vinculados con la política, no son probabilidades. \n",
    "\n",
    "En la practica, cambios ligeros en el proceso de optimización de las funciones de pérdida pueden ocasionar cambios importantes en estos valores. Por otro lado, si la salida de la red neuronal es un vector de probabilidades, tales cambios por lo general resultan más suaves. Por todas razones, en esta lección pondremos nuestro objetivo en la política del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La hipótesis de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje reforzado se basa en que toda las metas y propósitos de una agente pueden ser pensados en términos de la optimización del valor esperado de la suma acumulativa de una señal escalar recibida que llamamos recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesos de decisión de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente trabaja siguiendo un proceso de decisión de Markov (PDM). el cual consiste de una decisión (¿Cuál acción tomar?) que debe tomar en cada estado. Esto da origen a una sucesión de estados, acciones y recompensas llamada  `trayectoria` y que se puede visualizar como\n",
    "\n",
    "$$\n",
    "s_0, a_0, r_1, s_1, a_1,r_2,\\cdots\n",
    "$$\n",
    "\n",
    "y el objetivo es maximizar este conjunto de recompensas.\n",
    "\n",
    "Un `Proceso de decisión de Markov (PDM)` es una tupla $(S,A,R,p, \\gamma)$ tal que\n",
    "\n",
    "$$\n",
    "p(s',r| s,a) = Pr[s_{t+1}= s', R_{t+1}=r|S_t=s, A_t=a]\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\cdots\n",
    "$$\n",
    "\n",
    "en donde $S_t \\in S$ (espacio de estados), $A_t \\in A$(espacio de acciones), $R_t \\in R$ (espacio de recompensas). $p$ se denomina dinámica del ambiente. \n",
    "\n",
    "En palabras simples un PDM define la probabilidad de transición a un nuevo estado $s'$ recibir una recompensa $r$ partiendo del estado actual $s$ y ejecutando una acción $a$.\n",
    "\n",
    "Técnicamente es un proceso de Markov de primer orden. Un elemento importante del modelo es el factor de descuento $\\gamma$, el cual es un valor entre 0 y 1. Sumando las recompensas futuras a lo largo del tiempo, descontadas con una potencia del factor de descuento se obtiene el concepto de retorno $G_t$. \n",
    "\n",
    "La idea central es que el agente encuentre trayectorias que maximizen el valor esperado del retorno.\n",
    "\n",
    "La dinámica del ambiente $p$ está fuera del alcance del agente. Recuerde el juego de Frozen Lake. Un ejemplo de la vida real puede ser el siguiente. Suponga que se encuentra en un lugar con demasiado viento y conduce un vehículo muy liviano. O puede imaginarse en un velero. Se puede intentar ir en una determinada dirección (la acción). Pero el viento extremo lo puede conducir en otra dirección. Si embargo puede ser que sea posible elegir una dirección diferente que permita ir en la dirección correcta. Esta es la política, que el agente si controla.\n",
    "Política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un agente sigue una política $\\pi$, genera una sucesión de estados, acciones y recompensas que denominaremos trayectoria. Técnicamente la  política se define la probabilidad de acciones dado un estado:\n",
    "\n",
    "$$\n",
    "\\pi(A_t=a|S_t=s), \\hspace{3mm} \\forall A_t\\in A(s), S_t\\in S.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Gradientes de política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El `objetivo del aprendizaje reforzado` (AR) es maximizar la recompensa $r$ cuando sigue un política parametrizada $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta)= \\mathbb{E}_{\\pi}[r(\\tau)],\n",
    "$$\n",
    "\n",
    "en donde $r(\\tau)$ representa la recompensa total para una trayectoria $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Puede demostrarse que bajos ciertos supuestos que generalmente se tienen en AR todo PDM finito tiene al menos una política optima en el sentido de la recompensa obtneida y que entre todas las políticas optimales al menos una es estacionaria y determinista.\n",
    "\n",
    "El procedimiento clásico para estimar el parámetro $\\theta$ es el método del gradiente descendiente, el cual ( en términos muy simples) se basa en la regla de actualización\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\theta_t).\n",
    "$$\n",
    "\n",
    "$\\nabla J(\\theta_t)$ es el gradiente de la política y $\\alpha$ una rata de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El reto que tenemos es encontrar el gradiente de la política. El primer  problema es que $J(\\theta)$ está definido como una esperanza (¡una integral!). \n",
    "\n",
    "Bajo algunas condiciones relacionadas con las derivadas de $\\pi$  es posible pasar el gradiente a través de la integral como sigue\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\mathbb{E}_{\\pi}[r(\\tau)] &= \\nabla \\int \\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int\\nabla \\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int \\pi(\\tau)\\nabla \\log\\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que se tiene el siguiente resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Teorema del gradiente de la política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)\\nabla \\log \\pi_{\\theta}(\\tau)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se expande la definición de $\\pi_{\\theta}(\\tau)$ se obtiene\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta}(\\tau) = P(s_0)\\prod_{t=1}^{T}\\pi_{\\theta}(a_t|s_t)p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "$P$ representa la distribución del estado inicial $s_0$ y se ha aplicado la regla del producto de la probabilidad y el hecho de ser un proceso de Markov que implica que cada nueva acción es independiente de la anterior. $T$ representa la longitud de la trayectoria. \n",
    "\n",
    "Tomando logaritmo se obtiene que\n",
    "\n",
    "$$\n",
    "\\log \\pi_{\\theta}(\\tau) = \\log P(s_0) + \\sum_{t=1}^{T}\\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\log p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "Por lo tanto se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[ r(\\tau) \\left(\\sum_{t=1}^{T}\\nabla \\log \\pi_{\\theta}(a_t|s_t)\\right) \\right]\n",
    "$$\n",
    "\n",
    "Este resultado dice que no es necesario conocer la distribución del estado inicial $P$ ni la dinámica del ambiente $p$ para calcular el gradiente de la política.\n",
    "\n",
    "Los algoritmos que usan este resultado son conocidos como `algoritmos libres del modelo`, debido a que no modelamos el ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la última ecuación se tiene que el cálculo del gradiente involucra una esperanza, es decir una integral, la cual por lo general es intratable. Aquí es ne donde entran en acción las técnicas Monte Carlo Markov Chain (MCMC). La idea central es que en cada paso de tiempo (cada iteración) se hace lo siguiente:\n",
    "\n",
    "1. Se obtiene una muestra aleatoria grande de acciones siguiendo la distribución (política) $\\pi(\\theta^*)$, en donde $\\theta^*$ es la estimación actual del parámetro $\\theta$.\n",
    "1. Se calcula el promedio de la expresión del gradiente para la muestra obtenida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensa de la trayectoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El termino $r(\\tau)$ de la recompensa total de la trayectoria $\\tau$ permanece inmutable en la expresión de $\\nabla J(\\theta)$. Este gradiente parametrizado no depende de  $r(\\tau)$. Sin embargo el termino agrega bastante varianza en el muestreo MCMC. En realidad hay $T$ fuentes de variación debidas a  cada $R_t$. En su lugar podemos hacer uso del retorno $G_t$ debido a que desde el punto de vista de la  optimización del objetivo de AR, la recompensa pasada no contribuye para nada.\n",
    "\n",
    "Cuando se reemplaza $r(\\tau)$ por el retorno descontado $G_t$ arribamos al algoritmo clásico del gradiente de la política llamado reforzamiento (`reinforce`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método Refuerzo (Reinforce) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo reinforce se basa en aproximar el gradiente de la política como \n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}G_t \\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, en la formula anterior no se ha resuelto el problema de la varianza en las trayectorias muestreadas. Desde el punto de vista de la inferencia bayesiana, se sabe que cuando el tamaño de muestra es grande, no importa la a priori seleccionada. Esto implica que con muestras grandes de trayectorias la distribución del estado inicial no es importante, el algortimo MCMC converge al modelo de los parámetros verdaderos. El problema es que varianzas muy grandes el problema de estabilizar los parámetros del modelo es bastante difícil. Resolveremos este problema en la lección Actor-crítico.\n",
    "\n",
    "De momento examinemos la implementación usual del método reinforce.\n",
    "\n",
    "Como hemos hecho antes a lo largo del curso, vamos a usar la aproximación del retorno con la función de Q-valores $Q(s,a)$. Así, para cada paso de la trayectoria tenemos que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\approx \\mathbb{E}_{\\pi_{\\theta}}\\left[  Q(s,a) \\nabla\\log \\pi_{\\theta}(a|s) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente de la política indica la dirección en la cual se deben cambiar los parámetros de la red neuronal para mejorar la política en términos de la recompensa total acumulada. En esta aproximación el gradiente se escala proporcionalmente al valor de la acción tomada $Q(s,a)$ y el gradiente en sí mismo es igual al gradiente del logaritmo de la probabilidad de la acción tomada. \n",
    "\n",
    "Esto significa que estamos tratando de aumentar la probabilidad de acciones que nos han dado buena recompensa total y disminuyen la probabilidad de acciones con\n",
    "malos resultados finales. \n",
    "\n",
    "La esperanza es aproximada mediante el promedio del gradiente en varios pasos, de acuerdo con las técnicas MCMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida y máxima log-verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida que usualmente se utiliza en los métodos de gradiente de la política es\n",
    "\n",
    "$$\n",
    "\\mathfrak{L} = -Q(s,a) \\log\\pi(a|s)\n",
    "$$\n",
    "\n",
    "Es función de pérdida es menos la log-verosimilitud del modelo estadístico\n",
    "\n",
    "$$\n",
    "L = \\pi(a|s)^{Q(s,a)}.\n",
    "$$\n",
    "\n",
    "Por ejemplo para una trayectoria $\\tau$ con $T$ pasos se tiene que\n",
    "\n",
    "$$\n",
    "\\log L(\\tau) = \\sum_{t=1}^{T} Q(s_t,a_t)\\log\\pi(a_t|s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El algortimo Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialice la red neuronal con pesos aleatorios.\n",
    "1. Corra N episodios completos (una trayectoria), almacenando  las transiciones $(s,a,r,s')$.\n",
    "1. Para cada paso, $t$, de cada uno de los episodios,  $k$, calcule la recompensa total descontada para la sucesión de pasos: $Q_{t,k}= \\sum_{i=0} \\gamma^i r_i$.\n",
    "1. Calcule la función de pérdida para todas las transiciones: $\\mathfrak{L} = - \\sum_{tk} Q_{t,k} \\log(\\pi(s_{t,k},a_{t,k}))$\n",
    "1. Ejecute el paso de actualización de pesos del algoritmo SGD seleccionado.\n",
    "1. Repita desde el paso 2 hasta convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias con Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método reinforce se diferencia del método Q-learning en varias cosas.\n",
    "\n",
    "1. No se requiere una exploración explícita de tipo epsilon-greedy, que se usa en Q-learning para evitar que el algoritmo quede estancado en un mínimo local. Ahora la red neuronal retorna probabilidades directamente. Al comienzo la red es inicializada con valores aleatorios, por lo que la primera salida corresponde a una distribución uniforme.\n",
    "1. No se usa la memoria de repetición (replay buffer). Los métodos gradiente de política son métodos basados en la política (Q-learning es libre de política). Es decir, los datos obtenidos para el entrenamiento so se basan en políticas viejas. Esto es bueno y malo. Lo  bueno es que estos métodos convergen más rápido por lo general. Lo malo es que  requieren por lo general mucha más interacción con el ambiente que por ejemplo DQN.\n",
    "1. No se requiere una red neuronal objetivo (target). Aquí se usan los Q-valores, pero ya no son aproximados como DQN, sino que se calculan completamente en cada trayectoria. En DQN se requiere la red target para romper la correlación entre valores $Q(s,a)$, porque en cada paso ellos son aproximados. Aquí los Q-valores se calculan sin aproximación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Ejemplo CartPole </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vemos el método en acción. usaremos el problema CartPole para ilustrar el método con código real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa  módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "#import ptan\n",
    "import numpy as np\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#ptan\n",
    "import actions\n",
    "import agent\n",
    "import experience\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase PGN (Policy Gradient Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la red neuronal que usaremos. Ya familiar para todos.\n",
    "Observe que la red no regresa probabilidades como hemos previsto. Esto se hace porque en la implementación decidimos usar en la perdida la función *log_softmax* que calcula de manera eficiente el logaritmo del softmax que ne realidad es lo que se requiere para la función de pérdida. Además este cálculo es bastante más estable que calcular inicialmente softmax y luego el logaritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de los Q-valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función se hace el cálculo de los Q-valores de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "11: reward:  11.00, mean_100:  11.00, episodes: 1\n",
      "23: reward:  12.00, mean_100:  11.50, episodes: 2\n",
      "37: reward:  14.00, mean_100:  12.33, episodes: 3\n",
      "48: reward:  11.00, mean_100:  12.00, episodes: 4\n",
      "65: reward:  17.00, mean_100:  13.00, episodes: 5\n",
      "75: reward:  10.00, mean_100:  12.50, episodes: 6\n",
      "87: reward:  12.00, mean_100:  12.43, episodes: 7\n",
      "100: reward:  13.00, mean_100:  12.50, episodes: 8\n",
      "111: reward:  11.00, mean_100:  12.33, episodes: 9\n",
      "126: reward:  15.00, mean_100:  12.60, episodes: 10\n",
      "137: reward:  11.00, mean_100:  12.45, episodes: 11\n",
      "158: reward:  21.00, mean_100:  13.17, episodes: 12\n",
      "167: reward:   9.00, mean_100:  12.85, episodes: 13\n",
      "209: reward:  42.00, mean_100:  14.93, episodes: 14\n",
      "219: reward:  10.00, mean_100:  14.60, episodes: 15\n",
      "237: reward:  18.00, mean_100:  14.81, episodes: 16\n",
      "257: reward:  20.00, mean_100:  15.12, episodes: 17\n",
      "270: reward:  13.00, mean_100:  15.00, episodes: 18\n",
      "291: reward:  21.00, mean_100:  15.32, episodes: 19\n",
      "302: reward:  11.00, mean_100:  15.10, episodes: 20\n",
      "340: reward:  38.00, mean_100:  16.19, episodes: 21\n",
      "371: reward:  31.00, mean_100:  16.86, episodes: 22\n",
      "392: reward:  21.00, mean_100:  17.04, episodes: 23\n",
      "403: reward:  11.00, mean_100:  16.79, episodes: 24\n",
      "424: reward:  21.00, mean_100:  16.96, episodes: 25\n",
      "435: reward:  11.00, mean_100:  16.73, episodes: 26\n",
      "452: reward:  17.00, mean_100:  16.74, episodes: 27\n",
      "468: reward:  16.00, mean_100:  16.71, episodes: 28\n",
      "478: reward:  10.00, mean_100:  16.48, episodes: 29\n",
      "492: reward:  14.00, mean_100:  16.40, episodes: 30\n",
      "521: reward:  29.00, mean_100:  16.81, episodes: 31\n",
      "542: reward:  21.00, mean_100:  16.94, episodes: 32\n",
      "557: reward:  15.00, mean_100:  16.88, episodes: 33\n",
      "591: reward:  34.00, mean_100:  17.38, episodes: 34\n",
      "605: reward:  14.00, mean_100:  17.29, episodes: 35\n",
      "644: reward:  39.00, mean_100:  17.89, episodes: 36\n",
      "670: reward:  26.00, mean_100:  18.11, episodes: 37\n",
      "690: reward:  20.00, mean_100:  18.16, episodes: 38\n",
      "703: reward:  13.00, mean_100:  18.03, episodes: 39\n",
      "731: reward:  28.00, mean_100:  18.27, episodes: 40\n",
      "750: reward:  19.00, mean_100:  18.29, episodes: 41\n",
      "764: reward:  14.00, mean_100:  18.19, episodes: 42\n",
      "783: reward:  19.00, mean_100:  18.21, episodes: 43\n",
      "828: reward:  45.00, mean_100:  18.82, episodes: 44\n",
      "849: reward:  21.00, mean_100:  18.87, episodes: 45\n",
      "866: reward:  17.00, mean_100:  18.83, episodes: 46\n",
      "928: reward:  62.00, mean_100:  19.74, episodes: 47\n",
      "948: reward:  20.00, mean_100:  19.75, episodes: 48\n",
      "973: reward:  25.00, mean_100:  19.86, episodes: 49\n",
      "987: reward:  14.00, mean_100:  19.74, episodes: 50\n",
      "1007: reward:  20.00, mean_100:  19.75, episodes: 51\n",
      "1074: reward:  67.00, mean_100:  20.65, episodes: 52\n",
      "1093: reward:  19.00, mean_100:  20.62, episodes: 53\n",
      "1105: reward:  12.00, mean_100:  20.46, episodes: 54\n",
      "1137: reward:  32.00, mean_100:  20.67, episodes: 55\n",
      "1151: reward:  14.00, mean_100:  20.55, episodes: 56\n",
      "1296: reward: 145.00, mean_100:  22.74, episodes: 57\n",
      "1379: reward:  83.00, mean_100:  23.78, episodes: 58\n",
      "1389: reward:  10.00, mean_100:  23.54, episodes: 59\n",
      "1424: reward:  35.00, mean_100:  23.73, episodes: 60\n",
      "1454: reward:  30.00, mean_100:  23.84, episodes: 61\n",
      "1473: reward:  19.00, mean_100:  23.76, episodes: 62\n",
      "1496: reward:  23.00, mean_100:  23.75, episodes: 63\n",
      "1528: reward:  32.00, mean_100:  23.88, episodes: 64\n",
      "1553: reward:  25.00, mean_100:  23.89, episodes: 65\n",
      "1593: reward:  40.00, mean_100:  24.14, episodes: 66\n",
      "1630: reward:  37.00, mean_100:  24.33, episodes: 67\n",
      "1712: reward:  82.00, mean_100:  25.18, episodes: 68\n",
      "1735: reward:  23.00, mean_100:  25.14, episodes: 69\n",
      "1778: reward:  43.00, mean_100:  25.40, episodes: 70\n",
      "1804: reward:  26.00, mean_100:  25.41, episodes: 71\n",
      "1818: reward:  14.00, mean_100:  25.25, episodes: 72\n",
      "1852: reward:  34.00, mean_100:  25.37, episodes: 73\n",
      "1914: reward:  62.00, mean_100:  25.86, episodes: 74\n",
      "2008: reward:  94.00, mean_100:  26.77, episodes: 75\n",
      "2025: reward:  17.00, mean_100:  26.64, episodes: 76\n",
      "2043: reward:  18.00, mean_100:  26.53, episodes: 77\n",
      "2178: reward: 135.00, mean_100:  27.92, episodes: 78\n",
      "2230: reward:  52.00, mean_100:  28.23, episodes: 79\n",
      "2254: reward:  24.00, mean_100:  28.18, episodes: 80\n",
      "2299: reward:  45.00, mean_100:  28.38, episodes: 81\n",
      "2351: reward:  52.00, mean_100:  28.67, episodes: 82\n",
      "2418: reward:  67.00, mean_100:  29.13, episodes: 83\n",
      "2447: reward:  29.00, mean_100:  29.13, episodes: 84\n",
      "2482: reward:  35.00, mean_100:  29.20, episodes: 85\n",
      "2587: reward: 105.00, mean_100:  30.08, episodes: 86\n",
      "2642: reward:  55.00, mean_100:  30.37, episodes: 87\n",
      "2684: reward:  42.00, mean_100:  30.50, episodes: 88\n",
      "2826: reward: 142.00, mean_100:  31.75, episodes: 89\n",
      "2907: reward:  81.00, mean_100:  32.30, episodes: 90\n",
      "2946: reward:  39.00, mean_100:  32.37, episodes: 91\n",
      "2990: reward:  44.00, mean_100:  32.50, episodes: 92\n",
      "3037: reward:  47.00, mean_100:  32.66, episodes: 93\n",
      "3059: reward:  22.00, mean_100:  32.54, episodes: 94\n",
      "3144: reward:  85.00, mean_100:  33.09, episodes: 95\n",
      "3167: reward:  23.00, mean_100:  32.99, episodes: 96\n",
      "3215: reward:  48.00, mean_100:  33.14, episodes: 97\n",
      "3287: reward:  72.00, mean_100:  33.54, episodes: 98\n",
      "3376: reward:  89.00, mean_100:  34.10, episodes: 99\n",
      "3447: reward:  71.00, mean_100:  34.47, episodes: 100\n",
      "3523: reward:  76.00, mean_100:  35.12, episodes: 101\n",
      "3619: reward:  96.00, mean_100:  35.96, episodes: 102\n",
      "3675: reward:  56.00, mean_100:  36.38, episodes: 103\n",
      "3708: reward:  33.00, mean_100:  36.60, episodes: 104\n",
      "3786: reward:  78.00, mean_100:  37.21, episodes: 105\n",
      "3822: reward:  36.00, mean_100:  37.47, episodes: 106\n",
      "3959: reward: 137.00, mean_100:  38.72, episodes: 107\n",
      "4004: reward:  45.00, mean_100:  39.04, episodes: 108\n",
      "4114: reward: 110.00, mean_100:  40.03, episodes: 109\n",
      "4177: reward:  63.00, mean_100:  40.51, episodes: 110\n",
      "4243: reward:  66.00, mean_100:  41.06, episodes: 111\n",
      "4314: reward:  71.00, mean_100:  41.56, episodes: 112\n",
      "4367: reward:  53.00, mean_100:  42.00, episodes: 113\n",
      "4483: reward: 116.00, mean_100:  42.74, episodes: 114\n",
      "4517: reward:  34.00, mean_100:  42.98, episodes: 115\n",
      "4606: reward:  89.00, mean_100:  43.69, episodes: 116\n",
      "4645: reward:  39.00, mean_100:  43.88, episodes: 117\n",
      "4713: reward:  68.00, mean_100:  44.43, episodes: 118\n",
      "4731: reward:  18.00, mean_100:  44.40, episodes: 119\n",
      "4807: reward:  76.00, mean_100:  45.05, episodes: 120\n",
      "5007: reward: 200.00, mean_100:  46.67, episodes: 121\n",
      "5158: reward: 151.00, mean_100:  47.87, episodes: 122\n",
      "5268: reward: 110.00, mean_100:  48.76, episodes: 123\n",
      "5412: reward: 144.00, mean_100:  50.09, episodes: 124\n",
      "5494: reward:  82.00, mean_100:  50.70, episodes: 125\n",
      "5599: reward: 105.00, mean_100:  51.64, episodes: 126\n",
      "5756: reward: 157.00, mean_100:  53.04, episodes: 127\n",
      "5922: reward: 166.00, mean_100:  54.54, episodes: 128\n",
      "5997: reward:  75.00, mean_100:  55.19, episodes: 129\n",
      "6127: reward: 130.00, mean_100:  56.35, episodes: 130\n",
      "6191: reward:  64.00, mean_100:  56.70, episodes: 131\n",
      "6336: reward: 145.00, mean_100:  57.94, episodes: 132\n",
      "6476: reward: 140.00, mean_100:  59.19, episodes: 133\n",
      "6581: reward: 105.00, mean_100:  59.90, episodes: 134\n",
      "6654: reward:  73.00, mean_100:  60.49, episodes: 135\n",
      "6753: reward:  99.00, mean_100:  61.09, episodes: 136\n",
      "6895: reward: 142.00, mean_100:  62.25, episodes: 137\n",
      "6983: reward:  88.00, mean_100:  62.93, episodes: 138\n",
      "7092: reward: 109.00, mean_100:  63.89, episodes: 139\n",
      "7226: reward: 134.00, mean_100:  64.95, episodes: 140\n",
      "7305: reward:  79.00, mean_100:  65.55, episodes: 141\n",
      "7460: reward: 155.00, mean_100:  66.96, episodes: 142\n",
      "7495: reward:  35.00, mean_100:  67.12, episodes: 143\n",
      "7617: reward: 122.00, mean_100:  67.89, episodes: 144\n",
      "7757: reward: 140.00, mean_100:  69.08, episodes: 145\n",
      "7913: reward: 156.00, mean_100:  70.47, episodes: 146\n",
      "8088: reward: 175.00, mean_100:  71.60, episodes: 147\n",
      "8204: reward: 116.00, mean_100:  72.56, episodes: 148\n",
      "8390: reward: 186.00, mean_100:  74.17, episodes: 149\n",
      "8590: reward: 200.00, mean_100:  76.03, episodes: 150\n",
      "8705: reward: 115.00, mean_100:  76.98, episodes: 151\n",
      "8889: reward: 184.00, mean_100:  78.15, episodes: 152\n",
      "9062: reward: 173.00, mean_100:  79.69, episodes: 153\n",
      "9262: reward: 200.00, mean_100:  81.57, episodes: 154\n",
      "9392: reward: 130.00, mean_100:  82.55, episodes: 155\n",
      "9550: reward: 158.00, mean_100:  83.99, episodes: 156\n",
      "9674: reward: 124.00, mean_100:  83.78, episodes: 157\n",
      "9874: reward: 200.00, mean_100:  84.95, episodes: 158\n",
      "10068: reward: 194.00, mean_100:  86.79, episodes: 159\n",
      "10239: reward: 171.00, mean_100:  88.15, episodes: 160\n",
      "10298: reward:  59.00, mean_100:  88.44, episodes: 161\n",
      "10438: reward: 140.00, mean_100:  89.65, episodes: 162\n",
      "10459: reward:  21.00, mean_100:  89.63, episodes: 163\n",
      "10593: reward: 134.00, mean_100:  90.65, episodes: 164\n",
      "10793: reward: 200.00, mean_100:  92.40, episodes: 165\n",
      "10993: reward: 200.00, mean_100:  94.00, episodes: 166\n",
      "11193: reward: 200.00, mean_100:  95.63, episodes: 167\n",
      "11294: reward: 101.00, mean_100:  95.82, episodes: 168\n",
      "11333: reward:  39.00, mean_100:  95.98, episodes: 169\n",
      "11414: reward:  81.00, mean_100:  96.36, episodes: 170\n",
      "11614: reward: 200.00, mean_100:  98.10, episodes: 171\n",
      "11695: reward:  81.00, mean_100:  98.77, episodes: 172\n",
      "11865: reward: 170.00, mean_100: 100.13, episodes: 173\n",
      "11959: reward:  94.00, mean_100: 100.45, episodes: 174\n",
      "12065: reward: 106.00, mean_100: 100.57, episodes: 175\n",
      "12131: reward:  66.00, mean_100: 101.06, episodes: 176\n",
      "12234: reward: 103.00, mean_100: 101.91, episodes: 177\n",
      "12280: reward:  46.00, mean_100: 101.02, episodes: 178\n",
      "12413: reward: 133.00, mean_100: 101.83, episodes: 179\n",
      "12554: reward: 141.00, mean_100: 103.00, episodes: 180\n",
      "12648: reward:  94.00, mean_100: 103.49, episodes: 181\n",
      "12716: reward:  68.00, mean_100: 103.65, episodes: 182\n",
      "12812: reward:  96.00, mean_100: 103.94, episodes: 183\n",
      "12908: reward:  96.00, mean_100: 104.61, episodes: 184\n",
      "13001: reward:  93.00, mean_100: 105.19, episodes: 185\n",
      "13051: reward:  50.00, mean_100: 104.64, episodes: 186\n",
      "13227: reward: 176.00, mean_100: 105.85, episodes: 187\n",
      "13427: reward: 200.00, mean_100: 107.43, episodes: 188\n",
      "13458: reward:  31.00, mean_100: 106.32, episodes: 189\n",
      "13488: reward:  30.00, mean_100: 105.81, episodes: 190\n",
      "13561: reward:  73.00, mean_100: 106.15, episodes: 191\n",
      "13638: reward:  77.00, mean_100: 106.48, episodes: 192\n",
      "13745: reward: 107.00, mean_100: 107.08, episodes: 193\n",
      "13922: reward: 177.00, mean_100: 108.63, episodes: 194\n",
      "14014: reward:  92.00, mean_100: 108.70, episodes: 195\n",
      "14092: reward:  78.00, mean_100: 109.25, episodes: 196\n",
      "14212: reward: 120.00, mean_100: 109.97, episodes: 197\n",
      "14320: reward: 108.00, mean_100: 110.33, episodes: 198\n",
      "14348: reward:  28.00, mean_100: 109.72, episodes: 199\n",
      "14424: reward:  76.00, mean_100: 109.77, episodes: 200\n",
      "14606: reward: 182.00, mean_100: 110.83, episodes: 201\n",
      "14635: reward:  29.00, mean_100: 110.16, episodes: 202\n",
      "14710: reward:  75.00, mean_100: 110.35, episodes: 203\n",
      "14800: reward:  90.00, mean_100: 110.92, episodes: 204\n",
      "14890: reward:  90.00, mean_100: 111.04, episodes: 205\n",
      "15090: reward: 200.00, mean_100: 112.68, episodes: 206\n",
      "15222: reward: 132.00, mean_100: 112.63, episodes: 207\n",
      "15340: reward: 118.00, mean_100: 113.36, episodes: 208\n",
      "15377: reward:  37.00, mean_100: 112.63, episodes: 209\n",
      "15535: reward: 158.00, mean_100: 113.58, episodes: 210\n",
      "15735: reward: 200.00, mean_100: 114.92, episodes: 211\n",
      "15924: reward: 189.00, mean_100: 116.10, episodes: 212\n",
      "16005: reward:  81.00, mean_100: 116.38, episodes: 213\n",
      "16033: reward:  28.00, mean_100: 115.50, episodes: 214\n",
      "16166: reward: 133.00, mean_100: 116.49, episodes: 215\n",
      "16246: reward:  80.00, mean_100: 116.40, episodes: 216\n",
      "16357: reward: 111.00, mean_100: 117.12, episodes: 217\n",
      "16557: reward: 200.00, mean_100: 118.44, episodes: 218\n",
      "16757: reward: 200.00, mean_100: 120.26, episodes: 219\n",
      "16789: reward:  32.00, mean_100: 119.82, episodes: 220\n",
      "16989: reward: 200.00, mean_100: 119.82, episodes: 221\n",
      "17189: reward: 200.00, mean_100: 120.31, episodes: 222\n",
      "17389: reward: 200.00, mean_100: 121.21, episodes: 223\n",
      "17414: reward:  25.00, mean_100: 120.02, episodes: 224\n",
      "17614: reward: 200.00, mean_100: 121.20, episodes: 225\n",
      "17814: reward: 200.00, mean_100: 122.15, episodes: 226\n",
      "17898: reward:  84.00, mean_100: 121.42, episodes: 227\n",
      "17929: reward:  31.00, mean_100: 120.07, episodes: 228\n",
      "18034: reward: 105.00, mean_100: 120.37, episodes: 229\n",
      "18223: reward: 189.00, mean_100: 120.96, episodes: 230\n",
      "18296: reward:  73.00, mean_100: 121.05, episodes: 231\n",
      "18496: reward: 200.00, mean_100: 121.60, episodes: 232\n",
      "18615: reward: 119.00, mean_100: 121.39, episodes: 233\n",
      "18815: reward: 200.00, mean_100: 122.34, episodes: 234\n",
      "19015: reward: 200.00, mean_100: 123.61, episodes: 235\n",
      "19215: reward: 200.00, mean_100: 124.62, episodes: 236\n",
      "19415: reward: 200.00, mean_100: 125.20, episodes: 237\n",
      "19572: reward: 157.00, mean_100: 125.89, episodes: 238\n",
      "19697: reward: 125.00, mean_100: 126.05, episodes: 239\n",
      "19778: reward:  81.00, mean_100: 125.52, episodes: 240\n",
      "19907: reward: 129.00, mean_100: 126.02, episodes: 241\n",
      "20107: reward: 200.00, mean_100: 126.47, episodes: 242\n",
      "20307: reward: 200.00, mean_100: 128.12, episodes: 243\n",
      "20507: reward: 200.00, mean_100: 128.90, episodes: 244\n",
      "20707: reward: 200.00, mean_100: 129.50, episodes: 245\n",
      "20907: reward: 200.00, mean_100: 129.94, episodes: 246\n",
      "21046: reward: 139.00, mean_100: 129.58, episodes: 247\n",
      "21246: reward: 200.00, mean_100: 130.42, episodes: 248\n",
      "21446: reward: 200.00, mean_100: 130.56, episodes: 249\n",
      "21646: reward: 200.00, mean_100: 130.56, episodes: 250\n",
      "21846: reward: 200.00, mean_100: 131.41, episodes: 251\n",
      "21875: reward:  29.00, mean_100: 129.86, episodes: 252\n",
      "21945: reward:  70.00, mean_100: 128.83, episodes: 253\n",
      "22145: reward: 200.00, mean_100: 128.83, episodes: 254\n",
      "22345: reward: 200.00, mean_100: 129.53, episodes: 255\n",
      "22545: reward: 200.00, mean_100: 129.95, episodes: 256\n",
      "22745: reward: 200.00, mean_100: 130.71, episodes: 257\n",
      "22945: reward: 200.00, mean_100: 130.71, episodes: 258\n",
      "23145: reward: 200.00, mean_100: 130.77, episodes: 259\n",
      "23251: reward: 106.00, mean_100: 130.12, episodes: 260\n",
      "23451: reward: 200.00, mean_100: 131.53, episodes: 261\n",
      "23651: reward: 200.00, mean_100: 132.13, episodes: 262\n",
      "23851: reward: 200.00, mean_100: 133.92, episodes: 263\n",
      "24051: reward: 200.00, mean_100: 134.58, episodes: 264\n",
      "24251: reward: 200.00, mean_100: 134.58, episodes: 265\n",
      "24451: reward: 200.00, mean_100: 134.58, episodes: 266\n",
      "24651: reward: 200.00, mean_100: 134.58, episodes: 267\n",
      "24851: reward: 200.00, mean_100: 135.57, episodes: 268\n",
      "25051: reward: 200.00, mean_100: 137.18, episodes: 269\n",
      "25251: reward: 200.00, mean_100: 138.37, episodes: 270\n",
      "25451: reward: 200.00, mean_100: 138.37, episodes: 271\n",
      "25651: reward: 200.00, mean_100: 139.56, episodes: 272\n",
      "25851: reward: 200.00, mean_100: 139.86, episodes: 273\n",
      "26051: reward: 200.00, mean_100: 140.92, episodes: 274\n",
      "26251: reward: 200.00, mean_100: 141.86, episodes: 275\n",
      "26451: reward: 200.00, mean_100: 143.20, episodes: 276\n",
      "26651: reward: 200.00, mean_100: 144.17, episodes: 277\n",
      "26851: reward: 200.00, mean_100: 145.71, episodes: 278\n",
      "27051: reward: 200.00, mean_100: 146.38, episodes: 279\n",
      "27251: reward: 200.00, mean_100: 146.97, episodes: 280\n",
      "27451: reward: 200.00, mean_100: 148.03, episodes: 281\n",
      "27651: reward: 200.00, mean_100: 149.35, episodes: 282\n",
      "27851: reward: 200.00, mean_100: 150.39, episodes: 283\n",
      "28051: reward: 200.00, mean_100: 151.43, episodes: 284\n",
      "28251: reward: 200.00, mean_100: 152.50, episodes: 285\n",
      "28451: reward: 200.00, mean_100: 154.00, episodes: 286\n",
      "28651: reward: 200.00, mean_100: 154.24, episodes: 287\n",
      "28851: reward: 200.00, mean_100: 154.24, episodes: 288\n",
      "29051: reward: 200.00, mean_100: 155.93, episodes: 289\n",
      "29251: reward: 200.00, mean_100: 157.63, episodes: 290\n",
      "29447: reward: 196.00, mean_100: 158.86, episodes: 291\n",
      "29647: reward: 200.00, mean_100: 160.09, episodes: 292\n",
      "29847: reward: 200.00, mean_100: 161.02, episodes: 293\n",
      "30047: reward: 200.00, mean_100: 161.25, episodes: 294\n",
      "30223: reward: 176.00, mean_100: 162.09, episodes: 295\n",
      "30423: reward: 200.00, mean_100: 163.31, episodes: 296\n",
      "30623: reward: 200.00, mean_100: 164.11, episodes: 297\n",
      "30823: reward: 200.00, mean_100: 165.03, episodes: 298\n",
      "31023: reward: 200.00, mean_100: 166.75, episodes: 299\n",
      "31223: reward: 200.00, mean_100: 167.99, episodes: 300\n",
      "31423: reward: 200.00, mean_100: 168.17, episodes: 301\n",
      "31623: reward: 200.00, mean_100: 169.88, episodes: 302\n",
      "31823: reward: 200.00, mean_100: 171.13, episodes: 303\n",
      "32023: reward: 200.00, mean_100: 172.23, episodes: 304\n",
      "32223: reward: 200.00, mean_100: 173.33, episodes: 305\n",
      "32423: reward: 200.00, mean_100: 173.33, episodes: 306\n",
      "32623: reward: 200.00, mean_100: 174.01, episodes: 307\n",
      "32823: reward: 200.00, mean_100: 174.83, episodes: 308\n",
      "33023: reward: 200.00, mean_100: 176.46, episodes: 309\n",
      "33223: reward: 200.00, mean_100: 176.88, episodes: 310\n",
      "33423: reward: 200.00, mean_100: 176.88, episodes: 311\n",
      "33623: reward: 200.00, mean_100: 176.99, episodes: 312\n",
      "33823: reward: 200.00, mean_100: 178.18, episodes: 313\n",
      "34023: reward: 200.00, mean_100: 179.90, episodes: 314\n",
      "34223: reward: 200.00, mean_100: 180.57, episodes: 315\n",
      "34423: reward: 200.00, mean_100: 181.77, episodes: 316\n",
      "34623: reward: 200.00, mean_100: 182.66, episodes: 317\n",
      "34820: reward: 197.00, mean_100: 182.63, episodes: 318\n",
      "35020: reward: 200.00, mean_100: 182.63, episodes: 319\n",
      "35220: reward: 200.00, mean_100: 184.31, episodes: 320\n",
      "35420: reward: 200.00, mean_100: 184.31, episodes: 321\n",
      "35620: reward: 200.00, mean_100: 184.31, episodes: 322\n",
      "35820: reward: 200.00, mean_100: 184.31, episodes: 323\n",
      "36020: reward: 200.00, mean_100: 186.06, episodes: 324\n",
      "36220: reward: 200.00, mean_100: 186.06, episodes: 325\n",
      "36351: reward: 131.00, mean_100: 185.37, episodes: 326\n",
      "36551: reward: 200.00, mean_100: 186.53, episodes: 327\n",
      "36655: reward: 104.00, mean_100: 187.26, episodes: 328\n",
      "36855: reward: 200.00, mean_100: 188.21, episodes: 329\n",
      "36927: reward:  72.00, mean_100: 187.04, episodes: 330\n",
      "37127: reward: 200.00, mean_100: 188.31, episodes: 331\n",
      "37327: reward: 200.00, mean_100: 188.31, episodes: 332\n",
      "37346: reward:  19.00, mean_100: 187.31, episodes: 333\n",
      "37546: reward: 200.00, mean_100: 187.31, episodes: 334\n",
      "37746: reward: 200.00, mean_100: 187.31, episodes: 335\n",
      "37763: reward:  17.00, mean_100: 185.48, episodes: 336\n",
      "37790: reward:  27.00, mean_100: 183.75, episodes: 337\n",
      "37990: reward: 200.00, mean_100: 184.18, episodes: 338\n",
      "38190: reward: 200.00, mean_100: 184.93, episodes: 339\n",
      "38390: reward: 200.00, mean_100: 186.12, episodes: 340\n",
      "38590: reward: 200.00, mean_100: 186.83, episodes: 341\n",
      "38608: reward:  18.00, mean_100: 185.01, episodes: 342\n",
      "38726: reward: 118.00, mean_100: 184.19, episodes: 343\n",
      "38926: reward: 200.00, mean_100: 184.19, episodes: 344\n",
      "39126: reward: 200.00, mean_100: 184.19, episodes: 345\n",
      "39149: reward:  23.00, mean_100: 182.42, episodes: 346\n",
      "39339: reward: 190.00, mean_100: 182.93, episodes: 347\n",
      "39454: reward: 115.00, mean_100: 182.08, episodes: 348\n",
      "39648: reward: 194.00, mean_100: 182.02, episodes: 349\n",
      "39835: reward: 187.00, mean_100: 181.89, episodes: 350\n",
      "40013: reward: 178.00, mean_100: 181.67, episodes: 351\n",
      "40192: reward: 179.00, mean_100: 183.17, episodes: 352\n",
      "40372: reward: 180.00, mean_100: 184.27, episodes: 353\n",
      "40482: reward: 110.00, mean_100: 183.37, episodes: 354\n",
      "40681: reward: 199.00, mean_100: 183.36, episodes: 355\n",
      "40878: reward: 197.00, mean_100: 183.33, episodes: 356\n",
      "41059: reward: 181.00, mean_100: 183.14, episodes: 357\n",
      "41245: reward: 186.00, mean_100: 183.00, episodes: 358\n",
      "41445: reward: 200.00, mean_100: 183.00, episodes: 359\n",
      "41633: reward: 188.00, mean_100: 183.82, episodes: 360\n",
      "41833: reward: 200.00, mean_100: 183.82, episodes: 361\n",
      "42033: reward: 200.00, mean_100: 183.82, episodes: 362\n",
      "42219: reward: 186.00, mean_100: 183.68, episodes: 363\n",
      "42315: reward:  96.00, mean_100: 182.64, episodes: 364\n",
      "42515: reward: 200.00, mean_100: 182.64, episodes: 365\n",
      "42714: reward: 199.00, mean_100: 182.63, episodes: 366\n",
      "42911: reward: 197.00, mean_100: 182.60, episodes: 367\n",
      "42995: reward:  84.00, mean_100: 181.44, episodes: 368\n",
      "43195: reward: 200.00, mean_100: 181.44, episodes: 369\n",
      "43395: reward: 200.00, mean_100: 181.44, episodes: 370\n",
      "43595: reward: 200.00, mean_100: 181.44, episodes: 371\n",
      "43795: reward: 200.00, mean_100: 181.44, episodes: 372\n",
      "43991: reward: 196.00, mean_100: 181.40, episodes: 373\n",
      "44191: reward: 200.00, mean_100: 181.40, episodes: 374\n",
      "44388: reward: 197.00, mean_100: 181.37, episodes: 375\n",
      "44405: reward:  17.00, mean_100: 179.54, episodes: 376\n",
      "44605: reward: 200.00, mean_100: 179.54, episodes: 377\n",
      "44805: reward: 200.00, mean_100: 179.54, episodes: 378\n",
      "44844: reward:  39.00, mean_100: 177.93, episodes: 379\n",
      "45044: reward: 200.00, mean_100: 177.93, episodes: 380\n",
      "45244: reward: 200.00, mean_100: 177.93, episodes: 381\n",
      "45425: reward: 181.00, mean_100: 177.74, episodes: 382\n",
      "45625: reward: 200.00, mean_100: 177.74, episodes: 383\n",
      "45825: reward: 200.00, mean_100: 177.74, episodes: 384\n",
      "46025: reward: 200.00, mean_100: 177.74, episodes: 385\n",
      "46225: reward: 200.00, mean_100: 177.74, episodes: 386\n",
      "46425: reward: 200.00, mean_100: 177.74, episodes: 387\n",
      "46625: reward: 200.00, mean_100: 177.74, episodes: 388\n",
      "46825: reward: 200.00, mean_100: 177.74, episodes: 389\n",
      "47025: reward: 200.00, mean_100: 177.74, episodes: 390\n",
      "47225: reward: 200.00, mean_100: 177.78, episodes: 391\n",
      "47425: reward: 200.00, mean_100: 177.78, episodes: 392\n",
      "47625: reward: 200.00, mean_100: 177.78, episodes: 393\n",
      "47825: reward: 200.00, mean_100: 177.78, episodes: 394\n",
      "48025: reward: 200.00, mean_100: 178.02, episodes: 395\n",
      "48225: reward: 200.00, mean_100: 178.02, episodes: 396\n",
      "48425: reward: 200.00, mean_100: 178.02, episodes: 397\n",
      "48625: reward: 200.00, mean_100: 178.02, episodes: 398\n",
      "48825: reward: 200.00, mean_100: 178.02, episodes: 399\n",
      "49025: reward: 200.00, mean_100: 178.02, episodes: 400\n",
      "49225: reward: 200.00, mean_100: 178.02, episodes: 401\n",
      "49425: reward: 200.00, mean_100: 178.02, episodes: 402\n",
      "49625: reward: 200.00, mean_100: 178.02, episodes: 403\n",
      "49825: reward: 200.00, mean_100: 178.02, episodes: 404\n",
      "50025: reward: 200.00, mean_100: 178.02, episodes: 405\n",
      "50225: reward: 200.00, mean_100: 178.02, episodes: 406\n",
      "50425: reward: 200.00, mean_100: 178.02, episodes: 407\n",
      "50625: reward: 200.00, mean_100: 178.02, episodes: 408\n",
      "50825: reward: 200.00, mean_100: 178.02, episodes: 409\n",
      "51025: reward: 200.00, mean_100: 178.02, episodes: 410\n",
      "51225: reward: 200.00, mean_100: 178.02, episodes: 411\n",
      "51425: reward: 200.00, mean_100: 178.02, episodes: 412\n",
      "51625: reward: 200.00, mean_100: 178.02, episodes: 413\n",
      "51825: reward: 200.00, mean_100: 178.02, episodes: 414\n",
      "52025: reward: 200.00, mean_100: 178.02, episodes: 415\n",
      "52225: reward: 200.00, mean_100: 178.02, episodes: 416\n",
      "52425: reward: 200.00, mean_100: 178.02, episodes: 417\n",
      "52625: reward: 200.00, mean_100: 178.05, episodes: 418\n",
      "52825: reward: 200.00, mean_100: 178.05, episodes: 419\n",
      "53025: reward: 200.00, mean_100: 178.05, episodes: 420\n",
      "53225: reward: 200.00, mean_100: 178.05, episodes: 421\n",
      "53425: reward: 200.00, mean_100: 178.05, episodes: 422\n",
      "53625: reward: 200.00, mean_100: 178.05, episodes: 423\n",
      "53825: reward: 200.00, mean_100: 178.05, episodes: 424\n",
      "54025: reward: 200.00, mean_100: 178.05, episodes: 425\n",
      "54225: reward: 200.00, mean_100: 178.74, episodes: 426\n",
      "54425: reward: 200.00, mean_100: 178.74, episodes: 427\n",
      "54625: reward: 200.00, mean_100: 179.70, episodes: 428\n",
      "54825: reward: 200.00, mean_100: 179.70, episodes: 429\n",
      "55025: reward: 200.00, mean_100: 180.98, episodes: 430\n",
      "55225: reward: 200.00, mean_100: 180.98, episodes: 431\n",
      "55425: reward: 200.00, mean_100: 180.98, episodes: 432\n",
      "55625: reward: 200.00, mean_100: 182.79, episodes: 433\n",
      "55825: reward: 200.00, mean_100: 182.79, episodes: 434\n",
      "56025: reward: 200.00, mean_100: 182.79, episodes: 435\n",
      "56225: reward: 200.00, mean_100: 184.62, episodes: 436\n",
      "56425: reward: 200.00, mean_100: 186.35, episodes: 437\n",
      "56625: reward: 200.00, mean_100: 186.35, episodes: 438\n",
      "56825: reward: 200.00, mean_100: 186.35, episodes: 439\n",
      "57025: reward: 200.00, mean_100: 186.35, episodes: 440\n",
      "57225: reward: 200.00, mean_100: 186.35, episodes: 441\n",
      "57425: reward: 200.00, mean_100: 188.17, episodes: 442\n",
      "57625: reward: 200.00, mean_100: 188.99, episodes: 443\n",
      "57825: reward: 200.00, mean_100: 188.99, episodes: 444\n",
      "58025: reward: 200.00, mean_100: 188.99, episodes: 445\n",
      "58225: reward: 200.00, mean_100: 190.76, episodes: 446\n",
      "58425: reward: 200.00, mean_100: 190.86, episodes: 447\n",
      "58625: reward: 200.00, mean_100: 191.71, episodes: 448\n",
      "58825: reward: 200.00, mean_100: 191.77, episodes: 449\n",
      "59025: reward: 200.00, mean_100: 191.90, episodes: 450\n",
      "59225: reward: 200.00, mean_100: 192.12, episodes: 451\n",
      "59425: reward: 200.00, mean_100: 192.33, episodes: 452\n",
      "59625: reward: 200.00, mean_100: 192.53, episodes: 453\n",
      "59825: reward: 200.00, mean_100: 193.43, episodes: 454\n",
      "60025: reward: 200.00, mean_100: 193.44, episodes: 455\n",
      "60225: reward: 200.00, mean_100: 193.47, episodes: 456\n",
      "60425: reward: 200.00, mean_100: 193.66, episodes: 457\n",
      "60625: reward: 200.00, mean_100: 193.80, episodes: 458\n",
      "60825: reward: 200.00, mean_100: 193.80, episodes: 459\n",
      "61025: reward: 200.00, mean_100: 193.92, episodes: 460\n",
      "61225: reward: 200.00, mean_100: 193.92, episodes: 461\n",
      "61425: reward: 200.00, mean_100: 193.92, episodes: 462\n",
      "61625: reward: 200.00, mean_100: 194.06, episodes: 463\n",
      "61825: reward: 200.00, mean_100: 195.10, episodes: 464\n",
      "Solved in 61825 steps and 464 episodes!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agente = agent.PolicyAgent(net, preprocessor= agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = experience.ExperienceSourceFirstLast(env, agente, gamma=GAMMA) # iterator\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            #writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            #writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            #writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "\n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparación con DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente imagen tomada del texto del Maxi Lapan, página 292, se compara en condicones similares el comportamiento d elos algoritmos DQN i REINFORCE. Los datos por supuesto dependen de los valres pseudo aleatorios inicilaes, pero claramente el problema es resuleto mucho más rápido con REINFORCE. Esto se conoce en la literatura y ha sido  la motivación de estudiar el método. En la siguiente sección hacemos una ampliación en la cual se trata de resolver el problema de la varianza introducida en el gradiente de política por el retorno $G_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/DQN_Reinforce.png\"  width=\"400\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de política v.s. Métodos basados en valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de política son una generalización del método de entropía cruzada.\n",
    "\n",
    "+ Los métodos de política optimizan directamente la política. Los  métodos de valor, como DQN, hacen lo mismo indirectamente.\n",
    "+ Los métodos de política están basados totalmente en la política y requieren muestras frescas del ambiente. Los métodos de valor pueden beneficiarse de datos antiguos, obtenidos de la vieja política y otras fuentes.\n",
    "+ Los métodos de política suelen ser menos eficientes en la muestra, lo que significa que requieren mayor interacción con el entorno. Los métodos de valor pueden beneficiarse de grandes búferes de reproducción (replay buffer). Sin embargo, la eficiencia de la muestra no significa que el valor Los métodos son más eficientes desde el punto de vista computacional y, muy a menudo, es todo lo contrario.\n",
    "+ En el ejemplo anterior, durante el entrenamiento, necesitábamos acceder a nuestro RN una sola vez, para obtener las probabilidades de las acciones. En DQN, necesitamos procesar dos lotes de estados: uno para el estado actual y otro para el estado siguiente en la actualización de Bellman.\n",
    "\n",
    "En general hay situaciones en donde DQN es más natural. >Por ejmplo, las soluiciones del estado del arte para juegos de Atari son variciones de DQN. En constraste los métodos de política son mejor aplicados en problemas de control contínuo o en casos en donde el acceso al ambiente de barato y rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Refuerzo con línea base </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ya mencionamos antes que nuestra aproximación del gradiente de política dada por $\\nabla J \\approx \\mathbb{E}[Q(s,a)\\nabla \\pi(s|a)]$ es proporcional a la recompensa descontada desde un estado dado. Esta recompensa depende por supeuesto del ambiente e introduce una fuente importante de variación. Una solución a este incoveniente substraer de la recompensa una contante. Posibles solucioness son restar de la recompensa\n",
    " \n",
    "+ La media de las recompensas descontadas.\n",
    "+ La media móvil de las recompensas descontadas.\n",
    "+ El valor del estado V(s).\n",
    "\n",
    "La última solución es introducida en la lección actor-crítico.\n",
    "\n",
    "Por otro lado para recortar los episodios y no hace cálculos innecesarios se puede determinar a parir de que momento el factor de decuento es tan pequeño que más pasos en el episodio no aportan mucho al cáculo de la recompensa dscontada. Por ejemplo, $0.9^{50}= 0.005$. Entonces si el factor de descuento es $\\gamma=0.9$, quizás sea suficiente parar después de 50 pasos en un episodio.\n",
    "\n",
    "\n",
    "La introducción de un constante que reste a la recompensa descontada se basa en el siguiente hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verificarse que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a_t|s_t) \\right) \\right]=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, bajo el supuesto que la integral y el gradiente pueden intercambiarse, se tiene que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a|s) \\right) \\right] =\\int \\pi_{\\theta}(a|s)\\nabla \\log \\pi_{\\theta}(a|s)d\\tau =  \\int \\nabla \\pi_{\\theta}(a|s)d\\tau= \\nabla\\int  \\pi_{\\theta}(a|s)d\\tau = \\nabla  1 = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La verificación meticulosa, se  deja como ejercicio al lector interesado. Sea $b$ una variable que no depende de la trayectoria $\\tau$.  Esta variable se denominará `línea base`. Debido al resultado anterior se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}(G_t-b)\\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right].\n",
    "$$\n",
    "\n",
    "Usando $b$ en teoría y en la práctica la varianza puede ser reducida, manteniendo el gradiente de la política insesgado. Un bune valor que puede usarse como línea base el valor del estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de implementación de Gradiente de política con línea base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(old: Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    reward_sum = 0.0\n",
    "    bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        reward_sum += exp.reward\n",
    "        baseline = reward_sum / (step_idx + 1)\n",
    "        #writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            #writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            #writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            #writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc KL-div\n",
    "        new_logits_v = net(states_v)\n",
    "        new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "        #writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "        grad_max = 0.0\n",
    "        grad_means = 0.0\n",
    "        grad_count = 0\n",
    "        for p in net.parameters():\n",
    "            grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "            grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "            grad_count += 1\n",
    "\n",
    "        bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))\n",
    "        entropy = smooth(entropy, entropy_v.item())\n",
    "        l_entropy = smooth(l_entropy, entropy_loss_v.item())\n",
    "        l_policy = smooth(l_policy, loss_policy_v.item())\n",
    "        l_total = smooth(l_total, loss_v.item())\n",
    "        \"\"\"\n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        writer.add_scalar(\"entropy\", entropy, step_idx)\n",
    "        writer.add_scalar(\"loss_entropy\", l_entropy, step_idx)\n",
    "        writer.add_scalar(\"loss_policy\", l_policy, step_idx)\n",
    "        writer.add_scalar(\"loss_total\", l_total, step_idx)\n",
    "        writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "        writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "        writer.add_scalar(\"batch_scales\", bs_smoothed, step_idx)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()\n",
    "\n",
    "    #writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
