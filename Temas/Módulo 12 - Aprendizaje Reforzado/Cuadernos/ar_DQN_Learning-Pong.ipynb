{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Deep Q-Learning para entrenar un jugador de Atari</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Entrenamiento de una agente para jugar Pong</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Envolturas (Wrappers)](#Envolturas-(Wrappers))\n",
    "* [Modelo DQN](#Modelo-DQN)\n",
    "\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [Experiencia por repetición](#Experiencia-por-repetición)\n",
    "* [Congelando la red objetivo](#Congelando-la-red-objetivo)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Ejemplo ambiente CartPole-v1 ](#Ejemplo-ambiente-CartPole-v1-en-Gym)\n",
    "* [Implementación del algoritmo DQN. CartPole](#Implementación-del-algoritmo-Q-learning.-CartPole)\n",
    "* [Video luego de entrenado el agente](#Video-luego-de-entrenado-el-agente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lección entrenaremos una agente cpaza de jugar y ganar uno delos priemwros video-juegos de la historia: el juego de Atari Pong. La pri3mra versión fue introducida por Mnih et al. en un artículo de 2015 publicado por Nature como [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). \n",
    "\n",
    "El problema de atacar los juegos de Atari con aprendizaje reforzado (AR) presenta varias complicaciones desde el punto de vista de los recursos que es necesario resolver. Primero es espacio de estados es finito.\n",
    "\n",
    "Para empezar, se  tiene una grilla de una tamaño por ejemplo de 210\\*160 pixels a color. Esto siginifica que a primera vista el agente puede recibir  una imagen completa del estado actual del ambiente representado en un tensor de tamaño 210\\*160\\*3. \n",
    "\n",
    "En segudo lugar, de un paso de tiempo al siguiente no hay un cambio significativo en la imagen. Esto implica que si se considera cada imagen de forma separada en el entrenamiento, no es fácil reconocer la dinámica del ambiente.\n",
    "\n",
    "Por estas razones los científicos de AR, empezando con Minh et. al., usualmente hacen transformaciones a la interface del juego para resolver estos problemas. La plataforma OpenAP-Gym dispone de un ambiente para los juegos de Atari. Gracias a ello, los científicos utilizan las envolturas (Wrappers) para implementar tales transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Envolturas (Wrappers)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones más conocidas por os científicos del AR  son la siguientes.\n",
    "\n",
    "1. Convertir cada vida en un episodio. Clásicamente un episodio termina cuando el juego termina (*game over*). Pero para entrenar al agente es suficiente usar como episodio cada vida.\n",
    "1. Al comienzo del juego se ejeucta un salto de un número aleatorio de acciones. Esto permite evitar algunas escenas introductorias de los juegos de Atari.\n",
    "1. Cada selección de la siguiente acción se hace cada $K$ pasps (frames). Usualmente $K$ es 3 o 4. Para cada frame intermedio la aación simplemente es repetida. Esta estrategia acelera significativamente el entrenamiento. Tomar cada frame individualmente, es un poco más exigente en recursos, pero la diferencia entre frames consecutivos es por lo general pequeña.\n",
    "1. Se toma el máximo de cada pixel en los últimos dos frames y se usa como una observación. El problema que se resuelve en este caso, es que algunos juegos de Atari tienen un efecto de parpadeo, debido a algunas limitaciones de la plataforma de la época. Por lo general el efecto es invisible al ojo humano, pero puede confundir a la red neuronal.\n",
    "1. Presionar **FIRE** al comienzo del juego. olgunos juegos incluido Pong, requieren que el usuario p1. resione **FIRE** al comienos del juego.\n",
    "1. Reescalar los frame a  tamaño 84\\*84 a u solo color (escala de grises).\n",
    "1. Acumular varios frames seguidos (sualmente 4) para tratarlos como uso solo. Esta estrategia permite detectra mejo la dimpanica del juego.\n",
    "1. Establecer las recompsensas en -1, 0, 1. El sistema de recompensas varia enlos diferentes juegos. En Pong, se entrega 1, cada vez que el oponente deja pasar la bola, -1, cada vez que el agente deja pasar la bola y 0 en el resto del juego.\n",
    "1. Covertir la observaciones ala escala $[0, 1]$ para evitar problemas  a la red enel entreanamiento. Valores entre 0 y 255 son problemáticos en general.\n",
    "\n",
    "La siguiente implementación de las envoluturas par el juego Pong es adaptada del texto de [Maxim Lapan](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67).\n",
    "\n",
    "Las clases estan diseñadas para modificar el ambiente, implementando las transformaciones descritas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase  FireResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementa presionar **FIRE**, requerido en Pong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase MaxAndSkipEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permite recibir varios frames,  4 por defecto, mediante al utilización de la misma acción. Se conservan únicamente los dos últimos frames en la cola (collections.deque). La nueva observación consiste en tomar el máximmo  (max pooling) por cada pixel de los dos frame conservados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ProcessFrame84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma la observación al tamaño 84\\*84\\*1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ImageToPyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma el la observación de tipo (Ancho, alto, canal) -> (canal, ancho, alto) requirido por Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ScaledFloatFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transformar la observación a la escala $[0,1]$ para evitar problemas de convergencia de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ScaledFloatFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESta es la clase que crea la pila de observaciones que finalmente es la observación que le llega al agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None)\n",
    "        \"\"\"imula presionar FIRE para iniciar el juego.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Retorna solamente cada  `skip`-ésimo frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # conserva las más recientes obs( 2 para max pooling a lo largo de los pasos en el tiempo)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"limpia el pasado buffer e incializa para la priemra observación desde \n",
    "        el env interno.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(  np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(   np.float32)\n",
    "        else:\n",
    "            assert False, \"Resolución desconocida.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + \\\n",
    "              img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=new_shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            old_space.low.repeat(n_steps, axis=0),\n",
    "            old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(\n",
    "            self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modelo DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra la implementación de la red que se usará para el entrenamiento del agente. La diferencia con la lección de CartPole es que en esta caso, tenem osun  red convolucional de regresión. Como antes, la entrada a la red es una observación y a la salida los valores de las acciones para la siguiente acción dada la obssrvación de entrada.\n",
    "\n",
    "Lo único por explicar en estre caso es la razón por la cual separamos la arquitectura en dos partes. Esto se debe a que a diferencia de Keras, en Pytorch no existe una capa Flatten. Entonces a la salida de la parte convolucional se  usa *view* para tener una vista del tensor de entreda como un vector plano y luegonetregarlo ala capa densa (perceptrón)-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algortimo Q-Learning profundo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnih et al. introdujeron el algoritmo Deep Q-Network o DQN, publicado en Nature en 2015 como [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). Este algoritmo combina el algoritmo *Q-learning* con redes neuronales profundas (Deep Neural Networks). \n",
    "\n",
    "Como es sabido en el campo de la IA, las redes neuronales son una fantástica manera de aproximar funciones no lineales. Este algoritmo usa una red neuronal para aproximar la función *Q*, evitando así utilizar una tabla para representar la misma. \n",
    "\n",
    "En realidad, utiliza dos redes neuronales para estabilizar el proceso de aprendizaje. \n",
    "\n",
    "1. La primera, la *red neuronal principal* (main Neural Network), representada por los parámetros $\\theta$, se utiliza para estimar los valores-Q del estado s y acción a actuales: $Q(s, a;\\theta)$. \n",
    "2. La segunda, la *red neuronal objetivo* (target Neural Network), parametrizada por $\\theta^{'}$, tendrá la misma arquitectura que la red principal pero se usará para aproximar los *valores-Q* del siguiente estado $s'$ y la siguiente acción $a'$. \n",
    "\n",
    "El aprendizaje ocurre en la red principal y no en la objetivo.\n",
    "\n",
    "La red objetivo se congela (sus parámetros no se cambian) durante varias iteraciones, normalmente alrededor de 10000. Luego de transcurrido ese número de iteraciones, los parámetros de la red principal se copian a la red objetivo, transmitiendo así el aprendizaje de una a otra, haciendo que las estimaciones calculadas por la red objetivo sean más precisas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos requerido para entrenar la Q-network provienen de  la experiencia del agente: $(s_0 a_0 r_1 s_1, s_1 a_1 r_2  s_2,\\ldots, s_{T-1} a_{T-1} r_T s_T)$. Cada muestra de entrenamiento es una unidad de experiencia, $s_t a_t r_{t+1} s_{t+1}$.\n",
    "\n",
    "En el tiempo $t$ se tiene el estado $s=s_t$.  La acción $a=a_t$ es determinada usando el algoritmo Q-learning, en donde la selección de la acción es de tipo epsilon-greedy., es decir, la siguiente acción, digamos $\\pi(a)$ se obtiene como:\n",
    "\n",
    "$$\n",
    "\\pi(a) = \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ecuación de Bellman en DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación de Bellman en el caso DQN se expresa como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a; \\theta) = r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar una red neuronal, necesitamos una función de pérdida o coste (loss or cost function), la cual definimos como el cuadrado de la diferencia entre ambos lados de la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "L(\\theta)= \\mathbb{E}[  r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})- Q(s,a; \\theta)]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta será la función que minimizaremos usando el algoritmo de descenso de gradiente (gradient descent), el cuál se ejecuta automáticamente si usamos una librería de diferenciación automática con redes neuronales, como TensorFlow o Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el punto de vista del algoritmo,  se usa la $Q$-network para predecir el valor $Q$ de cada posible siguiente acción, dado el estado siguiente y escogiendo el máximo entre ellos, como henos hecho antes. \n",
    "\n",
    "En el caso en el cual el estado es terminal se tiene que $\\max_{a^{'}}  Q(s^{'},a^{'})=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de la ecuación de Bellman en DQN: diferencia temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre la parte izquierda y la parte derecha de la ecuacipon de Bellman es denominada diferencia temporal. Si, $\\Delta$ denota tal diferencia se tiene que\n",
    "\n",
    "$$\n",
    "\\Delta_{s,a} = Q(s,a; \\theta) - (r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})).\n",
    "$$\n",
    "\n",
    "Esta cantidad $\\Delta$ representa la diferencia entre  el Q-valor actual $Q(s,a)$ y el nuevo Q-valor que se obtiene al ejecutar la acción $a$ y alcanzar el nuevo estado $s'$.\n",
    "\n",
    "Lo que se hace se espera es que luego de muchos pasos de experiencia acumulada, esta diferencia sea muy pequeña. En otras palabras, que el agente haya encontrado una buena aproximación de la política optimal para el ambiente. \n",
    "\n",
    "Lo que se busca entonces es minimizar la esperanza de $\\Delta^2$, sobre espacio de estados por acciones.\n",
    "\n",
    "\n",
    "Para la implementación en el algortimo se hace lo siguiente:\n",
    "\n",
    "* La parte izquierda de la  ecuación, es decir, $Q(s,a; \\theta)$ se calcula con la red principal, es decir la red que estamos entrenando.\n",
    "* La parte derecha de la ecuación, es decir, $r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})$ se calcula con la red objetivo (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Experiencia por repetición</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general el entrenamiento de la Q-network es inestable. Hay dos causas de la instabilidad\n",
    "\n",
    "1. Alta correlación entre las muestras.\n",
    "2. El target no es estacionario.\n",
    "\n",
    "Para resolver el problema de la alta correlación, los datos de entrenamiento son seleccionados aleatorios de un buffer que creamos para tal fín. Este proceso se conoce como experience replay (experiencia por repetición)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Congelando la red objetivo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL problema de no estacionariedad de la red objetivo (target) es resuelto congelando sus pesos durante $C$ pasos de  entrenamiento. En realidad, esta es la razón de tener dos redes con idéntica arquitectura. Los parámetros de la Q-network target son copiados desde la Q-network principal cada $C$ pasos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo DQN </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo DQN es de tipo epsilon-greedy. \n",
    "\n",
    "**Primero se crean los objetos necesarios y se inicializan.**\n",
    "\n",
    "- Inicializar la memory  replay (buffer) *D* con capacidad *N*.\n",
    "- Inicializar la función de acción-valor $Q_{main}$ con pesos aleatorios $\\theta$\n",
    "- Inicializar la función acción-valor target $Q_{\\text{target}}$ con pesos con pesos $\\theta^{-}= \\theta$\n",
    "- Definir la rata de exploración inicial $\\epsilon$ y el factor de descuento, $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ciclo de aprendizaje**\n",
    "\n",
    "1. Para $\\text{ episodio } = 1,\\ldots, M$, do:\n",
    "2.        Dado el estado incial s\n",
    "3.        Para step = 1,...,T haga:\n",
    "4.            Escoja la acción (epsilon-greddy)\n",
    "$$\n",
    "a \\gets \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$\n",
    "5. Ejecuta la acción *a* observe la recompensa *r* y el siguiente estado *s'*\n",
    "6. Almacene la transición $(s,a,r,s')$ en el buffer D\n",
    "7. Actualice el estado $s=s'$\n",
    "\n",
    "**Experiencia por repetición**\n",
    "\n",
    "8.Tome un mini-lote de muestra de experiencias de episodios $(s_j,a_j, r_{j+1}, s_{j+1})$ desde el buffer *D*.\n",
    "\n",
    "9. $$\n",
    "Q_{\\text{max}} = \\begin{cases} r_{j+1} &\\text{ si el episodio termina en } j+1\\\\\n",
    "r_{j+1} + \\gamma \\max_{a_{j+1}} Q_{\\text{target}}(s_{j+1},a_{j+1};\\theta ^{-}) &\\text{ en otro caso }\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "10. Ejecuta un paso de gradiente descendiente para la función de pédida $\\left(Q_{\\text{max}}- Q(s_j,a_j;\\theta) \\right)^2$ con respecto a los parámetros\n",
    "\n",
    "**Actualización periódica de la red objetivo(target)**\n",
    "\n",
    "11. Cada $C$ pasos, $Q_{\\text{target}}=Q_{main}$, es decir $\\theta^{-} = \\theta$\n",
    "12. fin\n",
    "13. fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementación del algoritmo DQN. Atari-Pong</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección implementamos el Algortimo DQN. Usaremos el ambiente Pong (\"PongNoFrameskip-v4\"), como ejemplo, aunque el código es bastante genérico. Se presenta la implementación en Pytorch y se deja como ejercio la implementación en Tensorflow.\n",
    "\n",
    "El código puede ser un poco lento en su máquina local, si no dispone de GPU's. Si eso ocurre ejecútelo en Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tupla nombrada Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tupla nombrada es para organizar los datos que irán al buffer de experiencia. Sus elementos son (estado, acción, recompensa, hecho y nuevo estado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase ExperienceBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta clase se crea y manipula el buffer que mantiene la experiencia adquirida por el agente a medida que explora el ambiente. El buffer es una cola de tipo *deque*. \n",
    "\n",
    "- El constructor *\\_\\_init\\_\\_()* crea el buffer de tamaño máximo *capacity*.\n",
    "- El método *append()* agrega elementos al buffer.\n",
    "- El método *sample()* extare una muestra aleatoria del buffer con tamaño *batch_size*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementa al agente.\n",
    "\n",
    "- El constructor *\\_\\_init\\_\\_()* referencia localmente una ambiente y el el buffer de experiencia. reincia el ambiente.\n",
    "- El método privado *\\_reset()* reinicia el ambiente y la recompensa total de un episodio.\n",
    "- El métod *play_step()* ejecuta un paso de experiencia: selecionauna acción usando la técnica epsilon-greddy, le informa al ambiente, recibe la retroalimentación del ambienete y sube la experiencia la buffer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función calc_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula la función de pérdida MSE con base en la diferencia temporal explicada arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# namedtuple para cosntruir la tupla (estado, acción, recompensa, hecho y nuevo estado)\n",
    "Experience = collections.namedtuple(\n",
    "    'Experience', field_names=['state', 'action', 'reward',\n",
    "                               'done', 'new_state'])\n",
    "\n",
    "# Clase para manipular  el buffer de experiencia\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), \\\n",
    "               np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), \\\n",
    "               np.array(next_states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward,\n",
    "                         is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "\n",
    "# función de pérdida\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(np.array(\n",
    "        states, copy=False)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(\n",
    "        next_states, copy=False)).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(\n",
    "        1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + \\\n",
    "                                   rewards_v\n",
    "    return nn.MSELoss()(state_action_values,\n",
    "                        expected_state_action_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.0\n",
    "\n",
    "\n",
    "# define el dispositovo que usará\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# define el ambiente\n",
    "env = wrappers.make_env(args.env)\n",
    "\n",
    "# Crea las dos redes\n",
    "net = dqn_model.DQN(env.observation_space.shape,\n",
    "                    env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape,\n",
    "                        env.action_space.n).to(device)\n",
    "# inicia el SummaryWriter\n",
    "writer = SummaryWriter(comment=\"-\" + args.env)\n",
    "print(net)\n",
    "\n",
    "# Crea el buffer de experiencia\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "\n",
    "# Crea el agente\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "# define le epsilon inicial para la política epsilon-voráz\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "# instancia el optimizador Adam\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Inicializa objetos de trabajo\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None\n",
    "\n",
    "# ciclo de entrenamiento\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START -\n",
    "                  frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, reward %.3f, \"\n",
    "              \"eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), m_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), args.env +\n",
    "                       \"-best_%.0f.dat\" % m_reward)\n",
    "            if best_m_reward is not None:\n",
    "                print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                    best_m_reward, m_reward))\n",
    "            best_m_reward = m_reward\n",
    "        if m_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
