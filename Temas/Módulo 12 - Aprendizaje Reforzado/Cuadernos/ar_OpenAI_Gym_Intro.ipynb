{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción a OpenAI Gym</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dca78",
   "metadata": {},
   "source": [
    "<center>OpenAI Gym</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5846fe",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d09f7",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072351c",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b23f0c",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0a794",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc4de4",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Lista de ambientes en OpenAI Gym](#Lista-de-ambientes-en-OpenAI-Gym)\n",
    "* [Agente Aleatorio CartPole](#Agente-Aleatorio-CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1b5d79-17b8-4d84-b938-c6353b5b3467",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">La API OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa5bf5-614f-4420-a33e-81936045c7a0",
   "metadata": {},
   "source": [
    "La API `OpenAI Gym` tiene una rica colección de ambientes para experimentos de aprendizaje reforzado (AR), usando una interfaz unificada.\n",
    "\n",
    "La clase principal de Gym es *Env* (de environment). Sus métodos y atributos proveen la información necesaria para poder entrenar agentes que interactúen con el medio ambientes. Las piezas más importante disponibles con *Env* son:\n",
    "\n",
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar un acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*).\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e5fb-ca34-4653-9f3e-c709d652dd81",
   "metadata": {},
   "source": [
    "### El espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157683-e2d3-43ff-8f29-7bd14309fffe",
   "metadata": {},
   "source": [
    "Las acciones que puede realizar un agente puede ser discretas, continuas o incluso una combinación de ambas. Por ejemplo en un automóvil manejado de manera autónoma puede ser posible oprimir varios botones a la vez (discreto), oprimir el pedal del acelerador o del freno (continuo) y así. El espacio de acciones contiene o modela todas las posibilidades de acción del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ad16b-e42c-4957-8524-5476670451bd",
   "metadata": {},
   "source": [
    "### El espacio de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1305-62f4-4dd5-91a4-fe68ce77191e",
   "metadata": {},
   "source": [
    "Las observaciones son piezas de información que el ambiente puede entregar al agente en cualquier momento del tiempo, además de la recompensa. Las observaciones pueden ser muy simples como un arreglo unidimensional de números o  complejos tensores, como las imágenes provenientes de varias cámaras del automóvil.\n",
    "\n",
    "Es claro que acciones y observaciones tienen similaridades, por lo que Gym dispone de una clase abstracta llamada *Space* que permite derivar espacios de acciones y espacios de observaciones. La siguiente imagen muestra una diagrama de la clase *Space*.\n",
    "\n",
    "Existen otras subclases de *Space*, pero estas son las más comúnmente utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374739ea-141b-4a29-89e1-2ef87bed6b80",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/jerarquia_clase_Space_Gym.jpeg\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Jerarquía de la clase Space de OpenAi Gym. Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc147e-8c2f-4028-ad45-aa897b0fd0fb",
   "metadata": {},
   "source": [
    "La clase abstracta *Space* incluye dos métodos relevantes:\n",
    "\n",
    "* *sample()*: retorna una muestra aleatoria del espacio.\n",
    "* *contains(x)*: chequea su el argumento es un elemento del dominio del espacio.\n",
    "\n",
    "Ambos métodos son abstractos y deben ser implementados en las subclases de *Space*.\n",
    "\n",
    "* La clase *Discrete* representa un conjunto de ítems mutuamente excluyentes, numerados entre 0 y $n-1$. Su único atributo *n*, es una cuenta de los ítems que la clase describe. Por ejemplo. *Discrete(n=4)* puede representar por ejemplo un espacio de acciones de cuatro direcciones para moverse: *[left, right, up, down]*.\n",
    "\n",
    "* La clase *Box* representa un tensor *n*-dimensional de números racionales con intervalos *[low, high]*. Por ejemplo puede representar el pedal de un acelerador con un valor simple entre 0.0 y 1.0. En pedal sería codificado en tal caso como *Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)*. Otro ejemplo puede ser la observación de la pantalla en un juego Atari, la cual es RGB de tamaño $210\\times 160$: *Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)*.\n",
    "\n",
    "* La última hija de *Space* es *Tuple*, la cual permite combinar varias instancias de *Space* juntas. esta subclase permite crear espacios de acciones y observaciones de la complejidad que se desee.\n",
    "\n",
    "Un ejemplo del uso de tupla puede ser el siguiente. Supongamos que deseamos modelar algunos controles de un automóvil. Para empezar la dirección (el ángulo de la direeción, en realidad), el acelrador y el freno pueder cambiar en cada instante de tiempo. Por otro lado, podemos tener botones discretos para representar la luces direccionales *(apagadas, izquierda, derecha)* y la bocina *(encendida, apagada)*. Entonces, podemos crear *Tupla(spaces=(Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(3), Discrete(2)\n",
    ")*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b93b-c965-4151-9465-f48ffc6d15e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### El ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234d15-59fb-48f8-b091-8a19fc0bfc01",
   "metadata": {},
   "source": [
    "El ambiente es representado por la clase *Env*. Todo ambiente tiene dos miembros de tipo *Space*: \n",
    "\n",
    "* *action_space*: acciones permitidas que entrega el ambiente al agente.\n",
    "* *observation_space*: observaciones proveidas por el ambiente al agente.\n",
    "\n",
    "Además el ambiente tiene los siguientes miembros:\n",
    "\n",
    "* *reset()*: coloca el ambiente en el estado inicial.\n",
    "* *step()*: Permite al agente tomar una acción y retorna información como respuesta a la acción: la siguiente observación, la recompensa y la bandera indicando si el episodio ha  finalizado. Esta es la pieza central de la funcionalidad del ambiente.\n",
    "\n",
    "Esto permite crear código genérico que podría trabajar con cualquier ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77f45f-bca5-4224-a32e-74486611c682",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Lista de ambientes en OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbaf73-c251-4590-b9c0-0ef891d837a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(*env_ids, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fea79-4bca-4a0e-800c-38a8c40dc71b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Una sesión con CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c5078-0287-4a92-af21-9fd0d6cca4a2",
   "metadata": {},
   "source": [
    "Fuente [OpenAI Gym](https://gym.openai.com/)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"200\" height=\"200\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "    \n",
    "Este es nuestro primer acercamiento a OpenAI Gym. En este primer  ejemplo conocido como CartPole es un clásico en la teoría de control. En la animación *gif* a la izquierda  observa el objeto de interés. Es una palo, sostenido en la parte inferior por un pasador unido a una base. Ignoramos posibles complicaciones como la fricción. Es decir suponemos que el palo se mueve libremente sobre el pasador. \n",
    "\n",
    "\n",
    "Obviamente el palo puede caer muy fácilmente, por lo que el propósito es mover la base hacia la izquierda o hacia la derecha para evitar que el palo caiga. Ese es el problema a resolver. El problema puede ser abordado como un problema de control que lleva a diseñan un modelo en ecuaciones diferenciales. Pero en aprendizaje reforzado buscamos resolver el problema, sin utilizar los conceptos de la física correspondientes. De momento vamos a exploras un poco el  ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f22f8d-a004-4d38-8898-74b10c0c850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01420031,  0.02282784, -0.02182518, -0.00839914])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploración del ambiente CartPole\n",
    "import gym\n",
    "\n",
    "# crea una instancia del ambiente y lo configura en el estado inicial\n",
    "e = gym.make('CartPole-v0')\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "El ambiente ha regresado una observación que consiste de una línea cuyos valores corresponden respectivamente a \n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa02777-367c-4613-8857-fa9dc4ceedc9",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Agente Aleatorio CartPole</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf24622c-e1ac-44ca-9d61-2438cb53cdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02273616,  0.01449582, -0.03549939,  0.01192305])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "e = gym.make('CartPole-v0')\n",
    "\n",
    "obs = e.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8cf7106-bf7c-46ed-8d92-6b7cc4f01540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1e77f9-ab11-4333-9ddf-9ae247920827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad0de20-9396-43e4-bb12-e8e38d869751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02302607, -0.18009951, -0.03526092,  0.29319752]), 1.0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08adfb6-9079-4d8e-8c59-69bf0496631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8779053e-01, -4.9415845e+37, -3.4483558e-01,  2.0978084e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22dd49e-8271-4853-be1c-9ddd4dc4a769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e406fb4d-43c0-4687-a73d-88c2ea2b7161",
   "metadata": {},
   "source": [
    "### The agente aleatorio CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99478e66-3e72-46df-ab0d-89db0decce7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01942408, -0.37470145, -0.02939697,  0.57455441]), 1.0, False, {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0b14aa-c09a-48fe-a919-2b3d32b65412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d966fb25-8c7d-40cf-acf9-c7dc08dd95b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio hecho en 39 pasos, recompensa total 39.00\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print('Episodio hecho en %d pasos, recompensa total %.2f' % (\n",
    "    total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be460638-b4a2-4555-9217-7826b7a37550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
