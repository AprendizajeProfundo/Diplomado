{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Método entropía cruzada</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dca78",
   "metadata": {},
   "source": [
    "<center>Naive frozen lake</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5846fe",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d09f7",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072351c",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b23f0c",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0a794",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc4de4",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [De Boer, P. and Kroese D.,  A Tutorial on the Cross-Entropy Method](http://web.mit.edu/6.454/www/www_fall_2003/gew/CEtutorial.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [La API OpenAI Gym](#La-API-OpenAI-Gym)\n",
    "* [El ambiente FrozenLake](#El-ambiente-FrozenLake)\n",
    "* [Modificación del espacio de observaciones de FrozenLake](#Modificación-del-espacio-de-observaciones-de-FrozenLake)\n",
    "* [El método de entropía cruzada en FrozenLake](#El-método-de-entropía-cruzada-en-FrozenLake)\n",
    "* [El método se la entropía cruzada](#El-método-de-la-entropía-cruzada)\n",
    "* [Modificación para el ambiente FrozenLake](#Modificación-para-el-ambiente-FrozenLake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección vamos a usar el ambiente frozen lake para entrenar una agente con la técnica de entopía cruzada como hicimos con el ambiente CartPole. \n",
    "\n",
    "Vamos a modificar el espacio de observaciones mediante el uso de la clase *gym.ObservationWrapper*, de tal manera que nuestro código de CarPole pueda ser reutilizdo.\n",
    "\n",
    "Lo que vamos a darnos cuenta es que el método de entropía cruzada no funciona en este caso. Est sistuacipon nos conducirá a implementar una solicipón mejorada del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b92992-3c1b-4af4-b8b0-44ac0460c5b4",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">La API OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa5bf5-614f-4420-a33e-81936045c7a0",
   "metadata": {},
   "source": [
    "La API `OpenAI Gym` tiene una rica colección de ambientes para experimentos de aprendizaje reforzado (AR), usando una interfaz unificada.\n",
    "\n",
    "La clase principal de Gym es *Env* (de environment). Sus métodos y atributos proveen la información necesaria para poder entrenar agentes que interactúen con el medio ambientes. Las piezas más importante disponibles con *Env* son:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/environment.png\" width=\"600\" height=\"600\" align=\"right\"/>\n",
    "</figure> \n",
    "\n",
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar un acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*).\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación.\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e5fb-ca34-4653-9f3e-c709d652dd81",
   "metadata": {},
   "source": [
    "### El espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157683-e2d3-43ff-8f29-7bd14309fffe",
   "metadata": {},
   "source": [
    "Las acciones que puede realizar un agente puede ser discretas, continuas o incluso una combinación de ambas. Por ejemplo en un automóvil manejado de manera autónoma puede ser posible oprimir varios botones a la vez (discreto), oprimir el pedal del acelerador o del freno (continuo) y así. El espacio de acciones contiene o modela todas las posibilidades de acción del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ad16b-e42c-4957-8524-5476670451bd",
   "metadata": {},
   "source": [
    "### El espacio de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1305-62f4-4dd5-91a4-fe68ce77191e",
   "metadata": {},
   "source": [
    "Las observaciones son piezas de información que el ambiente puede entregar al agente en cualquier momento del tiempo, además de la recompensa. Las observaciones pueden ser muy simples como un arreglo unidimensional de números o  complejos tensores, como las imágenes provenientes de varias cámaras del automóvil.\n",
    "\n",
    "Es claro que acciones y observaciones tienen similaridades, por lo que Gym dispone de una clase abstracta llamada *Space* que permite derivar espacios de acciones y espacios de observaciones. La siguiente imagen muestra una diagrama de la clase *Space*.\n",
    "\n",
    "Existen otras subclases de *Space*, pero estas son las más comúnmente utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374739ea-141b-4a29-89e1-2ef87bed6b80",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/jerarquia_clase_Space_Gym.jpeg\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Jerarquía de la clase Space de OpenAI Gym. Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc147e-8c2f-4028-ad45-aa897b0fd0fb",
   "metadata": {},
   "source": [
    "La clase abstracta *Space* incluye dos métodos relevantes:\n",
    "\n",
    "* *sample()*: retorna una muestra aleatoria del espacio.\n",
    "* *contains(x)*: chequea si el argumento es un elemento del dominio del espacio.\n",
    "\n",
    "Ambos métodos son abstractos y deben ser implementados en las subclases de *Space*.\n",
    "\n",
    "* La clase *Discrete* representa un conjunto de ítems mutuamente excluyentes, numerados entre 0 y $n-1$. Su único atributo *n*, es una cuenta de los ítems que la clase describe. Por ejemplo. *Discrete(n=4)* puede representar por ejemplo un espacio de acciones de cuatro direcciones para moverse: *[left, right, up, down]*.\n",
    "\n",
    "* La clase *Box* representa un tensor *n*-dimensional de números racionales con intervalos *[low, high]*. Por ejemplo puede representar el pedal de un acelerador con un valor simple entre 0.0 y 1.0. En pedal sería codificado en tal caso como *Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)*. Otro ejemplo puede ser la observación de la pantalla en un juego Atari, la cual es RGB de tamaño 210*160: *Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)*.\n",
    "\n",
    "* La última subclase de *Space* es *Tuple*, la cual permite combinar varias instancias de *Space* juntas. esta subclase permite crear espacios de acciones y observaciones de la complejidad que se desee.\n",
    "\n",
    "Un ejemplo del uso de tupla puede ser el siguiente. Supongamos que deseamos modelar algunos controles de un automóvil. Para empezar la dirección (el ángulo de la dirección, en realidad), el acelerador y el freno pueder cambiar en cada instante de tiempo. Por otro lado, podemos tener botones discretos para representar la luces direccionales *(apagadas, izquierda, derecha)* y la bocina *(encendida, apagada)*. Entonces, podemos crear *Tupla(spaces=(Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(3), Discrete(2)\n",
    ")*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b93b-c965-4151-9465-f48ffc6d15e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### El ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234d15-59fb-48f8-b091-8a19fc0bfc01",
   "metadata": {},
   "source": [
    "El ambiente es representado por la clase *Env*. Todo ambiente tiene dos miembros de tipo *Space*: \n",
    "\n",
    "* *action_space*: acciones permitidas que entrega el ambiente al agente.\n",
    "* *observation_space*: observaciones proveidas por el ambiente al agente.\n",
    "\n",
    "Además el ambiente tiene los siguientes miembros:\n",
    "\n",
    "* *reset()*: coloca el ambiente en el estado inicial.\n",
    "* *step()*: Permite al agente tomar una acción y retorna información como respuesta a la acción: la siguiente observación, la recompensa y la bandera indicando si el episodio ha  finalizado. Esta es la pieza central de la funcionalidad del ambiente.\n",
    "\n",
    "Estas características permiten crear código genérico que podría trabajar con cualquier ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fea79-4bca-4a0e-800c-38a8c40dc71b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El ambiente FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c5078-0287-4a92-af21-9fd0d6cca4a2",
   "metadata": {},
   "source": [
    "Fuente: Alvaro Montenegro\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/frozenlake.png\" width=\"400\" height=\"400\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "    \n",
    "La imagen es una representación del ambiente FrozenLake de Gym. En este ambiente, hay 16 posibles posiciones, por lo que el conjunto de observaciones es discreto con 16 valores. El agente siempre comienza en la parte superior izquierda y el objetivo es llegar a la parte inferior derecha.\n",
    "\n",
    "Los sitios marcado con una X son huecos. Un episodio termina si el agente cae en un hueco o si llega al objetivo. En el primer caso (hueco) el agente recibe recompensa cero (0) y en el segundo caso (meta) el agente recibe recompensa uno (1). No importa cuanto se tarde en llegar ya sea a un hueco o a la meta, la recompensa siempre es la misma, es decir, 0 o 1.\n",
    "\n",
    "Las acciones posible del agento son: arriba (up), abajo (down), izquierda (left) y derecha (right). Observe además que según en donde se encuentre el agente, no todas las acciones son posible en el siguiente paso del agente.\n",
    "\n",
    "Este ambiente resulta más complicado que CartPole, debido al esquema de recompensa que entrega el ambiente. Una complicación adicional se introduce. Se va a suponer que el lago es resbaladizo. Esto implica que por ejemplo, el agente puede seleccionar izquierda, pero con un probabilidad almacenada en el ambiente puede resultar a la derecha (33.3\\%), arriba (33.3\\%) o abajo (33.3\\%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "Lo primero que vamos  a hacer es modificar el espacio de observaciones a una codificación del tipo one-hot, con el propósito de reutilizar la implementación de CartPole que hicimos antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e3ae1-6a7b-4ed6-8aab-a73608771ece",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modificación del espacio de observaciones de FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c2307-ed6e-4d71-8563-85dba80a2515",
   "metadata": {},
   "source": [
    "Primero demos una mirada al espacio de observaciones del ambiente. El método render de un objeto de tipo Env renderiza el espacio de acciones. Para interpretrar *S* es start, *F* es free, *H*  es hole y *G* es goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0bce797-5d20-4ca5-b01c-cee44207d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "e = gym.make(\"FrozenLake-v1\")\n",
    "print(e.observation_space)\n",
    "e.reset()\n",
    "e.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70da891-5ee0-42a8-b524-35cd6ce40e26",
   "metadata": {},
   "source": [
    "### importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b1f1eb-d0bb-40b1-8a5d-79cfc5a440f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096afa6-cdba-4548-8e8b-b95ae8425d61",
   "metadata": {},
   "source": [
    "### variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df5aeaf-9bdf-4491-8684-cba19d4b3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "MAX_ITER = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e56c89-32ee-417c-8427-cb4ac86770f9",
   "metadata": {},
   "source": [
    "### Envuelve el espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b28d64-2ee9-4606-a2e4-1cfecebed55d",
   "metadata": {},
   "source": [
    "Con esta clase se envuelve el epacio de acciones para convertirlo en un espacio de tipo de tal manera que sea compatle con el tipo de espacio de CartPole. El nuevo tipo de observation será de  *Box* y contendrá un vector de tamaño 16, de tipo *onehot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e26d4e9-1c66-41f7-8f53-6d38c0e2f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape=shape, dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656d2d2-ca42-4bb9-af8c-d3f9abc90c51",
   "metadata": {},
   "source": [
    "Hicimos esto para poder reultizar el resto de la construcción de CartPole. Para la explicaciones consulte la lección [Entropía cruzada CartPole](ar_Metodo_Entropia_Cruzada.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d6ba5f-d040-4655-bf2d-8e1aaae13496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# red neuronal\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# namedtuples para episodios y paso de episodio\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfb14a-2eda-4bb7-8215-77acbaa0ea52",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24622c-e1ac-44ca-9d61-2438cb53cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "    \n",
    "    \n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 0.8:\n",
    "            print(\"Resuelto!\")\n",
    "            break\n",
    "        if iter_no == max_iter:\n",
    "            print(\"Terminado por máximo número de iteraciones. No resuelto\")\n",
    "            break\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283e9b4-0de0-4a20-bbaa-3bc48fc46dcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Abre tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98879d9c-8e42-4835-802b-249eda22ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f78e08a-b693-4882-84ab-cddfa22598da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cierra el ambiente. LA venta X11 se cierra.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dd0d2-9b23-4431-9934-09a73f284e4e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método de la entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e854d2-7361-4365-abca-aa4e46403812",
   "metadata": {},
   "source": [
    "Como se puede observar, en el ejemplo del ambiente FrozenLake el algoritmo termina por número máximo de iteraciones. Vamos a revisar la razón por la cual el método no funciona en este caso. En la siguiente sección se introducen algunas modificaciones que harpam que método converja. Más adelante retomaremos este ambiente con otro tipo de entrenamiento.  En la siguiente imagen se observan respectivamente la distribución de la recompensa para los casos de CartPole y FrozenLake.\n",
    "\n",
    "Vamos a describir los detalles mínimos del métdo de entropía cruzada. Para obtenr información ampliada del método puede consultar [A Tutorial on the Cross-Entropy Method](http://web.mit.edu/6.454/www/www_fall_2003/gew/CEtutorial.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f3a56-0d9b-45f1-9b29-85e5bcec0ea8",
   "metadata": {},
   "source": [
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/distribucion_Reward_CartPole.png\" width=\"400\" height=\"400\" align=\"left\"/>\n",
    "<img src=\"../Imagenes/distribucion_Reward_FrozenLake.png\" width=\"400\" height=\"400\" align=\"right\"/>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8644d-ad75-42d9-81d7-6008be2be6f6",
   "metadata": {},
   "source": [
    "Debido a que en el caso de CartPole, la recompensa es entregada por cada paso en cada episodio. Cada episodio entrega valores de recompensa que permite tener una distribución, que puede verse de forma aproximada como normal. No lo es en realidad, pero ciertamente hay valores diferentes para cada episodio, que dan cuenta de como estuvo el agente en el episodio. Por otro lado, FrozenLake solamente entrega dos tipos  de recompensa por episodio: uno o cero, según el agente haya arribado a la meta o no. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187427fa-2f8c-4b81-b840-c132c9a08698",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/probility_space.png\" width=\"500\" height=\"500\" align=\"right\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "El objetivo del algoritmo es estimar la distribución de las acciones en el espacio de las observaciones. Si se dispone de una muestra de la distribución, la estimación es posible mediante en entrenamiento de una red neuronal. Eso es lo que lo se hace exáctamente cuando se entrena una red neuronal de clasificación.\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a57b08-a228-4994-a1a5-c664fa770c53",
   "metadata": {},
   "source": [
    "El problema que tenemos en aprendizaje reforzado es que tal muestra no está disponible incialmente. Entonces, el método de entropía cruzada (MEC) provee un procedimiento estacástico para resolver el problema.\n",
    "\n",
    "El MEC se basa en que en cada paso del algoritmo se obtiene una aproximación nejorada de la distribución de tal manera que cda vez se obtienen mejores muestras, en el sentido que la distribución aproximante se parece cada vez más a la distribución de las acciones.\n",
    "\n",
    "La distribución aproximante es calculada por la red neuronal. El propósito de tomar los episodios *élite* por supuesto es mantener aquellos episodios que entregan mayor recompensa. \n",
    "\n",
    "El MEC funciona en CarPole, porque los episodios más largos son aquellos en donde el agente usa mejor la información de las observaciones para evitar que la viga caiga. \n",
    "\n",
    "El problema en el ambiente frozeLake, es que la recompensa de cada episodio es 1 o 0, sin importar que sucedió en cada paso del episodio. Como ilustra la imagen de la distribución en este caso. Entonces, no hay realmente forma de obtener episodios élite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25272856-b47f-4c46-99ff-36da9bbe8873",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Modificación para el  ambiente FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ef99f-604d-42ea-b326-535b1de64629",
   "metadata": {},
   "source": [
    "Más adelante en este curso volveremos a este ambiente para resolver las limitaciones del MEC con otros métodos de parendizaje reforzado.\n",
    "\n",
    "De momento haremos unas mejoras que ayuden al MEC a estimar la distribución de las acciones en el espacio de las observaciones. Haremos los siguiente:\n",
    "\n",
    "* Lotes de episodios más largos. Pasaremos a 100 episodios por lote.\n",
    "* Aplicaremos  el factor de descuento $\\gamma$ a la recompensa. Asi espisodios más largos tendrán una menor recompensa y viceversa.Esto incrementa la variabilidad de la distribución de la recompensa.\n",
    "* Mantendremos episodios élite por más largo tiempo.\n",
    "* Decreceremos la rata de aprendizaje. Esto implica que la red neuronal tendra más tiemponpara ver en promedio mas muestras de entrenamiento.\n",
    "* Mucho mayor tiempo de entrenamiento.\n",
    "\n",
    "Por favor revise, corra y modifique si lo considera necesario, el siguiente código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae680fd-51e8-4cea-9d68-f71adc714394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "MAX_ITER = 1000\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    filter_fun = lambda s: s.reward * (GAMMA ** len(s.steps))\n",
    "    disc_rewards = list(map(filter_fun, batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation,\n",
    "                                 example.steps))\n",
    "            train_act.extend(map(lambda step: step.action,\n",
    "                                 example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-ajustado)\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(\n",
    "            lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = \\\n",
    "            filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, rw_mean=%.3f, \"\n",
    "              \"rw_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean,\n",
    "            reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"pérdida\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"recompensa_promedio\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"recompensa_frontera\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Resuelto!\")\n",
    "            break\n",
    "            if iter_no == MAX_ITER:\n",
    "            print(\"Terminado por máximo número de iteraciones. No resuelto\")\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cd712-f83a-4dfc-8d94-a1f68e25242e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Abre tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07aa374-8aa7-4c14-b2f7-8c4db978e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c2ba00-bae9-4406-849e-eba7e4a80b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cierra el ambiente. LA venta X11 se cierra.\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
