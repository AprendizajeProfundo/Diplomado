{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Generalizaciones de DQN Learning</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Algoritmos Double DQN y Dueling DQN</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) \n",
    "1. [Adaptado de Markel Sanz, Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)\n",
    "1.  [Van Hasselt, Hado, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. Thirtieth AAAI conference on artificial intelligence. 2016.](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389/11847) \n",
    "1.  [Wang, Ziyu, et al. Dueling network architectures for deep reinforcement learning](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Algortimos basados en el modelo](#Algortimos-basados-en-el-modelo)\n",
    "* [Algortimos basados en la política](#Algortimos-basados-en-la-política)\n",
    "* [Double DQN](#Double-DQN)\n",
    "* [Dueling DQN](#Dueling-DQN)\n",
    "* [El código](#El-código)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lecciones previas de DQN-Learning hemos visto cómo funciona el algoritmo DQN, y cómo éste puede aprender a solucionar problemas complejos. En esta lección veremos dos nuevos algoritmos que suponen mejoras respecto a DQN, ellos son *Double DQN* y *Dueling DQN*. Pero antes, introduzcamos algunos términos que hemos pasado por alto.\n",
    "\n",
    "\n",
    "Los algoritmos de aprendizaje reforzado se pueden clasificar en varias familias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algortimos basados en el modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera de estas familias depende de si el agente aprende cómo funciona el entorno de manera explícita o no. Si el algoritmo utiliza la dinámica del entorno (también conocido como modelo) durante la toma de decisiones, entonces el algoritmo es **basado en el modelo** (model based), y si no lo hace será **libre de modelo** (model free). Un agente basado en el modelo tiene que aprender (o tener acceso a) todas las probabilidades de transición de un estado a otro. \n",
    "\n",
    "Como muchos entornos son estocásticos (probabilísticos) y sus dinámicas desconocidas, el agente debe aprender el modelo detrás de estas transiciones probabilísticas. Una vez aprendidas, utilizará esa información para tomar mejores decisiones. Por ejemplo, si tomar la acción 1 llevará al estado A con probabilidad 0.9 y se recibiera una recompensa de -10, o  llevará al estado B con una probabilidad de 0.1 y se recibiera una recompensa de +10; pero tomar la acción 2  llevara al estado C con probabilidad de 1.0 y el agente obtendría una recompensa de +5, la mejor decisión es tomar la acción 2, ya que aunque la posible recompensa del estado B es muy grande, la probabilidad de terminar en ese estado es muy baja. \n",
    "\n",
    "Por lo tanto, no solo es importante usar las recompensas en la toma de decisiones, sino también el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmos basados en la política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda familia en la que se pueden clasificar los algoritmos son **fuera-de-la-política** (off policy) y **dentro-de-la-política** (on-policy). Los algoritmos fuera-de-la-política aprenden la función de valor (value function), sin importar qué acciones tome el agente. Es decir, que la política de comportamiento (behavior policy) y la política objetivo (target policy) pueden ser distintas. \n",
    "\n",
    "La primera es la que utiliza el agente para explorar el entorno y recoger datos, mientras que la segunda es la que el agente intenta aprender y mejorar. Ésto significa que el agente puede explorar el entorno de forma completamente aleatoria con la política de comportamiento, y usar esos datos para aprender una política objetivo que sea capaz de obtener una recompensa muy alta en el futuro. En los algoritmos dentro-de-la-política, la política de comportamiento y la objetivo deben ser las mismas. Los algoritmos aprenden de los datos que deben recibir tras seguir la misma política que están aprendiendo. \n",
    "\n",
    "A partir de ahora, clasificaremos los algoritmos en estas dos familias. Por ejemplo, tanto el algoritmo *Q-Learning* como *DQN* son algoritmos libre de modelo y *fuera-de-la-política*.  Los dos algoritmos que veremos en esta parte, *Double DQN* y *Dueling DQN*, también son algoritmos libre de modelo y fuera-de-la-política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Double DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con el algoritmo DQN es que **sobreestima las recompensas reales**; es decir, *los valores-Q* que aprende llevan a hacer creer al agente que recibirá una recompensa mayor de la que en realidad obtendrá. Para solucionarl este problema, los autores del algoritmo **Double DQN [1]** proponen un sencillo truco: separar la selección y evaluación de la acción en dos pasos diferentes. En lugar de usar la ecuación de Bellman del algoritmo DQN, este algoritmo la cambia y se convierte en:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a; \\theta) = r + \\lambda Q(s', \\arg \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta);\\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero la red neuronal principal $\\theta$ decide cuál es la mejor acción entre todas las posibles, y luego la red objetivo evalúa esa acción para conocer su *valor-Q*. \n",
    "\n",
    "Este simple cambio ha demostrado **reducir las sobreestimaciones** y resultar en mejores políticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifique el algoritmo DQN de Atari-Pong desarrollado en la lección anterior para implementar este algortimo Double DQN. La implementación es inmediata. Pruebe su algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de pérdida\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(np.array(\n",
    "        states, copy=False)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(\n",
    "        next_states, copy=False)).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(\n",
    "        1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + \\\n",
    "                                   rewards_v\n",
    "    return nn.MSELoss()(state_action_values,\n",
    "                        expected_state_action_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Dueling DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo divide los *valores-Q* en dos partes distintas, la función de valor (value function) $V(s)$ y la función de ventaja (advantage function) $A(s, a)$.\n",
    "\n",
    "La función de valor $V(s)$ nos dice cuánta recompensa obtendremos desde el estado *s*. La función de ventaja A$(s, a)$ nos dice cuánto mejor es una acción respecto a las demás. Combinando el valor *V* y la ventaja *A* de cada acción, obtenemos los *valores-Q*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) + A(s,a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que se propone con el **algoritmo Dueling DQN es que la misma red neuronal divida su capa final en dos, una para estimar el valor del estado** *s* $(V(s))$ y otra para estimar la ventaja de cada acción *a* ($A(s, a)$), y al final juntar ambas partes en una sola, la cual estimará los *valores-Q*. \n",
    "\n",
    "Este cambio ayuda en algunos casos, porque a veces no es necesario saber exactamente al valor de cada acción, por lo que aprender el valor del estado puede ser suficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/red_Dueling.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Red neronal asociada al algortimo Dueling DNQ</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-4-double-dqn-y-dueling-dqn-b24ac0a5a46c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, entrenar la red neuronal de esta simple manera, sumando el valor y la ventaja en la capa final, no es posible. \n",
    "\n",
    "Si se tiene $Q=V+A$, dada la función $Q$, no podemos determinar $V$ y $A$, es decir, el problema es **no-identificable**. \n",
    "\n",
    "Para solucionarlo, el artículo propone un truco: forzar al *valor-Q* más alto a ser igual al valor *V*, haciendo que el valor más alto en la función de ventaja sea cero, y los demás valores sean negativos. \n",
    "\n",
    "Esto  dirá exactamente cuál es el valor de $V$, y se pueden calcular los valores de la función de ventaja desde ahí, solucionando el problema. Así es como se entrenaría el algoritmo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) +( A(s,a)- \\max_{a'\\in|A|} A(s,a)) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, el artículo sugiere un pequeño cambio a estas ecuaciones. En lugar de usar el máximo, usaremos la media de las ventajas, así que eso será lo que hagamos (vea el artículo para más detalles). Así es como entrenaremos el algoritmo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a) = V(s) +( A(s,a)- \\frac{1}{|A|} \\sum_{a'} A(s,a)) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El código</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solucionaremos de nuevo el juego de Atari 2600 Pong usando OpenAI Gym. \n",
    "\n",
    "Usaremos el código de la clase *DQN learning* como referencia. Pero esta vez cambiaremos la arquitectura de la red neuronal para dividir la capa final en dos, *las funciones de valor y de ventaja* (*V* y *A* en el código), y luego combinarlas para formar los *valores-Q*. \n",
    "\n",
    "Usaremos capas convolucionales. El número de unidades en cada capa y los parámetros será el mismo que en el artículo *Dueling DQN*.\n",
    "\n",
    "Puede consultar los códigos originales en Colab para [Tensorflow](https://colab.research.google.com/drive/16RjttswTuAjgqVV2jA-ioY2xVEQqIcQE#offline=true&sandboxMode=true) y [Pytorch](https://colab.research.google.com/drive/1EW7i4Jo_u2VbZAls7CVON_bKfFyKqKIn#offline=true&sandboxMode=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Atari.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo del juego Atari</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase DuelingDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(tf.keras.Model):\n",
    "  \"\"\"Convolutional neural network for the Atari games.\"\"\"\n",
    "  def __init__(self, num_actions):\n",
    "    super(DuelingDQN, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=8, strides=4, activation=\"relu\",\n",
    "    )\n",
    "    self.conv2 = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=4, strides=2, activation=\"relu\",\n",
    "    )\n",
    "    self.conv3 = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=1, activation=\"relu\",\n",
    "    )\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=512, activation=\"relu\")\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    self.A = tf.kears.layers.Dense(num_actions)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, states):\n",
    "    \"\"\"Forward pass of the neural network with some inputs.\"\"\"\n",
    "    x = self.conv1(states)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    V = self.V(x)\n",
    "    A = self.A(x)\n",
    "    Q = V + tf.subtract(A, tf.reduce_mean(A, axis=1, keepdims=True))\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, cambiaremos la función que ejecuta un paso de entrenamiento usando el descenso de gradiente. La modificaremos para implementar el paso de entrenamiento del algoritmo *Double DQN* en lugar del *DQN normal*. \n",
    "\n",
    "Esto significa que la ecuación de Bellman será la descrita anteriormente en este artículo, que es ligeramente diferente a la descrita en la lección *DQN-Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data.\"\"\"\n",
    "    # Select best next action using main_nn.\n",
    "    next_qs_main = main_nn(next_states)\n",
    "    next_qs_argmax = tf.argmax(next_qs_main, axis=-1)\n",
    "    next_action_mask = tf.one_hot(next_qs_argmax, num_actions)\n",
    "  \n",
    "    # Evaluate that best action using target_nn to know its Q-value.\n",
    "    next_qs_target = target_nn(next_states)\n",
    "    masked_next_qs = tf.reduce_sum(next_action_mask * next_qs_target, axis=-1)\n",
    "  \n",
    "    # Create target using the reward and the discounted next Q-value.\n",
    "    target = rewards + (1. - dones) * discount * masked_next_qs\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Q-values for the current state.\n",
    "        qs = main_nn(states)\n",
    "        action_mask = tf.one_hot(actions, num_actions)\n",
    "        masked_qs = tf.reduce_sum(action_mask * qs, axis=-1)\n",
    "        loss = loss_fn(target, masked_qs)\n",
    "    \n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase DuelingDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "  \"\"\"Convolutional neural network for the Atari games.\"\"\"\n",
    "  def __init__(self, num_actions):\n",
    "    super(DuelingDQN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "    std = math.sqrt(2.0 / (4 * 84 * 84))\n",
    "    nn.init.normal_(self.conv1.weight, mean=0.0, std=std)\n",
    "    self.conv1.bias.data.fill_(0.0)\n",
    "\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "    std = math.sqrt(2.0 / (32 * 4 * 8 * 8))\n",
    "    nn.init.normal_(self.conv2.weight, mean=0.0, std=std)\n",
    "    self.conv2.bias.data.fill_(0.0)\n",
    "\n",
    "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "    std = math.sqrt(2.0 / (64 * 32 * 4 * 4))\n",
    "    nn.init.normal_(self.conv3.weight, mean=0.0, std=std)\n",
    "    self.conv3.bias.data.fill_(0.0)\n",
    "\n",
    "    self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "    std = math.sqrt(2.0 / (64 * 64 * 3 * 3))\n",
    "    nn.init.normal_(self.fc1.weight, mean=0.0, std=std)\n",
    "    self.fc1.bias.data.fill_(0.0)\n",
    "    self.V = nn.Linear(512, 1)\n",
    "    self.A = nn.Linear(512, num_actions)\n",
    "\n",
    "\n",
    "  def forward(self, states):\n",
    "    \"\"\"Forward pass of the neural network with some inputs.\"\"\"\n",
    "    x = F.relu(self.conv1(states))\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = F.relu(self.fc1(x.view(x.size(0), -1)))  # Flatten imathut.\n",
    "    V = self.V(x)\n",
    "    A = self.A(x)\n",
    "    Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "    return Q\n",
    "\n",
    "    \n",
    "# Create main and target neural networks.\n",
    "main_nn = DuelingDQN(num_actions).to(device)\n",
    "target_nn = DuelingDQN(num_actions).to(device)\n",
    "\n",
    "# Loss function and optimizer.\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-5)\n",
    "loss_fn = nn.SmoothL1Loss()  # Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "  \"\"\"Perform a training iteration on a batch of data.\"\"\"\n",
    "  next_qs_argmax = main_nn(next_states).argmax(dim=-1, keepdim=True)\n",
    "  masked_next_qs = target_nn(next_states).gather(1, next_qs_argmax).squeeze()\n",
    "  target = rewards + (1.0 - dones) * discount * masked_next_qs\n",
    "  masked_qs = main_nn(states).gather(1, actions.unsqueeze(dim=-1)).squeeze()\n",
    "  loss = loss_fn(masked_qs, target.detach())\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutaremos el bucle principal de entrenamiento de la misma forma que en las partes anteriories, recogiendo datos y guardándolos en el buffer para usarlos más tarde. \n",
    "\n",
    "Tras entrenar el algoritmo durante 1000 episodios, esto será lo que escriba. El retorno indica cuantos goles ha marcado cada jugador."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
