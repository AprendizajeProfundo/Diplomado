{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conceptos clave del aprendizaje reforzado son:\n",
    "\n",
    "1. Agente (Agent)\n",
    "2. Recompensa (Reward)\n",
    "3. Ambiente (Environment)\n",
    "4. Estado (State)\n",
    "5. Función de valor (Value function)\n",
    "6. Política (Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Reinforcement_learning_diagram.png\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Wikipedia](https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente (Agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el aprendizaje reforzado, el agente(software) es la máquina que será entrenado y por tanto la que posee la inteligencia (artificial) y toma las decisiones sobre que hacer. Modernamente algunos expertos llaman al agente la inteligencia artificial. Así entrenar una agente es entrenar una inteligencia artificial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompensas (Rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una recompensa en el tiempo $t$, es denotada por $R_t$, y corresponde a una cantidad escalar con la cual se establece la retroalimentación (feedback) con el agente, de acuerdo con las decisiones que este toma.\n",
    "\n",
    "La meta del agente es maximizar la suma de recompensas. Entonces $R_t$ le indica al agente que tan bien está haciendo su trabajo en el paso o tiempo $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de Recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En *juegos*: +1 cada vez que se incrementa el puntaje (score) y -1 cada vez que decrese el puntaje (o se pierde una vida).\n",
    "2. En *mercado de acciones*: +1 por cada dólar  ganado y -1 por cada dólar perdido.\n",
    "3. En *menejo autónomo de un vehículo*: +1 por cada kilómetro conducido, -100 por cada colisión.\n",
    "4. *Recompensa dispersa* o *diferida*. Cuando al recompensa se entrega solamente al final de un proceso completo. Por ejemplo en el aprendizaje de juegos como ajedrez o Go, la recompensa puede ser +1, si el agente gan el juego y -1 si lo pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente (Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ambiente es la plataforma que representa el problema o la tarea en la cual estamos interesados. El agente interactua siempre con el ambiente.\n",
    "\n",
    "En cada tiempo o paso $t$, el agente recibe una observación  $O_t$ del ambiente  y en como respuesta ejecuta una acción $A_t$, por la cual recibe de regreso una recompensa $R_t$ de parte de ambiente, junto con la siguiente observación $O_{t+1}$. Este proceso se repite hasta que se alcanza un estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el agente interactua con el ambiente, el proceso resultante de tal interaccción se puede representar como una sucesión de observaciones $(O_t)$, accciones $(A_t)$ y recompensas $(R_t)$. En el tiempo (o paso) $t$, lo único que conoce el agente hasta ese momento es la sucesión de $\\{O_i, A_i, R_i \\}$ que han sido observados hasta el tiempo $t$.\n",
    "\n",
    "Vamos a denotar tal historia como \n",
    "\n",
    "$$\n",
    "H_t = \\{\\{O_1, A_1, R_1 \\},\\{O_2, A_2, R_2 \\}, \\ldots,  \\{O_t, A_t, R_t \\}\\}\n",
    "$$\n",
    "\n",
    "Lo que suceda en el tiempo $t+1$ depende de esta historia. Formalmente, la información usada para determinar lo que sucederá en seguida se llama *estado*. Como esto depende de la historia hasta el tiempo $t$, se escribirá\n",
    "\n",
    "$$\n",
    "S_t = f(H_t).\n",
    "$$\n",
    "\n",
    "Aquí $f$ denota alguna función.\n",
    "\n",
    "LLegamos a un momento en donde debemos hacer una distinción importante. Como la interacción sucede entre agente y ambiente, es necesario que ambas entidades (objetos) tengan conocimieneto del estado. Sin embargo, cada uno tiene su propia representación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado del ambiente se denotara $S_t^e$. Este estado es por lo general invisible o inaccesible al agente. Por otro lado el agente tiene su propia representación del estado, la cual denotaremos $S_t^a$. Esta es la información usada por el agente como base para decidir sus acciones y le corresponde a este usar alguna función para respresentarla.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estado de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un estado de Markov es una representación del estado en donde la probabilidad del siguiente estado depende solamente del estado actual. En símbolos se escribe\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1,S_2,\\ldots, S_t].\n",
    "$$\n",
    "\n",
    "Esto se interpreta como que una vez se conoce el nuevo estado, la historia anterior no es requierida para determinar la probabilidad del siguiente estado.\n",
    "\n",
    "Es común que el estado del ambiente $S_t^e$ tenga la propiedad Markoviana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente completamente observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunas ocasiones el ambiente puede hacer visible directamente su estado al agente. En tales casos se dice que el ambiente es completamente observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente parcialmente observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otros casos el agente no puede observar directamente el estado del ambiente. En este caso el agente construye su propia representación a partir de lo que observa. Tales ambientes se llaman parcialmente observables.\n",
    "\n",
    "Por ejemplo, un agente jugando pocker puede observar únicamente las cartas públicas. Otro ejemplo es el auto autónomo. En este caso, el auto tiene una cámara que no le permite conocer su pocisión absoluta.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
