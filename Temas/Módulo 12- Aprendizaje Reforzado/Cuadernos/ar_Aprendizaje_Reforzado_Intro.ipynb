{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Introducción a aprendizaje reforzado</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Conceptos básicos</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Elementos del aprendizaje reforzado](#Elementos-del-aprendizaje-reforzado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje reforzado es una línea completamente definida en el área del aprendizaje de máquinas. La imagen ilustra el mapa de lo que se conoce actualemnte como aprendizaje de máquina. Observe la presencia del aprendizaje reforzado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/machine_learning_map.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Homemade Machine Learning in Python](https://dev.to/trekhleb/homemade-machine-learning-in-python-4gbj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Elementos del aprendizaje reforzado</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El ratón el queso y la descarga eléctrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/laberinto_rata.jpeg\" width=\"200\" height=\"200\" align=\"right\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Imagine que tiene un agente que tiene que tomar decisiones en algún ambiente. Para agudizar la imaginación piense en un robot ratón en un laberinto. El `agente` es el robot y el `ambiente` es el laberinto, en el cual el robot encontrará pasillo en donde no encuentra nada, pasillos en donde recibe descarga eléctricas y pasillos en donde encuentra comida (queso).\n",
    "\n",
    "El robot debe decidir sobre las `acciones` a tomar: girar a la izquierda, girar a la derecha o seguir de frente. Suponemos que no puede hacer giros para ir en el sentido contrario. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada acción que decide tomar el robot tiene como consecuencia una `recompensa` que el ambiente le entrega al robot: le entrega comida (`recompensa positiva`), le entrega una descarga eléctrica (`recompensa negativa`) o quizás nada (`recompensa nula`).\n",
    "\n",
    "El propósito de entrenar el robot es que encuentre caminos que maximizan la `recompensa total` recibida en un `episodio`. Un episodio es un ejercicio de entrenamiento durante el cual el robot experimenta siguiendo diferente rutas. El final de un episodio puede definirse de distintas maneras. Por ejemplo, hasta alcanzar toda la comida, hasta completar un número de pasos en el laberinto, hasta alcanzar un número de descargas y así. La siguiente imagen idealiza las interacciones entre el agente y el ambiente. El agente reporta al ambiente sobre su próxima acción, y el ambiente le responde al agente  con una recompensa por su acción. Adicionalmente el ambiente le entrega al agente una `observación`  que que le indica al agente sobre las posibles acciones que puede ejecutar la próxima vez. Observe que dependiendo del sitio en el cual se encuentra el robot, no todas las acciones son posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RL_components.jpeg\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el robot, para obtener la máxima recompensa posible en este caso se asemeja, aunque no es lo mismo, al famoso problema del [vendedor  viajero](https://en.wikipedia.org/wiki/Travelling_salesman_problem), que tiene aplicaciones practicas en problemas de transporte. En dicho problema se tiene un mapa con ciudades y distancias entre ciudades. El problema es encontrar la ruta óptima. Es decir, la ruta más para visitar todas las ciudades exactamente una sola vez y regresar ala ciudad de origen.\n",
    "\n",
    "\n",
    "La pregunta obvia es ¿Cómo entrenar al agente para que aprenda a encontrar la ruta óptima. Es decir, encontrar toda la comida con le menor cantidad de descargas.\n",
    "\n",
    "Tómese uno minutos para pensar en el problema y su posible solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando al agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ya tiene claro el problema del ratón robótico, es momento de pensar  en cómo entrenar al robot para que encuentre la ruta que le entrega la mayor recompensa. Por un momento pongamonos en los *zapatos del robot*. El ambiente le ha entregado una observación, que le permite decidir sobre su siguiente acción. Pero, ¿Cómo decidir cual puede ser la siguiente acción?\n",
    "\n",
    "Como ocurre en la realidad, el robot debe aprender a partir de su experiencia. ¿Pero como se hace eso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Procesos de Markov</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avanzar en la posible solución vamos a recurrir a la estadística. Tengamos en cuenta que nuestro agente, el ratón robótico, solamente puede estar en tres posibles sitios, los cuales denominaremos técnicamente `estados`.  El conjunto de todos los posibles estados de un proceso de Markov se denomina `espacio de estados`. Vamos a denominar a los posibles estados en los que puede estar el robot como *comida*, *descarga* y *descanso* respectivamente. En el ejemplo el conjunto de todas las estados en los que puede estar el robot es finito. Este no es siempre el caso, pero de momento solamente vamos a preocuparnos de nuestro ratón robótico. Entonces el asunto es que podemos imaginar que en cada instante, el robot se encuentra en una determinado estado. Pasar de un estado a otro ocurre de manera probabilística y en nuestro caso va a depender de la decisión que tome el agente. De momento vamos a considerar solamente el paso de un estado a otro sin preocuparnos por las decisiones que toma el agente.\n",
    "\n",
    "Desde este punto de vista, acabamos de describir en el párrafo anterior lo se conoce como un [proceso estocástico](https://es.wikipedia.org/wiki/Proceso_estoc%C3%A1stico#:~:text=En%20la%20teor%C3%ADa%20de%20la,otra%20variable%2C%20generalmente%20el%20tiempo.). Más precisamente  una [cadena de Markov](https://es.wikipedia.org/wiki/Cadena_de_M%C3%A1rkov). Observe que para determinar el siguiente estado basta conocer el estado actual. No se requiere conocer todos los estados anteriores en donde estuvo el robot. \n",
    "\n",
    "Por otro lado, es claro que existe una cierta probabilidad para que el robot pase de un estado a otro. La siguiente tabla ilustra la situación\n",
    "\n",
    "|$i \\to j$| *comida*|*descarga*|*descanso*|\n",
    "|---|---|---|---|\n",
    "|**comida**|0.1|0.5|0.4|\n",
    "|**descarga**|0.3|0.1|0.6|\n",
    "|**descanso**|0.7|0.2|0.1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la primera línea de la tabla se observan las probabilidades del siguiente estado en el estará el robot. Si se pregunta porque el siguiente estado puede ser el mismo, piense lo siguiente. El robot puede decide ir a otro sitio, para lo cual debe activar sus motores de desplazamiento. Pero podría ocurrir que al tratar de avanzar patinara y no lograra girar seguir de frente. Entonces, estamos suponiendo de manera realista que el ratón puede seguir en el mismo estado en el siguiente estado.\n",
    "\n",
    "Esta tabla se denomina `matriz de transición` de estados del proceso Markoviano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Procesos markovianos de recompensa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para introducir el concepto de recompesa vamos a introducir un ejemplo un poco más complejo, conocido como el *oficinista*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/oficinista_MP.png\" width=\"300\" height=\"300\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "En este caso el agente es el oficinista y el ambiente está constituido por la oficina en donde trabaja y su casa. El oficinista puede estar en uno de cuatro estados: *coffe*, *computer*. *chat* y *home*. \n",
    "\n",
    "La imagen ilustra una forma clásica de mostrar un proceso de Markov. Los círculos muestran los estados posibles del agente y las flechas en el grafo están etiquetados las probabilidad de pasar de un estado a otro. Por ejemplo, la probabilidad de pasar de *cofffe* a *chat* es 0.7. Puede imaginar que si el oficinista va por un café, lo mas probable es que termine entablando una conversación con algún compañero. Por fuerza de la costumbre llamaremos a nuestro oficinista *Dilbert*. El nombre proviene del personaje principal de la serie televisa *Scott Adams*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escriba la tabla completa de las probabilidades de cambios de estado que describe la gráfica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora supongamos que el agente, Dilbert, recibe una recompensa dependiendo del siguiente estado al que llega. Vamos a establecer el siguiente esquema de recompensas.\n",
    "\n",
    "Tabla de transiciones  y recompensas del ejemplo del oficinista. Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/oficinista_recompensa.png\" width=\"400\" height=\"400\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "La imagen indica que por ejemplo Dilbert recibe una recompensa de 2 si pasa de *coffee* a *computer.  Escribimos por ejemplo\n",
    "\n",
    "* *coffee* $\\to$ *computer*: 2, con probabilidad 0.3.\n",
    "* *home* $\\to$ *home*: 1, con probabilidad 0.6.\n",
    "\n",
    "Observe que ahora tenemos la probabilidad de pasar de un estado a otro y una recompensa asociada en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escriba la tabla completa del sistema de recompensas mostrado en la imagen. \n",
    "2. Escriba la matriz de transición de estados para el agente Dilibert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Retorno</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro siguiente concepto básico es retorno. El propósito del entrenamiento es que el agente tome una decisión estando en cada estado, de tal manera que la recompensa total recibida luego del entrenamiento sea máxima. Esto nos lleva a pensar en la recompensa total que el agente recibirá  a partir de un  instante, digamos $t$. Esta recompensa total se denotará $G_t$, se denomina  `retorno` en el tiempo $t$ y se define mediante\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación del retorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El retorno definido en cada instante mide la cantidad total de recompensa que recibirá el agente a partir del instante $t$. Asumiremos que $\\gamma$ es un  valor en el intervalo $[0, 1]$ y lo llamaremos `factor de descuento`. Si $\\gamma=0$, significa que solamente estamos considerando únicamente la recompensa que recibirá en el siguiente instante. En este caso, se está asumiendo que solamente se tiene conciencia de la siguiente recompensa. Esto no parece muy  útil, pero establece un límite mínimo de la recompensa que el agente espera recibir.\n",
    "\n",
    "En el otro extremo, si $\\gamma=1$, estamos dando el mismo peso a todas las posibles recompensas futuras. Podemos interpretar este caso, como que el agente tiene una perfecta visión de las siguientes recompensas. Este valor resulta siendo problemático cuando consideramos episodio muy largos. En tal caso el retorno en cualquier instante tiende a ser infinito, si por ejemplo todas las recompensas son positivas. Sin embargo, en casos de episodios cortos este valor de $\\gamma$ puede ser aplicable.\n",
    "\n",
    "\n",
    "Entre estos dos valores extremos que nos sirve como referencia se encuentran los valor de $\\gamma$ que resultan útiles al redefinir el retorno. De acuerdo con la definición si tenemos  $0<\\gamma<1$, entonces se está asignado menos peso a las recompensas más lejanas. Esto tiene mucho sentido, si lo pensamos detenidamente. Esperamos que el retorno acumule las recompensas más cercana dándoles un mayor peso. En la práctica es común asignar al factor de descuento $\\gamma$ valores entre 0.9 y 0.99.\n",
    "\n",
    "El parámetro $\\gamma$ es importante en aprendizaje reforzado y aparece muchas veces en las estrategias de entrenamiento de los agentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Valor de los estados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad definida como retorno no es muy útil en la práctica, debido a que se definió puntualmente en cada instante del proceso de recompensa markoviano. Esto implica que el valor cambia d estado a estado y en cada instante. Sin embargo, podemos obtener un valor más útil calculando la esperanza matemática del retorno en cada estado. Esta nueva cantidad se denomina el `valor del estado` y se define mediante\n",
    "\n",
    "$$\n",
    "V(s) = E[G| S_t=s].\n",
    "$$\n",
    "\n",
    "La interpretación se simple. Para el estado $s$, $V(s)$ es el promedio (o esperanza) del retorno  luego de seguir el proceso de recompensa de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresemos al ejemplo del oficinista (Dilibert). De acuerdo con la tabla de transiciones y recompensas del ejemplo se tiene que\n",
    "\n",
    "* V(chat) = -1\\*0.5 + 2\\*0.3 + 1\\*0.2 = 0.3\n",
    "* V(coffee) = 2\\*0.7 + 1\\* 0.1 + 3 \\* 0.2 = 2.1\n",
    "* V(home) = 1\\* 0.6 + 1\\* 0.4 = 1.0\n",
    "* V(computer) = 5\\* 0.5 + (–3)\\* 0.1 + 1\\* 0.2 + 2\\* 0.2 = 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Adicionando acciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los elementos esenciales de un proceso de recompensa de Markov. Es el momento de incluir en nuestro modelo las acciones que toma el agente. El agente pasa de un estado a otro porque va tomando decisiones en cada instante.\n",
    "\n",
    "Supondremos que el conjunto de todas las posibles acciones que el agente pude tomar es finito. Tal conjunto se denomina `espacio de acciones`. \n",
    "\n",
    "Para incorporar formalmente las acciones al proceso markoviano de recompensas, recordemos que tenemos una matriz de transición de estados que contiene la probabilidad de pasar de un estado a cualquier otro. \n",
    "\n",
    "\n",
    "Lo que hacemos ahora es *condicionar* la probabilidad de transición de un estado a otro a las acciones del agente. En otras palabras, estamos introduciendo una nueva dimensión a la matriz de transición de estados, para considerar el efecto de las acciones sobre las probabilidades de transición de un estado a otro. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/matriz_estado_accion.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que la elección de una acción por parte del agente puede afectar las probabilidades para el siguiente estado. Este es el modelo del `proceso decisión de Markov` (MDP).\n",
    "\n",
    "Tómese uno minutos para reflexionar acerca de porque necesitamos este grado de complejidad en nuestro modelo.\n",
    "\n",
    "Con este formalismo, estamos listos para introducir el concepto más importante para los procesos de decisión de Markov y el aprendizaje reforzado: la `política`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera muy simple, se puede pensar en la `política` como un conjunto de reglas que controla el comportamiento del agente. Aún en los caso más simples de ambientes puede haber una gran variedad de posibles políticas.  \n",
    "\n",
    "Recordemos que el principal objetivo del agente es obtener tanto retorno como sea posible. Así las cosas, dos políticas diferentes pueden tener diferentes montes de retorno, por lo que es importante encontrar una buena política. Esta es la importancia de la noción de política.\n",
    "\n",
    "Formalmente, la política se define como la distribución de probabilidad sobre el espacio de acciones para cada posible estado:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P[A_t=a|S_t=s].\n",
    "$$\n",
    "\n",
    "Observe que la política ha sido definida como una probabilidad condicional. Una política determinista es un caso especial, en donde la probabilidad de una acción $a$ dado el estado, tiene probabilidad 1.\n",
    "\n",
    "Terminamos esta introducción con algunos ejemplos generales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En *juegos*: +1 cada vez que se incrementa el puntaje (score) y -1 cada vez que decrese el puntaje (o se pierde una vida).\n",
    "2. En el *mercado de acciones*: +1 por cada dólar  ganado y -1 por cada dólar perdido.\n",
    "3. En *menejo autónomo de un vehículo*: +1 por cada kilómetro conducido, -100 por cada colisión.\n",
    "4. *Recompensa dispersa* o *diferida*. Cuando al recompensa se entrega solamente al final de un proceso completo. Por ejemplo en el aprendizaje de juegos como ajedrez o Go, la recompensa puede ser +1, si el agente gan el juego y -1 si lo pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambientes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente completamente observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunas ocasiones el ambiente puede hacer visible directamente su estado al agente. En tales casos se dice que el ambiente es completamente observable. Los ejemplos del ratón robótico y el oficinista el ambiente es completamente observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente parcialmente observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otros casos el agente no puede observar directamente el estado del ambiente. En este caso el agente construye su propia representación a partir de lo que observa. Tales ambientes se llaman parcialmente observables.\n",
    "\n",
    "Por ejemplo, un agente jugando poker puede observar únicamente las cartas públicas. Otro ejemplo es el auto autónomo. En este caso, el auto tiene una cámara que no le permite conocer su posición absoluta.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
