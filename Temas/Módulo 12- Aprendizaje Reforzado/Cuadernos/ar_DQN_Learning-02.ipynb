{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-Learning con redes neuronales, algoritmo DQN.</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Deep Q-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Q-Learning general](#Q-Learning-general)\n",
    "* [Q-Learning profundo](#Q-Learning-profundo)\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [Experiencia por repetición](#Experiencia-por-repetición)\n",
    "* [Congelando la red objetivo](#Congelando-la-red-objetivo)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Ejemplo ambiente CartPole-v0 en GymImporta módulos ](#Ejemplo-ambiente-CartPole-v0-en-Gym-Importa-módulos )\n",
    "* [Crea la clase DQNAgent](#Crea-la-clase-DQNAgent)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Clase-DDQNA](#Clase-DDQNA)\n",
    "* [Video luego de entrenado el agente](#Video-luego-de-entrenado-el-agente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del algoritmo Q-Learning, este  funciona muy bien cuando el entorno es simple y la función $Q(s,a)$ se puede representar como una tabla o matriz de valores. \n",
    "\n",
    "Pero cuando hay miles de millones de estados diferentes y cientos de acciones distintas, la tabla se vuelve enorme, y no es viable su utilización. Por ello se introdujo el Q-learning profundo (DQN).\n",
    "\n",
    "Antes de introducir los conceptos de DQN vamos a dar un paso adelante, generalizando los métodos de iteración de valores e iteración de  acciones presentados en la lección [Q-learning](ar_QLearning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Q-Learning general</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de iteración de valores e iteración de  acciones  introducidos en la lección [Q-learning](ar_QLearning.ipynb), reqieren en cada paso recorrer toda la tabla de la función en cada iteración. En ambientes realistas, puede ocurrir que algunos estados no son mostrados ffecuentemente por el ambiente. Entonces, deberiamos tener cuidado con esa situación.\n",
    "\n",
    "Vamoa a hacer la siguiente modificación que conocida genéricamente como `Q-learning`. Lo métodos de iteración de valores e iteración de  acciones  introducidos son casos particulares, tomando $\\alpha=1$ en lo que sigue.\n",
    "\n",
    "Recordemos que el método iteración de acciones usa el mapeo explicito estado $\\to$ valor y sigue lo siguientes pasos.\n",
    " \n",
    "1. Empieza con una tabla vacía, que mapea estados en valors de las acciones.\n",
    "1. Al interactuar con el ambiente obtiene la tupla $(s, a, r, s')$ es decir (estado, acción, recompensa, nuevo estado). En este paso  se es necesario decidir cuál acción tomar y realmente no existe un camino simple para tomar la decisión.  Una solución que dimos  en la lección del [bandido multibrazo](ar_Bandido_Multi_brazo.ipynb) fue usar la técnica epsilon-greedy. que retomaremos en esta lección.\n",
    "1. Una vez se selecciona una acción $a$, se actualiza $Q(s,a)$ usando la aproximación de Bellman \n",
    "$$\n",
    "Q(s,a) \\gets r + \\gamma \\max_{a' \\in A} Q(s', a').\n",
    "$$\n",
    "1. Repita el paso 2.\n",
    "\n",
    "En la iteración de valor, la condición final puede ser algún umbral de la actualización, o podemos realizar episodios de prueba para estimar la recompensa esperada de la póliza, como hicimos. Otra cosa a tener en cuenta aquí es cómo actualizar los valores $Q$. Mientras tomamos muestras\n",
    "del entorno, generalmente es una mala idea simplemente asignar nuevos valores a la tabla $Q$ cada vez. En problemas complicadoe esto vuelve inestable el proceso.\n",
    "\n",
    "En el algoritmo Q-learning se introduce un suavizamiento del proceso, heredado de las técnica de apredizaje general. Es decir, el valor actual $Q(s,a)$ es modicado adicionandole solamente un porcentaje $\\alpha$ de la aproximación de Bellman para la más reciente acción. En símbolos, si $\\alpha$ es un valor entre 0 y 1, la actualización de $Q$ ahora ocurre de la siguiente forma:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\gets  (1-\\alpha) Q(s,a) + \\alpha \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Así el algoritmo Q-learnig completo es\n",
    "\n",
    "1. Empieza con una tabla vacía, para $Q(s,a)$\n",
    "1. Obtiene la tupla $(s, a, r, s')$ del ambiente.\n",
    "1. Se actualiza $Q(s,a)$ usando la aproximación de Bellman, mediante \n",
    "$$\n",
    "Q(s,a) \\gets  (1-\\alpha) Q(s,a) + \\alpha \\left(r + \\gamma \\max_{a' \\in A} Q(s', a')\\right)\n",
    "$$\n",
    "1. Revisa la convergencia, Si no se ha alcanzado repita desde  el paso 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor $\\alpha$ se conoce técnicamente como rata de aprendizaje.\n",
    "\n",
    "Vamos a revisar la implementación para el caso del lago congelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Frozen-Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comienzo del código se definen las variables globales, de las cuales destacamos ALPHA = 0.2 y TEST_EPISODES = 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase *Agent* que contiene las tablas que usaremos y los métodos que realizaran las tareas. El constructor de la clase crea las tablas, el ambiente y la variable que contendrá el estado actual. Al inicialziación e smás simple que ante, porque ahora no vamos contar las trancisiones de un estado a otro, ni la historia de las recompensas. Esto hace menos demandante de memoria a nuestro nuevo algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método sample_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la función que genera datos de experiencia para el agente. Simplemente recibe una muestra aleatoria dle ambiente. Reinicia el ambiente si un episodio ha terminado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método est_value_and_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método determina la mejor accion y el mejor valor para un estado dado, con base en lo que encuentra en la tabla actual de valores (Q). Si alguna pareja (estado, acción) no está aún en la tabla, se recibe cero para ese caso, en la busqueda del mejor valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método value_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método recibe la tupla (estado, acción, recompensa, nuevo valor) y hace la actualización de la tabla de valores, de acuerdo con el paso 3 de nuestro algoritmo arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método *play_episodio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ejecuta un episodio completo usando el correspondiente ambiente. La acción en cada paso se toma utilizando nuestra tabla de valor actual de\n",
    "Valores Q.  Este método se utiliza para evaluar nuestra política actual para verificar el progreso\n",
    "de aprendizaje. Este método no altera nuestra tabla de valores. Solo usa la tabla para encontrar la mejor acción a realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto del código es el ciclo de entrenamiento. Asegúrese de entenderlo completamente. Observe al comienzo del ciclo el agente toma una acción de muestra, la somete al ambieente y actualiza la tabla de valores $Q$ y que *play_episode* se usa para determinar si el agente ha encontrado ya la política optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actualizada la mejor recompensa 0.000 -> 0.050\n",
      "Actualizada la mejor recompensa 0.050 -> 0.100\n",
      "Actualizada la mejor recompensa 0.100 -> 0.150\n",
      "Actualizada la mejor recompensa 0.150 -> 0.200\n",
      "Actualizada la mejor recompensa 0.200 -> 0.250\n",
      "Actualizada la mejor recompensa 0.250 -> 0.350\n",
      "Actualizada la mejor recompensa 0.350 -> 0.450\n",
      "Actualizada la mejor recompensa 0.450 -> 0.650\n",
      "Actualizada la mejor recompensa 0.650 -> 0.700\n",
      "Actualizada la mejor recompensa 0.700 -> 0.850\n",
      "Resuelto in 7625 iteraciones!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return old_state, action, reward, new_state\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_v = r + GAMMA * best_v\n",
    "        old_v = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"recompensa\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Actualizada la mejor recompensa %.3f -> %.3f\" % (\n",
    "                best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Resuelto in %d iteraciones!\" % iter_no)\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Q-Learning profundo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mnih et al. introdujeron el algoritmo Deep Q-Network o DQN, publicado en Nature en 2015 como [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). Este algoritmo combina el algoritmo *Q-learning* con redes neuronales profundas (Deep Neural Networks). \n",
    "\n",
    "Como es sabido en el campo de la IA, las redes neuronales son una fantástica manera de aproximar funciones no lineales. Este algoritmo usa una red neuronal para aproximar la función *Q*, evitando así utilizar una tabla para representar la misma. \n",
    "\n",
    "En realidad, utiliza dos redes neuronales para estabilizar el proceso de aprendizaje. \n",
    "\n",
    "1. La primera, la *red neuronal principal* (main Neural Network), representada por los parámetros $\\theta$, se utiliza para estimar los valores-Q del estado s y acción a actuales: $Q(s, a;\\theta)$. \n",
    "2. La segunda, la *red neuronal objetivo* (target Neural Network), parametrizada por $\\theta^{'}$, tendrá la misma arquitectura que la red principal pero se usará para aproximar los *valores-Q* del siguiente estado $s'$ y la siguiente acción $a'$. \n",
    "\n",
    "El aprendizaje ocurre en la red principal y no en la objetivo.\n",
    "\n",
    "La red objetivo se congela (sus parámetros no se cambian) durante varias iteraciones (normalmente alrededor de 10000), y después los parámetros de la red principal se copian a la red objetivo, transmitiendo así el aprendizaje de una a otra, haciendo que las estimaciones calculadas por la red objetivo sean más precisas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos requerido para entrenar la Q-network provienen de  la experiencia del agente: $(s_0 a_0 r_1 s_1, s_1 a_1 r_2  s_2,\\ldots, s_{T-1} a_{T-1} r_T s_T)$. Cada muestra de entrenamiento es una unidad de experiencia, $s_t a_t r_{t+1} s_{t+1}$.\n",
    "\n",
    "En el tiempo $t$ se tiene el estado $s=s_t$.  La acción $a=a_t$ es determinada usando el algoritmo Q-learning es de tipo epsilon-greedy., es decir, la siguiente acción, digamos $\\pi(a)$ se obtiene como:\n",
    "\n",
    "$$\n",
    "\\pi(a) = \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ecuación de Bellman en DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación de Bellman tiene esta forma ahora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a; \\theta) = r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar una red neuronal, necesitamos una función de pérdida o coste (loss or cost function), la cual definimos como el cuadrado de la diferencia entre ambos lados de la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "L(\\theta)= \\mathbb{E}[  r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})- Q(s,a; \\theta)]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta será la función que minimizaremos usando el algoritmo de descenso de gradiente (gradient descent), el cuál se ejecuta automáticamente si usamos una librería de diferenciación automática con redes neuronales, como TensorFlow o Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el punto de vista del algoritmo, en este caso se usa la $Q$-network para predecir el valor $Q$ de cada posible siguiente acción, dado el estado siguiente y escogiendo el máximo entre ellos.\n",
    "\n",
    "En el caso en el cual el estado es terminal se tiene que $\\max_{a^{'}}  Q(s^{'},a^{'})=0$.\n",
    "\n",
    "La siguiente imagen ilustra el procedimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/Q-Network.png\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "</figure> \n",
    "\n",
    "Fuente:  [Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Experiencia por repetición</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general el entrenamiento de la Q-network es inestable. Hay dos causas de la instabilidad\n",
    "\n",
    "1. Alta correlación entre las muestras.\n",
    "2. El target no es estacionario.\n",
    "\n",
    "Para resolver el problema de la alta correlación, los datos de entrenamiento son seleccionados aleatorios de un buffer que creamos para tal fín. Este proceso se conoce como experience replay (experiencia por repetición)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Congelando la red objetivo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL problema de no estacionariedad de la red objetivo (target) es resuelto congelando los pesos de la red objetivo durante $C$ pasos de  entrenamiento.\n",
    "\n",
    "\n",
    "Es decir, en realidad esta es la razón de tener dos redes con idéntica arquitectura. Los parámetros de la Q-network target son copiados desde la Q-network principal cada $C$ pasos de entrenamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo DQN </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo DQN es de tipo epsilon-greedy. \n",
    "\n",
    "**Primero se crean los objetos necesarios y se inicializan.**\n",
    "\n",
    "- Inicializar la memory  replay (buffer) *D* con capacidad *N*.\n",
    "- Inicializar la función de valor Q con pesos aleatorios $\\theta$\n",
    "- Inicializar la función acción-valor target $Q_{\\text{target}}$ con pesos con pesos $\\theta^{-}= \\theta$\n",
    "- Definir la rata de exploración inicial $\\epsilon$ y el factor de descuento, $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ciclo de aprendizaje**\n",
    "\n",
    "1. Para $\\text{ episodio } = 1,\\ldots, M$, do:\n",
    "2.        Dado el estado incial *s*\n",
    "3.        Para step = 1,...T do:\n",
    "4.            Escoja la acción (epsilon-greddy)\n",
    "$$\n",
    "a \\gets \\begin{cases} \\text{muestra aleatoria}(a) &\\text{ con probabilidad } \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$\n",
    "5. Ejecuta la acción *a* observe la recompensa *r* y el siguiente estado *s'*\n",
    "6. Almacene la transición $(s,a,r,s')$ en el buffer D\n",
    "7. Actualice el estado $s=s'$\n",
    "\n",
    "**Experiencia por repetición**\n",
    "\n",
    "8.Tome un mini-lote de muestra de experiencias de episodios $(s_j,a_j, r_{j+1}, s_{j+1})$ desde el buffer *D*.\n",
    "\n",
    "9. \n",
    "$$\n",
    "Q_{\\text{max}} = \\begin{cases} r_{j+1} &\\text{ si el episodio termina en } j+1\\\\\n",
    "r_{j+1} + \\gamma \\max_{a_{j+1}} Q_{\\text{target}}(s_{j+1},a_{j+1};\\theta ^{-}) &\\text{ en otro caso }\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "10. Ejecuta un paso de gradiente descendiente para la función de pédida $\\left(Q_{\\text{max}}- Q(s_j,a_j;\\theta) \\right)^2$ con respecto a los parámetros\n",
    "\n",
    "**Actualización periódica de la red objetivo(target)*\n",
    "\n",
    "11. Cada $C$ pasos, $Q_{\\text{objetivo}}=Q$, es decir $\\theta^{-} = \\theta$\n",
    "12. fin\n",
    "13. fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Ejemplo ambiente CartPole-v0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenemos el entorno (environment) CartPole. Se ha utilizado la librería *OpenAI Gym* para visualizar y ejecutar este entorno. En este entorno, el objetivo es mover el carro hacia la derecha y la izquierda, con el objetivo de equilibrar el palo. Recuerde que  si el palo se tuerce más de 15 grados respecto al eje vertical, el episodio terminará y volveremos a empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejmplo se ha desarrollodo en Tensorflow. Se deja como ejercio pŕactico transformarlo a Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar depencendias para renderizar OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!pip install pyglet\n",
    "#!pip install gym pyvirtualdisplay\n",
    "#!pip install xvfbwrapper\n",
    "#!apt-get install -y xvfb python-opengl ffmpeg\n",
    "#!pip install tensorflow==2.1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mejor correr en Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a DQN/DDQN to solve CartPole-v0 problem\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea la clase DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 episodes=500):\n",
    "        \"\"\"DQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            state_space (tensor): state space\n",
    "            action_space (tensor): action space\n",
    "            episodes (int): number of episodes to train\n",
    "        \"\"\"\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # initially 90% exploración, 10% explotación\n",
    "        self.epsilon = 1.0\n",
    "        # Aplica de forma iterativa el decaimiento hasta\n",
    "        # 10% exploration/90% explotación\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** \\\n",
    "                             (1. / float(episodes))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = '../Datos/dqn_cartpole.h5'\n",
    "        # Q Network for training\n",
    "        n_inputs = state_space.shape[0]\n",
    "        n_outputs = action_space.n\n",
    "        self.q_model = self.build_model(n_inputs, n_outputs)\n",
    "        self.q_model.compile(loss='mse', optimizer=Adam())\n",
    "        # target Q Network\n",
    "        self.target_q_model = self.build_model(n_inputs, n_outputs)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "\n",
    "    \n",
    "    def build_model(self, n_inputs, n_outputs):\n",
    "        \"\"\"Q Network is 256-256-256 MLP\n",
    "\n",
    "        Arguments:\n",
    "            n_inputs (int): input dim\n",
    "            n_outputs (int): output dim\n",
    "\n",
    "        Return:\n",
    "            q_model (Model): DQN\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(n_inputs, ), name='state')\n",
    "        x = Dense(256, activation='relu')(inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(n_outputs,\n",
    "                  activation='linear', \n",
    "                  name='action')(x)\n",
    "        q_model = Model(inputs, x)\n",
    "        q_model.summary()\n",
    "        return q_model\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"save Q Network params to a file\"\"\"\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"eps-greedy policy\n",
    "        Return:\n",
    "            action (tensor): action to execute\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # explore - do random action\n",
    "            return self.action_space.sample()\n",
    "\n",
    "        # exploit\n",
    "        q_values = self.q_model.predict(state)\n",
    "        # select the action with max Q-value\n",
    "        action = np.argmax(q_values[0])\n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"store experiences in the replay buffer\n",
    "        Arguments:\n",
    "            state (tensor): env state\n",
    "            action (tensor): agent action\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        \"\"\"\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Return:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DQN chooses the max Q value among next actions\n",
    "        # selection and evaluation of action is \n",
    "        # on the target Q Network\n",
    "        # Q_max = max_a' Q_target(s', a')\n",
    "        q_value = np.amax(\\\n",
    "                     self.target_q_model.predict(next_state)[0])\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"experience replay addresses the correlation issue \n",
    "            between samples\n",
    "        Arguments:\n",
    "            batch_size (int): replay buffer batch \n",
    "                sample size\n",
    "        \"\"\"\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.q_model.predict(state)\n",
    "            \n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.fit(np.array(state_batch),\n",
    "                         np.array(q_values_batch),\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=1,\n",
    "                         verbose=0)\n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after \n",
    "        # every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"decrease the exploration, increase exploitation\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase DDQNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self,\n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 episodes=500):\n",
    "        super().__init__(state_space, \n",
    "                         action_space, \n",
    "                         episodes)\n",
    "        \"\"\"DDQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            state_space (tensor): state space\n",
    "            action_space (tensor): action space\n",
    "            episodes (int): number of episodes to train\n",
    "        \"\"\"\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'ddqn_cartpole.h5'\n",
    "        print(\"-------------DDQN------------\")\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Returns:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DDQN\n",
    "        # current Q Network selects the action\n",
    "        # a'_max = argmax_a' Q(s', a')\n",
    "        action = np.argmax(self.q_model.predict(next_state)[0])\n",
    "        # target Q Network evaluates the action\n",
    "        # Q_max = Q_target(s', a'_max)\n",
    "        q_value = self.target_q_model.predict(\\\n",
    "                                      next_state)[0][action]\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "env_id = 'CartPole-v0'\n",
    "outdir = \"../Datos/dqn-%s\" % env_id\n",
    "no_render = True\n",
    "    \n",
    "# the number of trials without falling over\n",
    "win_trials = 100\n",
    "\n",
    "# the CartPole-v0 is considered solved if \n",
    "# for 100 consecutive trials, he cart pole has not \n",
    "# fallen over and it has achieved an average \n",
    "# reward of 195.0 \n",
    "# a reward of +1 is provided for every timestep \n",
    "# the pole remains upright\n",
    "win_reward = { 'CartPole-v0' : 195.0 }\n",
    "\n",
    "# stores the reward per episode\n",
    "scores = deque(maxlen=win_trials)\n",
    "logger.setLevel(logger.ERROR)\n",
    "    \n",
    "# crea el ambiente desde gym\n",
    "env = gym.make(env_id)\n",
    "\n",
    "\n",
    "if no_render:\n",
    "    env = wrappers.Monitor(env,\n",
    "                           directory=outdir,\n",
    "                            video_callable=False,\n",
    "                            force=True)\n",
    "else:\n",
    "    env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "env.seed(0)\n",
    "\n",
    "# instantiate the DQN agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "\n",
    "# should be solved in this number of episodes\n",
    "episode_count = 3000\n",
    " state_size = env.observation_space.shape[0]\n",
    "batch_size = 64\n",
    "\n",
    "# by default, CartPole-v0 has max episode steps = 200\n",
    "# you can use this to experiment beyond 200\n",
    "# env._max_episode_steps = 4000\n",
    "\n",
    "# Q-Learning sampling and fitting\n",
    "for episode in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # in CartPole-v0, action=0 is left and action=1 is right\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "         # in CartPole-v0:\n",
    "        # state = [pos, vel, theta, angular speed]\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # store every experience unit in replay buffer\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "    # call experience relay\n",
    "    if len(agent.memory) >= batch_size:\n",
    "         agent.replay(batch_size)\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    mean_score = np.mean(scores)\n",
    "    if mean_score >= win_reward[args.env_id] \\\n",
    "            and episode >= win_trials:\n",
    "        print(\"Solved in episode %d: \\\n",
    "                   Mean survival = %0.2lf in %d episodes\"\n",
    "                  % (episode, mean_score, win_trials))\n",
    "        print(\"Epsilon: \", agent.epsilon)\n",
    "            agent.save_weights()\n",
    "        break\n",
    "    if (episode + 1) % win_trials == 0:\n",
    "           print(\"Episode %d: Mean survival = \\\n",
    "                   %0.2lf in %d episodes\" %\n",
    "                  ((episode + 1), mean_score, win_trials))\n",
    "\n",
    "    # close the env and write monitor result info to disk\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video luego de entrenado el agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea_entrenado.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea despues de entrenar al agente. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
