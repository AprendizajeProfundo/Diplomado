{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning con redes neuronales, algoritmo DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [Experience replay](#Experience-replay)\n",
    "* [Freezing the target network](#Freezing-the-target-network)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Ejemplo ambiente CartPole-v0 en GymImporta módulos ](#Ejemplo-ambiente-CartPole-v0-en-Gym-Importa-módulos )\n",
    "* [Crea la clase DQNAgent](#Crea-la-clase-DQNAgent)\n",
    "* [Algoritmo DQN](#Algoritmo-DQN)\n",
    "* [Clase-DDQNA](#Clase-DDQNA)\n",
    "* [Video luego de entrenado el agente](#Video-luego-de-entrenado-el-agente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adaptado de Rowel Atienza, [Advance Deep Learning with Tensorflow 2 and Keras](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ), Pack,2020.\n",
    "2. Sutton, R. S., & Barto, A. G. (2018). [Reinforcement learning: An introduction. MIT press](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "3. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del algoritmo Q-Learning, este  funciona muy bien cuando el entorno es simple y la función $Q(s,a)$ se puede representar como una tabla o matriz de valores. \n",
    "\n",
    "Pero cuando hay miles de millones de estados diferentes y cientos de acciones distintas, la tabla se vuelve enorme, y no es viable su utilización. \n",
    "\n",
    "Por ello, Mnih et al. inventaron el algoritmo [Deep Q-Network o DQN.](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)  \n",
    "\n",
    "Este algoritmo combina el algoritmo *Q-learning* con redes neuronales profundas (Deep Neural Networks). \n",
    "\n",
    "Como es sabido en el campo de la IA, las redes neuronales son una fantástica manera de aproximar funciones no lineales. \n",
    "\n",
    "Por lo tanto, este algoritmo usa una red neuronal para aproximar la función *Q*, evitando así utilizar una tabla para representar la misma. \n",
    "\n",
    "En realidad, utiliza dos redes neuronales para estabilizar el proceso de aprendizaje. \n",
    "\n",
    "1. La primera, la *red neuronal principal* (main Neural Network), representada por los parámetros $\\theta$, se utiliza para estimar los valores-Q del estado s y acción a actuales: $Q(s, a;\\theta)$. \n",
    "2. La segunda, la *red neuronal objetivo* (target Neural Network), parametrizada por $\\theta^{'}$, tendrá la misma arquitectura que la red principal pero se usará para aproximar los *valores-Q* del siguiente estado $s'$ y la siguiente acción $a'$. \n",
    "\n",
    "El aprendizaje ocurre en la red principal y no en la objetivo.\n",
    "\n",
    "La red objetivo se congela (sus parámetros no se cambian) durante varias iteraciones (normalmente alrededor de 10000), y después los parámetros de la red principal se copian a la red objetivo, transmitiendo así el aprendizaje de una a otra, haciendo que las estimaciones calculadas por la red objetivo sean más precisas.\n",
    "\n",
    "La siguiente imagen compara los procesos Q-learning y DN-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Q-Network.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Comparación  Q-learning v.s. DQ Learning</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "Fuente: [reinforcement-learning-deep-q-networks](https://blogs.oracle.com/datascience/reinforcement-learning-deep-q-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos requerido para entrenar la Q-network provienen de  la experiencia del agente: $(s_0 a_0 r_1 s_1, s_1 a_1 r_2  s_2,\\ldots, s_{T-1} a_{T-1} r_T s_T)$. Cada muestra de entrnamiento es una unidad de experiencia, $s_t a_t r_{t+1} s_{t+1}$.\n",
    "\n",
    "En el tiempo $t$ se tiene el estado $s=s_t$.  La acción $a=a_t$ es determinada usando el algoritmo Q-learning similar a las lecciones previas:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} sample(a) &\\text{ random } < \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuación de Bellman en DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación de Bellman tiene esta forma ahora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a; \\theta) = r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entrenar una red neuronal, necesitamos una función de pérdida o coste (loss or cost function), la cual definimos como el cuadrado de la diferencia entre ambos lados de la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "L(\\theta)= \\mathbb{E}[  r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})- Q(s,a; \\theta)]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta será la función que minimizaremos usando el algoritmo de descenso de gradiente (gradient descent), el cuál se ejecuta automáticamente si usamos una librería de diferenciación automática con redes neuronales, como TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el punto de vista del algoritmo, en este caso se usa la $Q$-network para predecir el valor $Q$ de cada posible siguiente acción, dado el estado siguiente y escogiendo el máximo entre ellos.\n",
    "\n",
    "En este caso, que en el estado terminal se tiene que $\\max_{a^{'}}  Q(s^{'},a^{'})=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general el entrenamiento de la Q-network es inestable. Hay dos causas de la instabilidad\n",
    "\n",
    "1. Alta correlación entre las muestras.\n",
    "2. El target no es estacionario.\n",
    "\n",
    "Para resolver el problema de la alta correlación, los datos de entrenamiento son seleccionados aleatorios de un buffer que creamos para tal fín. \n",
    "\n",
    "Este proceso se conoce como exprience replay (experiencia de repetición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Freezing the target network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL problema de no estacionariedad de la red objetivo (target) es resuelto congelando los pesos de la red objetivo durante $C$ pasos de  entrenamiento.\n",
    "\n",
    "\n",
    "Es decir, en realidad esta es la razón de tener dos redes idénticas. Los parámetros de la Q-network target son copiados desde la Q-network principal cada $C$ pasos de entrenamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo DQN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inicializar la replay memory *D* con cpacidad *N*.\n",
    "- Inicializar la función de valor Q con pesos aleatorios $\\theta$\n",
    "- Inicializar la función acción-valor target $Q_{\\text{target}}$ con pesos con pesos $\\theta^{-}= \\theta$\n",
    "- Definir la rata de exploración inicial $\\epsilon$ y el factor de descuento, $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para $\\text{ episodio } = 1,\\ldots, M$, do:\n",
    "2.        Dado el estado incial *s*\n",
    "3.        Para step =1,...T do:\n",
    "4.            Escoja la acción \n",
    "$$\n",
    "a = \\begin{cases} sample(a) &\\text{ random } < \\epsilon \\\\\n",
    "\\arg \\max_a{ Q(s,a)} &\\text{ en otro caso }\\end{cases}\n",
    "$$\n",
    "5. Ejecuta la acción *a* observe la recompensa *r* y el siguiente estado *s'*\n",
    "6. Almacene la transición $(s,a,r,s')$ en D\n",
    "7. Actualice el estado $s=s'$\n",
    "8. // experience replay\n",
    "9.Tome un mini-lote de muestra de experiencias de episodios $(s_j,a_j, r_{j+1}, s_{j+1})$ desde el buffer *D*.\n",
    "10.\n",
    "\n",
    "$$\n",
    "Q_{\\text{max}} = \\begin{cases} r_{j+1} &\\text{ si el episodio termina en } j+1\\\\\n",
    "r_{j+1} + \\lambda \\max_{a_{j+1}} Q_{\\text{target}}(s_{j+1},a_{j+1};\\theta ^{-}) &\\text{ en otro caso }\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "11. Ejecuta un paso de gradiente descendiente para la función de pédida $\\letf(Q_{\\text{max}}- Q(s_j,a_j;\\theta) \\right)^2$ con respecto a los parámetros\n",
    "12. // actualización periódica de la red target\n",
    "13. Cada $C$ pasos, $Q_{\\text{target}}=Q$, es decir $\\theta^{-} = \\theta$\n",
    "14. fin\n",
    "15. fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo ambiente CartPole-v0 en GymImporta módulos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tenemos el entorno (environment) conocido como CartPole. Se ha utilizado la librería *OpenAI Gym* para visualizar y ejecutar este entorno. En este entorno, el objetivo es mover el carro hacia la derecha y la izquierda, con el objetivo de equilibrar el palo. Y si el palo se tuerce más de 15 grados respecto al eje vertical, el episodio terminará y volveremos a empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar depencendias para renderizar OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!pip install pyglet\n",
    "#!pip install gym pyvirtualdisplay\n",
    "#!pip install xvfbwrapper\n",
    "#!apt-get install -y xvfb python-opengl ffmpeg\n",
    "#!pip install tensorflow==2.1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mejor correr en Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa módulos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a DQN/DDQN to solve CartPole-v0 problem\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers, logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea la clase DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 episodes=500):\n",
    "        \"\"\"DQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            state_space (tensor): state space\n",
    "            action_space (tensor): action space\n",
    "            episodes (int): number of episodes to train\n",
    "        \"\"\"\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # experience buffer\n",
    "        self.memory = []\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # initially 90% exploration, 10% exploitation\n",
    "        self.epsilon = 1.0\n",
    "        # iteratively applying decay til \n",
    "        # 10% exploration/90% exploitation\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** \\\n",
    "                             (1. / float(episodes))\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'dqn_cartpole.h5'\n",
    "        # Q Network for training\n",
    "        n_inputs = state_space.shape[0]\n",
    "        n_outputs = action_space.n\n",
    "        self.q_model = self.build_model(n_inputs, n_outputs)\n",
    "        self.q_model.compile(loss='mse', optimizer=Adam())\n",
    "        # target Q Network\n",
    "        self.target_q_model = self.build_model(n_inputs, n_outputs)\n",
    "        # copy Q Network params to target Q Network\n",
    "        self.update_weights()\n",
    "\n",
    "        self.replay_counter = 0\n",
    "\n",
    "    \n",
    "    def build_model(self, n_inputs, n_outputs):\n",
    "        \"\"\"Q Network is 256-256-256 MLP\n",
    "\n",
    "        Arguments:\n",
    "            n_inputs (int): input dim\n",
    "            n_outputs (int): output dim\n",
    "\n",
    "        Return:\n",
    "            q_model (Model): DQN\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(n_inputs, ), name='state')\n",
    "        x = Dense(256, activation='relu')(inputs)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dense(n_outputs,\n",
    "                  activation='linear', \n",
    "                  name='action')(x)\n",
    "        q_model = Model(inputs, x)\n",
    "        q_model.summary()\n",
    "        return q_model\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"save Q Network params to a file\"\"\"\n",
    "        self.q_model.save_weights(self.weights_file)\n",
    "\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"eps-greedy policy\n",
    "        Return:\n",
    "            action (tensor): action to execute\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # explore - do random action\n",
    "            return self.action_space.sample()\n",
    "\n",
    "        # exploit\n",
    "        q_values = self.q_model.predict(state)\n",
    "        # select the action with max Q-value\n",
    "        action = np.argmax(q_values[0])\n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"store experiences in the replay buffer\n",
    "        Arguments:\n",
    "            state (tensor): env state\n",
    "            action (tensor): agent action\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        \"\"\"\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory.append(item)\n",
    "\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Return:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DQN chooses the max Q value among next actions\n",
    "        # selection and evaluation of action is \n",
    "        # on the target Q Network\n",
    "        # Q_max = max_a' Q_target(s', a')\n",
    "        q_value = np.amax(\\\n",
    "                     self.target_q_model.predict(next_state)[0])\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"experience replay addresses the correlation issue \n",
    "            between samples\n",
    "        Arguments:\n",
    "            batch_size (int): replay buffer batch \n",
    "                sample size\n",
    "        \"\"\"\n",
    "        # sars = state, action, reward, state' (next_state)\n",
    "        sars_batch = random.sample(self.memory, batch_size)\n",
    "        state_batch, q_values_batch = [], []\n",
    "\n",
    "        # fixme: for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            # policy prediction for a given state\n",
    "            q_values = self.q_model.predict(state)\n",
    "            \n",
    "            # get Q_max\n",
    "            q_value = self.get_target_q_value(next_state, reward)\n",
    "\n",
    "            # correction on the Q value for the action used\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "\n",
    "            # collect batch state-q_value mapping\n",
    "            state_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        # train the Q-network\n",
    "        self.q_model.fit(np.array(state_batch),\n",
    "                         np.array(q_values_batch),\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=1,\n",
    "                         verbose=0)\n",
    "\n",
    "        # update exploration-exploitation probability\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # copy new params on old target after \n",
    "        # every 10 training updates\n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "\n",
    "        self.replay_counter += 1\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"decrease the exploration, increase exploitation\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase DDQNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent(DQNAgent):\n",
    "    def __init__(self,\n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 episodes=500):\n",
    "        super().__init__(state_space, \n",
    "                         action_space, \n",
    "                         episodes)\n",
    "        \"\"\"DDQN Agent on CartPole-v0 environment\n",
    "\n",
    "        Arguments:\n",
    "            state_space (tensor): state space\n",
    "            action_space (tensor): action space\n",
    "            episodes (int): number of episodes to train\n",
    "        \"\"\"\n",
    "\n",
    "        # Q Network weights filename\n",
    "        self.weights_file = 'ddqn_cartpole.h5'\n",
    "        print(\"-------------DDQN------------\")\n",
    "\n",
    "    def get_target_q_value(self, next_state, reward):\n",
    "        \"\"\"compute Q_max\n",
    "           Use of target Q Network solves the \n",
    "            non-stationarity problem\n",
    "        Arguments:\n",
    "            reward (float): reward received after executing\n",
    "                action on state\n",
    "            next_state (tensor): next state\n",
    "        Returns:\n",
    "            q_value (float): max Q-value computed\n",
    "        \"\"\"\n",
    "        # max Q value among next state's actions\n",
    "        # DDQN\n",
    "        # current Q Network selects the action\n",
    "        # a'_max = argmax_a' Q(s', a')\n",
    "        action = np.argmax(self.q_model.predict(next_state)[0])\n",
    "        # target Q Network evaluates the action\n",
    "        # Q_max = Q_target(s', a'_max)\n",
    "        q_value = self.target_q_model.predict(\\\n",
    "                                      next_state)[0][action]\n",
    "\n",
    "        # Q_max = reward + gamma * Q_max\n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "env_id = 'CartPole-v0'\n",
    "outdir = \"../Datos/dqn-%s\" % env_id\n",
    "no_render = True\n",
    "    \n",
    "# the number of trials without falling over\n",
    "win_trials = 100\n",
    "\n",
    "# the CartPole-v0 is considered solved if \n",
    "# for 100 consecutive trials, he cart pole has not \n",
    "# fallen over and it has achieved an average \n",
    "# reward of 195.0 \n",
    "# a reward of +1 is provided for every timestep \n",
    "# the pole remains upright\n",
    "win_reward = { 'CartPole-v0' : 195.0 }\n",
    "\n",
    "# stores the reward per episode\n",
    "scores = deque(maxlen=win_trials)\n",
    "logger.setLevel(logger.ERROR)\n",
    "    \n",
    "# crea el ambiente desde gym\n",
    "env = gym.make(env_id)\n",
    "\n",
    "\n",
    "if no_render:\n",
    "    env = wrappers.Monitor(env,\n",
    "                           directory=outdir,\n",
    "                            video_callable=False,\n",
    "                            force=True)\n",
    "else:\n",
    "    env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "env.seed(0)\n",
    "\n",
    "# instantiate the DQN agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "\n",
    "# should be solved in this number of episodes\n",
    "episode_count = 3000\n",
    " state_size = env.observation_space.shape[0]\n",
    "batch_size = 64\n",
    "\n",
    "# by default, CartPole-v0 has max episode steps = 200\n",
    "# you can use this to experiment beyond 200\n",
    "# env._max_episode_steps = 4000\n",
    "\n",
    "# Q-Learning sampling and fitting\n",
    "for episode in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # in CartPole-v0, action=0 is left and action=1 is right\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "         # in CartPole-v0:\n",
    "        # state = [pos, vel, theta, angular speed]\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # store every experience unit in replay buffer\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "    # call experience relay\n",
    "    if len(agent.memory) >= batch_size:\n",
    "         agent.replay(batch_size)\n",
    "    \n",
    "    scores.append(total_reward)\n",
    "    mean_score = np.mean(scores)\n",
    "    if mean_score >= win_reward[args.env_id] \\\n",
    "            and episode >= win_trials:\n",
    "        print(\"Solved in episode %d: \\\n",
    "                   Mean survival = %0.2lf in %d episodes\"\n",
    "                  % (episode, mean_score, win_trials))\n",
    "        print(\"Epsilon: \", agent.epsilon)\n",
    "            agent.save_weights()\n",
    "        break\n",
    "    if (episode + 1) % win_trials == 0:\n",
    "           print(\"Episode %d: Mean survival = \\\n",
    "                   %0.2lf in %d episodes\" %\n",
    "                  ((episode + 1), mean_score, win_trials))\n",
    "\n",
    "    # close the env and write monitor result info to disk\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video luego de entrenado el agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea_entrenado.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea despues de entrenar al agente. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
