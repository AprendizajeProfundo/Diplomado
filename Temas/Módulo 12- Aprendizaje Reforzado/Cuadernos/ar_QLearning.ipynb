{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Q-learning</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Ecuación de Belman</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1.[Markel Sanz, Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-2-q-learning-883cd42fb48e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Función de valor](#Función-de-valor)\n",
    "* [Q-Learning](#Q-Learning)\n",
    "* [El código](#El-código)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del bandido multibrazo, hemos descrito el problema del bandido multibrazo, y hemos introducido varios conceptos, como el estado, la acción, la recompensa, etc. Sin embargo, el problema del bandido multibrazo no representa el problema completo del aprendizaje reforzado. En los problemas de bandidos multibrazo, cada acción es completamente independiente de las anteriores, y el estado siempre es el mismo, como en el ejemplo de la lección del bandido multibrazo, donde siempre teníamos los 5 mismos brazos y su probabilidad de éxito no cambiaba en ningún momento.\n",
    "\n",
    "\n",
    "En el problema completo de aprendizaje por refuerzo, el estado cambia cada vez que ejecutamos una acción. Podemos representar el problema general de aprendizaje reforzado de la siguiente manera. \n",
    "\n",
    "1. El **agente** recibe la siguiente observación(que llamaremos estados en esta lección) del  **ambiente** (environment). los estados se denotaran por $s_i$\n",
    "2. El agente ejecuta entonces la **acción** que elija y le informa ambiente. Las acciones de denotaran $a_i$\n",
    "3. Al ejecutar esa acción, el ambiente responde proporcionando una **recompensa** y una nueva observación que llamaremos estado en esta lección. La recompensas se denotarán $r_i$.\n",
    "\n",
    "Este ciclo se puede observar en la siguiente imagen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/environment.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</figure> \n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, **la acción que el agente escoja no debe sólo depende de la recompensa  que vaya a recibir a corto plazo. Debe elegir las acciones que a largo plazo le traerán la máxima recompensa (o retorno) posible en todo el episodio (episode)**. \n",
    "\n",
    "Este ciclo trae una secuencia de estados, acciones y recompensas, desde el primer paso del ciclo hasta el último: \n",
    "\n",
    "$$\n",
    "s_1, a_1, r_1; s_2, a_2, r_2; \\ldots; s_T, a_T, r_T. \n",
    "$$\n",
    "\n",
    "Aquí, $T$ indica el fin del episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cuantificar cuanta recompensa obtendrá el agente a largo plazo desde cada estado, introducimos la función de valor $V(s)$. Esta función produce una estimación de la recompensa que obtendrá el agente hasta el final del episodio, empezando desde el estado s. Si conseguimos estimar este valor correctamente, podremos decidir ejecutar la acción que nos lleve al estado con el valor más alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver el problema del aprendizaje reforzado, el agente debe aprender a escoger la mejor acción posible para cada uno de los estados posibles. Para ello, el algoritmo **Q-Learning** intenta aprender cuanta recompensa obtendrá a largo plazo para cada pareja de estados y acciones $(s,a)$. \n",
    "\n",
    "A esa función se la llama la **función de acción-valor** (action-value function) y este algoritmo la representa como la función $Q(s,a)$, la cual devuelve la recompensa que el agente recibirá al ejecutar la acción a desde el estado $s$, y asumiendo que seguirá la misma política dictada por la función $Q$ hasta el final del episodio. \n",
    "\n",
    "\n",
    "Por lo tanto, si desde el estado $s$, tenemos dos acciones disponibles, $a_1$ y $a_2$, la función $Q$ nos proporcionará los **valores-Q** (Q-values) de cada una de las acciones. Por ejemplo, si $Q(s,a_1)=1$ y $Q(s,a_2)=4$, el agente sabe que la acción $a_2$ es mejor y le traerá mayor recompensa, por lo que será la acción que ejecutará, una vez haya sido entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Entorno de cuadrícula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"../Imagenes/cuadricula.png\" width=\"500\" height=\"500\" align=\"left\"/>\n",
    "</figure>\n",
    "En este ejemplo, tenemos un entorno en el que el agente empieza en el estado inicial $s_i$, y debe elegir entre moverse a la izquierda o a la derecha. \n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Si llega al estado de más a la izquierda, el episodio termina y el agente recibe una recompensa de -5. \n",
    "2. Por otro lado, si llega al estado de más a la derecha, el episodio termina y el agente recibe una recompensa de +5. \n",
    "\n",
    "El agente debe aprender a evitar el estado de -5 y moverse hacia el estado de +5. Si la política que aprende siempre termina en el estado con mayor recompensa, diremos que ha encontrado la **política óptima**(optimal policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver el problema, el algoritmo  *Q-Learning* utiliza la **ecuación de Bellman**. Esta ecuación se usa para aprender los *valores-Q* y es dada por\n",
    "\n",
    "$$\n",
    "\\large\n",
    "Q(s,a) = r + \\gamma \\max_{a^{'}}  Q(s^{'},a^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La explicación para esta ecuación es la siguiente.  El *valor-Q* del estado *s* y la acción *a* ($Q(s, a)$) debe ser igual a la recompensa *r* obtenida al ejecutar esa acción, más el *valor-Q* de ejecutar la mejor acción posible $a'$ desde el próximo estado $s'$, multiplicado por un **factor de descuento**  (discount factor), que es un valor con rango $\\gamma \\in (0, 1]$. Este valor $\\gamma$ se usa para decidir cuánto peso le queremos dar a las recompensas a corto y a largo plazo, y es un hiperparámetro que debemos decidir nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos definiendo nuestro entorno. \n",
    "\n",
    "1. Las recompensas son 0 para todos los estados, excepto para el estado de más a la izquierda y el de más a la derecha, que tienen recompensas de -5 y +5 respectivamente. \n",
    "2. También definimos una lista que define si un estado es final/terminal o no. \n",
    "3. Por último creamos la lista de variables llamada *Q_values*, donde guardaremos los *valores-Q* para todos los pares de estados y acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state_rewards = [-5, 0, 0, 0, 0, 0, 5]\n",
    "final_state = [True, False, False, False, False, False, True]\n",
    "Q_values = [[0.0, 0.0], \n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0]] # (s,a) matriz. [izquierda, derecha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos una función que escoja una acción usando la política **ε-voraz** para un estado que pasaremos como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(epsilon, state):\n",
    "    \"\"\"Toma una acción con probabilidad epsilon, sino toma la mejor acción.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        # exploración\n",
    "        return np.random.randint(0, 2) # acción aleatoria\n",
    "    else:\n",
    "        # explotación\n",
    "        return np.argmax(Q_values[state]) # Acción voraz (greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q_values[state] contiene los Q-values para cada una de las dos acciones. Entonces la función responde con la acción que tiene mayor Q-value para el *state* pasado a la función (que corresponde al estado actual del environment), en el caso de explotación. En el caso de exploración regrese aleatoriamente y con probabilidad epsilon cualquier accón disponible (en este caso alguna de las dos posibles acciones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También crearemos una función que ejerza de entorno. Le pasaremos el estado y la acción seleccionada por el agente, y nos devolverá la recompensa **r** y el siguiente estado **s’**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(state, action):\n",
    "    \"\"\"Aplica la acción seleccionada y obtiene la recompesna y el siguiente estado.\"\"\"\n",
    "    if action == 0:\n",
    "        next_state = state-1\n",
    "    else:\n",
    "        next_state = state+1\n",
    "    \n",
    "    return state_rewards[next_state], next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, decidimos varios hiperparámetros y ejecutamos el algoritmo que aprende usando el algoritmo de *Q-Learning* y la *ecuación de Bellman*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "epsilon = 0.2\n",
    "discount = 0.9 # \n",
    "\n",
    "for episode in range(num_episodes+1):\n",
    "    initial_state = 3 # estado en el medio\n",
    "    state = initial_state\n",
    "    while not final_state[state]: # corre hasta el final del episodio\n",
    "        # el agente selecciona una acción\n",
    "        action = select_epsilon_greedy_action(epsilon, state)\n",
    "        # se informa la acción al ambiente\n",
    "        # el ambiente cambia su estado\n",
    "        # regresa al recompensa y el siguiente estado\n",
    "        reward, next_state = apply_action(state, action)\n",
    "        # mejora los Q-values con la ecuación de  Bellman\n",
    "        if final_state[next_state]:\n",
    "            Q_values[state][action] = reward\n",
    "        else:\n",
    "            Q_values[state][action] = reward + discount * max(Q_values[next_state])\n",
    "        state = next_state\n",
    "        # print\n",
    "        print('episode: ', episode, 'Q_values:', Q_values)\n",
    "         \n",
    "# Imprime los valores Q para ver si la acción a la derecha es siempre mejor que la acción a la izquierda\n",
    "# excepto para los estados 0 y 6, que son estados terminales y no puede tomar\n",
    "# cualquier acción de ellos, por lo que no importa.\n",
    "print('los Q-values son:')\n",
    "print(Q_values)\n",
    "action_dict = {0:'izquierda', 1:'derecha'}\n",
    "state = 0\n",
    "\n",
    "for Q_vals in Q_values:\n",
    "    print('La mejor acción para el estado {} es {}'.format(state, \n",
    "                                             action_dict[np.argmax(Q_vals)]))\n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "episode:  1000 Q_values: [[0.0, 0.0], [-5, 3.2805], [2.9524500000000002, 3.645], [3.2805, 4.05], [3.645, 4.5], [4.05, 5], [0.0, 0.0]]\n",
    "los Q-values son:\n",
    "[[0.0, 0.0], [-5, 3.2805], [2.9524500000000002, 3.645], [3.2805, 4.05], [3.645, 4.5], [4.05, 5], [0.0, 0.0]]\n",
    "La mejor acción para el estado 0 es izquierda\n",
    "La mejor acción para el estado 1 es derecha\n",
    "La mejor acción para el estado 2 es derecha\n",
    "La mejor acción para el estado 3 es derecha\n",
    "La mejor acción para el estado 4 es derecha\n",
    "La mejor acción para el estado 5 es derecha\n",
    "La mejor acción para el estado 6 es izquierda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al terminar, observamos los *valores-Q* aprendidos y la mejor acción para cada estado. Al ejecutarlo vemos que ha aprendido exactamente lo que queríamos, la política óptima, a moverse hacia la derecha siempre. Hay que tener en cuenta que los *valores-Q* han sido descontados por el factor de descuento, que en este caso es 0.9. Los estados de la derecha y la izquierda tienen valores de 0.0 porque son terminales y el episodio termina al llegar a ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
