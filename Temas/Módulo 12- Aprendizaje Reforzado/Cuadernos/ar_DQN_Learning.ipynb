{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning con redes neuronales, algoritmo DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Oleg Jarma, ojarmam@unal.edu.co\n",
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Ecuación de Bellman en DQN](#Ecuación-de-Bellman-en-DQN)\n",
    "* [El código](#El-código)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adaptado de Markel Sanz, [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)\n",
    "2. Sutton, R. S., & Barto, A. G. (2018). [Reinforcement learning: An introduction. MIT press](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "3. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lección del algoritmo Q-Learning, este  funciona muy bien cuando el entorno es simple y la función $Q(s,a)$ se puede representar como una tabla o matriz de valores. \n",
    "\n",
    "Pero cuando hay miles de millones de estados diferentes y cientos de acciones distintas, la tabla se vuelve enorme, y no es viable su utilización. \n",
    "\n",
    "Por ello, Mnih et al. inventaron el algoritmo [Deep Q-Network o DQN.](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)  \n",
    "\n",
    "Este algoritmo combina el algoritmo *Q-learning* con redes neuronales profundas (Deep Neural Networks). \n",
    "\n",
    "Como es sabido en el campo de la IA, las redes neuronales son una fantástica manera de aproximar funciones no lineales. \n",
    "\n",
    "Por lo tanto, este algoritmo usa una red neuronal para aproximar la función *Q*, evitando así utilizar una tabla para representar la misma. \n",
    "\n",
    "En realidad, utiliza dos redes neuronales para estabilizar el proceso de aprendizaje. \n",
    "\n",
    "1. La primera, la *red neuronal principal* (main Neural Network), representada por los parámetros $\\theta$, se utiliza para estimar los valores-Q del estado s y acción a actuales: $Q(s, a;\\theta)$. \n",
    "2. La segunda, la *red neuronal objetivo* (target Neural Network), parametrizada por $\\theta^{'}$, tendrá la misma arquitectura que la red principal pero se usará para aproximar los *valores-Q* del siguiente estado $s'$ y la siguiente acción $a'$. \n",
    "\n",
    "El aprendizaje ocurre en la red principal y no en la objetivo.\n",
    "\n",
    "La red objetivo se congela (sus parámetros no se cambian) durante varias iteraciones (normalmente alrededor de 10000), y después los parámetros de la red principal se copian a la red objetivo, transmitiendo así el aprendizaje de una a otra, haciendo que las estimaciones calculadas por la red objetivo sean más precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuación de Bellman en DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación de Bellman tiene esta forma ahora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "Q(s,a; \\theta) = r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para poder entrenar una red neuronal, necesitamos una función de pérdida o coste (loss or cost function), la cual definimos como el cuadrado de la diferencia entre ambos lados de la ecuación de Bellman:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "L(\\theta)= \\mathbb{E}[  r + \\lambda \\max_{a^{'}}  Q(s^{'},a^{'}; \\theta^{'})- Q(s,a; \\theta)]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta será la función que minimizaremos usando el algoritmo de descenso de gradiente (gradient descent), el cuál se ejecuta automáticamente si usamos una librería de diferenciación automática con redes neuronales, como TensorFlow.\n",
    "\n",
    "Aquí tenemos el entorno (environment) conocido como CartPole. Se ha utilizado la librería *OpenAI Gym* para visualizar y ejecutar este entorno. En este entorno, el objetivo es mover el carro hacia la derecha y la izquierda, con el objetivo de equilibrar el palo. Y si el palo se tuerce más de 15 grados respecto al eje vertical, el episodio terminará y volveremos a empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar depencendias para renderizar OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mejor correr en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this asap since it takes 30 seconds.\n",
    "%%capture\n",
    "#!apt-get update\n",
    "#!pip install pyglet\n",
    "#!pip install gym pyvirtualdisplay\n",
    "#!pip install xvfbwrapper\n",
    "#!apt-get install -y xvfb python-opengl ffmpeg\n",
    "#!pip install tensorflow==2.1.* \n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Crea un agente DNQ y funciones utilitarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment and get action and state spaces.\n",
    "env = gym.make('CartPole-v0')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el algoritmo DQN, empezaremos creando las dos redes neuronales, la principal (main_nn) y la objetivo (target_nn). Ésta última será una copia de la principal, pero con sus propios pesos. \n",
    "\n",
    "También necesitaremos un optimizador (optimizer) y una función de pérdida (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensoflow as tf\n",
    "\n",
    "class DQN(tf.keras.Model):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32) # No activation\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "main_nn = DQN()\n",
    "target_nn = DQN()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos el buffer donde guardaremos la experiencia recogida para usarla después y entrenar la red neuronal.\n",
    "\n",
    "Usaremos *deque* — Cola doblemente terminada.\n",
    "\n",
    "Una cola doblemente terminada, o *deque*, admite agregar y eliminar elementos de cualquier extremo de la cola. Las pilas y colas más comunes son formas degeneradas de deques, donde las entradas y salidas están restringida a un solo extremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], [] \n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "    \n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "    \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También crearemos una función auxiliar para ejecutar la política ε-voraz, y otra para entrenar la red neuronal principal usando los datos guardados en el buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = tf.random.uniform((1,))\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() # Random action (left or right).\n",
    "    else:\n",
    "        return tf.argmax(main_nn(state)[0]).numpy() # Greedy action for state.\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\"\"\"\n",
    "    # Calculate targets.\n",
    "    next_qs = target_nn(next_states)\n",
    "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
    "    target = rewards + (1. - dones) * discount * max_next_qs\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        qs = main_nn(states)\n",
    "        action_masks = tf.one_hot(actions, num_actions)\n",
    "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
    "        loss = mse(target, masked_qs)\n",
    "  \n",
    "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras esto, definiremos los hiperparámetros y empezaremos a entrenar el algoritmo. Para ello, primero usaremos la **política ε-voraz** para jugar al juego y recoger experiencia para poder aprender de esos datos. \n",
    "\n",
    "Después de terminar un episodio jugado, llamaremos a la función que entrena la red neuronal. \n",
    "\n",
    "Cada 2000 pasos de entrenamiento, copiaremos los pesos de la red neuronal principal a la red neuronal objetivo. También reduciremos el valor de **epsilon (ε)**, para empezar con un valor de exploración alto y bajarlo poco a poco. \n",
    "\n",
    "Así, veremos cómo el algoritmo empieza a aprender a jugar al juego y la recompensa obtenida jugando al juego irá mejorando poco a poco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayBuffer(100000)\n",
    "cur_frame = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = tf.expand_dims(state, axis=0)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    buffer.add(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      target_nn.set_weights(main_nn.get_weights())\n",
    "\n",
    "    # Train neural network.\n",
    "    if len(buffer) >= batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      loss = train_step(states, actions, rewards, next_states, dones)\n",
    "  \n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "    \n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
    "          f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el agente ha aprendido a maximizar la recompensa para el entorno CartPole, haremos que el agente interactúe con el entorno una vez más, y visualizamos el resultado, viendo como ahora es capaz de mantener el palo equilibrado durante 200 frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea_entrenado.gif\" width=\"400\" height=\"300\" align=\"center\"/>\n",
    "    <figcaption>\n",
    "<p style=\"text-align:center\">Ejemplo de la polea despues de entrenar al agente. Original en Open AI</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Introducción al aprendizaje por refuerzo](https://medium.com/@markelsanz14/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-3-q-learning-con-redes-neuronales-algoritmo-dqn-bfe02b37017f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Result of Trained DQN Agent on Cartpole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video():\n",
    "  \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Video not found\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "state = env.reset()\n",
    "done = False\n",
    "ep_rew = 0\n",
    "while not done:\n",
    "  env.render()\n",
    "  state = tf.expand_dims(state, axis=0)\n",
    "  action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
    "  state, reward, done, info = env.step(action)\n",
    "  ep_rew += reward\n",
    "print('Episode reward was {}'.format(ep_rew))\n",
    "env.close()\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
