{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6f8f0e",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Método entropía cruzada</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dca78",
   "metadata": {},
   "source": [
    "<center>Aprendizaje reforzado</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5846fe",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d09f7",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072351c",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b23f0c",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0a794",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc4de4",
   "metadata": {},
   "source": [
    "5. Oleg Jarma, ojarmam@unal.edu.co \n",
    "6. Laura Lizarazo, ljlizarazore@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd63d24",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ee4c4",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El ambiente CartPole](#El-ambiente-CartPole)\n",
    "* [El ambiente CartPole](#El-ambiente-CartPole)\n",
    "* [El método de entropía cruzada en CartPole](#El-método-de-entropía-cruzada-en-CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección vamos a entrenar un agente con base en una red neuronal, la cual será una red de clasificación con función de pérdida entropía cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b92992-3c1b-4af4-b8b0-44ac0460c5b4",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">La API OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa5bf5-614f-4420-a33e-81936045c7a0",
   "metadata": {},
   "source": [
    "La API `OpenAI Gym` tiene una rica colección de ambientes para experimentos de aprendizaje reforzado (AR), usando una interfaz unificada.\n",
    "\n",
    "La clase principal de Gym es *Env* (de environment). Sus métodos y atributos proveen la información necesaria para poder entrenar agentes que interactúen con el medio ambientes. Las piezas más importante disponibles con *Env* son:\n",
    "\n",
    "Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/RL_components.jpeg\" width=\"300\" height=\"300\" align=\"right\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar un acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*).\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e5fb-ca34-4653-9f3e-c709d652dd81",
   "metadata": {},
   "source": [
    "### El espacio de acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157683-e2d3-43ff-8f29-7bd14309fffe",
   "metadata": {},
   "source": [
    "Las acciones que puede realizar un agente puede ser discretas, continuas o incluso una combinación de ambas. Por ejemplo en un automóvil manejado de manera autónoma puede ser posible oprimir varios botones a la vez (discreto), oprimir el pedal del acelerador o del freno (continuo) y así. El espacio de acciones contiene o modela todas las posibilidades de acción del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ad16b-e42c-4957-8524-5476670451bd",
   "metadata": {},
   "source": [
    "### El espacio de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1305-62f4-4dd5-91a4-fe68ce77191e",
   "metadata": {},
   "source": [
    "Las observaciones son piezas de información que el ambiente puede entregar al agente en cualquier momento del tiempo, además de la recompensa. Las observaciones pueden ser muy simples como un arreglo unidimensional de números o  complejos tensores, como las imágenes provenientes de varias cámaras del automóvil.\n",
    "\n",
    "Es claro que acciones y observaciones tienen similaridades, por lo que Gym dispone de una clase abstracta llamada *Space* que permite derivar espacios de acciones y espacios de observaciones. La siguiente imagen muestra una diagrama de la clase *Space*.\n",
    "\n",
    "Existen otras subclases de *Space*, pero estas son las más comúnmente utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374739ea-141b-4a29-89e1-2ef87bed6b80",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/jerarquia_clase_Space_Gym.jpeg\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Jerarquía de la clase Space de OpenAI Gym. Fuente: [Maxim Lapan](http://library.lol/main/2E612EDEF1D325B87C7F5B1FB5542964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc147e-8c2f-4028-ad45-aa897b0fd0fb",
   "metadata": {},
   "source": [
    "La clase abstracta *Space* incluye dos métodos relevantes:\n",
    "\n",
    "* *sample()*: retorna una muestra aleatoria del espacio.\n",
    "* *contains(x)*: chequea si el argumento es un elemento del dominio del espacio.\n",
    "\n",
    "Ambos métodos son abstractos y deben ser implementados en las subclases de *Space*.\n",
    "\n",
    "* La clase *Discrete* representa un conjunto de ítems mutuamente excluyentes, numerados entre 0 y $n-1$. Su único atributo *n*, es una cuenta de los ítems que la clase describe. Por ejemplo. *Discrete(n=4)* puede representar por ejemplo un espacio de acciones de cuatro direcciones para moverse: *[left, right, up, down]*.\n",
    "\n",
    "* La clase *Box* representa un tensor *n*-dimensional de números racionales con intervalos *[low, high]*. Por ejemplo puede representar el pedal de un acelerador con un valor simple entre 0.0 y 1.0. En pedal sería codificado en tal caso como *Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)*. Otro ejemplo puede ser la observación de la pantalla en un juego Atari, la cual es RGB de tamaño 210*160: *Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)*.\n",
    "\n",
    "* La última subclase de *Space* es *Tuple*, la cual permite combinar varias instancias de *Space* juntas. esta subclase permite crear espacios de acciones y observaciones de la complejidad que se desee.\n",
    "\n",
    "Un ejemplo del uso de tupla puede ser el siguiente. Supongamos que deseamos modelar algunos controles de un automóvil. Para empezar la dirección (el ángulo de la dirección, en realidad), el acelerador y el freno pueder cambiar en cada instante de tiempo. Por otro lado, podemos tener botones discretos para representar la luces direccionales *(apagadas, izquierda, derecha)* y la bocina *(encendida, apagada)*. Entonces, podemos crear *Tupla(spaces=(Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(3), Discrete(2)\n",
    ")*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b93b-c965-4151-9465-f48ffc6d15e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### El ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234d15-59fb-48f8-b091-8a19fc0bfc01",
   "metadata": {},
   "source": [
    "El ambiente es representado por la clase *Env*. Todo ambiente tiene dos miembros de tipo *Space*: \n",
    "\n",
    "* *action_space*: acciones permitidas que entrega el ambiente al agente.\n",
    "* *observation_space*: observaciones proveidas por el ambiente al agente.\n",
    "\n",
    "Además el ambiente tiene los siguientes métodos:\n",
    "\n",
    "* *reset()*: coloca el ambiente en el estado inicial.\n",
    "* *step()*: Permite al agente tomar una acción y retorna información como respuesta a la acción: la siguiente observación, la recompensa y la bandera indicando si el episodio ha  finalizado. Esta es la pieza central de la funcionalidad del ambiente.\n",
    "\n",
    "Estas características permiten crear código genérico que podría trabajar con cualquier ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fea79-4bca-4a0e-800c-38a8c40dc71b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El ambiente CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c5078-0287-4a92-af21-9fd0d6cca4a2",
   "metadata": {},
   "source": [
    "Fuente [OpenAI Gym](https://gym.openai.com/)\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Ejemplo_polea.gif\" width=\"200\" height=\"200\" align=\"left\"/>\n",
    "</center>\n",
    "</figure>\n",
    "    \n",
    "Este es nuestro primer acercamiento a OpenAI Gym. En este primer  ejemplo conocido como CartPole es un clásico en la teoría de control. En la animación *gif* a la izquierda  observa el objeto de interés. Es una palo, sostenido en la parte inferior por un pasador unido a una base. Ignoramos posibles complicaciones como la fricción. Es decir suponemos que el palo se mueve libremente sobre el pasador. \n",
    "\n",
    "\n",
    "Obviamente el palo puede caer muy fácilmente, por lo que el propósito es mover la base hacia la izquierda o hacia la derecha para evitar que el palo caiga. Ese es el problema a resolver. El problema puede ser abordado como un problema de control que lleva a diseñan un modelo en ecuaciones diferenciales. Pero en aprendizaje reforzado buscamos resolver el problema, sin utilizar los conceptos de la física correspondientes. De momento vamos a explorar un poco el  ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e471783-c9ea-4fc6-8a23-68eee6d30394",
   "metadata": {},
   "source": [
    "La observación que entrega el ambiente es un arreglo de cuatro posiciones conteniendo respectivamente  *coordenada del centro de masas*, *velocidad*, *ángulo con respecto a la plataforma*, y *velocidad angular*.\n",
    "\n",
    "El espacio de acciones es de tipo *Discrete* y contiene dos elementos 0 y 1, o *izquierda* y *derecha*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0164-7f64-4ddc-ab6d-dc02d10348cd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El método de entropía cruzada en CartPole</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfb14a-2eda-4bb7-8215-77acbaa0ea52",
   "metadata": {},
   "source": [
    "El núcleo de nuestro modelo es una red neuronal(RN) con una capa oculta de de 128 neuronas con un rectificador lineal (ReLU) como función de activación. El tamaño 128 es completamente arbitrario en este ejemplo. Sin embargo nuestro método es muy robusto y converge rápidmente.\n",
    "\n",
    "Comenzamos definiendo algunas constantes globales:\n",
    "\n",
    "- HIDDEN_SIZE: tamaño de la capa oculta.\n",
    "- BATCH SIZE: número de episodios en cada iteración.\n",
    "- PERCENTILE: porcentaje de recompensas totales que vamos a conservar para determinar episodios *elite*. \n",
    "\n",
    "Es decir, vamos a dejar por fuera del entrenamiento de la red, los datos asociados a episodios en donde la recompensa total esté por debajo del 30\\% inferior.\n",
    "\n",
    "Adicionalmente definimos la red neuronal que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f29f52-a9e1-4ad4-9c3d-e499f868e862",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf24622c-e1ac-44ca-9d61-2438cb53cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283e9b4-0de0-4a20-bbaa-3bc48fc46dcd",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98879d9c-8e42-4835-802b-249eda22ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd174c-88ed-4739-affb-0bafe5124356",
   "metadata": {},
   "source": [
    "### Red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057d957d-cc69-4b93-bb02-8266ae4a9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron con una capa oculta\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cb2a9-d77a-4009-8247-0078bec53bd4",
   "metadata": {},
   "source": [
    "### Información de los episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfb0e9-31c4-43be-b560-a039aef307a5",
   "metadata": {},
   "source": [
    "Vamos a definir dos clases auxiliares para manejar la información de los episodios. Estas son clases tupla de tipo `namedtuple`. Este tipo de tupla definida en el módulo estándard *collections* de Python 3+. Es una tupla en la cual cada elemento recibe un nombre, por el cual puede ser llamado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c630da5-15c1-482a-9126-63f90223b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clases namedtuple para manejo de información\n",
    "# de episodios\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d962ff-93b7-4783-be0a-a6dc208cc3b9",
   "metadata": {},
   "source": [
    "*EpisodeStep* se usará para representar la información de un paso simple que el agente hace en un episodio. Tal información es la observación recibida y la acción ejecutado por el agente.\n",
    "\n",
    "*Episode* mantiene la información global de un episodio: la recompensa total recibida en el episodio y la colección completa de EpisodeStep de tal episodio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb945a5-77cb-4a8e-b405-7c13335f98e6",
   "metadata": {},
   "source": [
    "### Como sucede el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ecec7-7a10-4767-9163-386c7ea2ddec",
   "metadata": {},
   "source": [
    "El entrenamiento sucederá de la siguiente forma.\n",
    "\n",
    "1. Se selecciona el 70% de los episodios con más alta recompensa. Estos episodios los llamamos *élite*.\n",
    "1. De los episodios *élite* se extrae la información de observaciones para la entrada de la red y acción para la salida de la red. La red es de clasificación, por lo que se usará como función de pérdida la entropía cruzada, la cual se calcula directamente de los valores de la salida de la red que denominamos en este caso `logits`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d8adc-f826-4e30-bed2-144c7936bb32",
   "metadata": {},
   "source": [
    "#### Objetivo del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd88003-12af-4c26-968a-f180a82aaef1",
   "metadata": {},
   "source": [
    "Buscamos entrenar la red neuronal, con la información de las observaciones, para que prediga la distribución de probabilidades de la siguiente acción. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff804985-4986-4cc9-b338-0259d49f3af8",
   "metadata": {},
   "source": [
    "#### Que hace el agente en cada iteración de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab9062-1d47-4147-879a-e617274ba623",
   "metadata": {},
   "source": [
    "1. El agente en cada paso recibe del ambiente la recompensa, la nueva observacion y las posibles acciones disponibles, que este caso siempre son dos: izquierda o derecha.\n",
    "1. Entonces le entrega a la red la nueva observación y recibe como respuesta de un vector de probabilidades para la siguiente acción.\n",
    "3. El agente selecciona la siguiente acción de forma aleatoria, usando el modelo de probabilidad recibido.\n",
    "4. El agente informa al ambiente sobre su siguiente acción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ed70e-2d54-47e0-a7fd-3d8fcdaf2a9e",
   "metadata": {},
   "source": [
    "#### Nota sobre eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a8a6e-8203-49f5-ab00-6ab328f0178e",
   "metadata": {},
   "source": [
    "Es más eficiente usar la función de pérdida de entropía cruzada de Pytorch basada en los logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9bb92-ed65-457f-ab4f-b9fc1f9e6b73",
   "metadata": {},
   "source": [
    "### Función de iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b50fb-2ef4-4664-bd03-dc61a49fa22f",
   "metadata": {},
   "source": [
    "Esta función genera lotes (batches) de episodios. Esta función es el núcleo de la acción del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c4d64d-cf2a-4419-bb7b-0285ca9d7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generador de lotes de datos\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb1a61-6138-4dc0-ac76-14d18a389589",
   "metadata": {},
   "source": [
    "Observe que la función *iterate_batches* realmente es un generador. La línea 25 muestra el `yield` de la función, lo que la hace un generador. \n",
    "\n",
    "La función ejecuta exáctamente *bath_size* episodios. En nuestro caso será 16.\n",
    "\n",
    "la función hace los siguiente dentro del ciclo while:\n",
    "\n",
    "1. Convierte la observación recibida en un tensor. línea 10.\n",
    "1. Para la observación a la red y recibe la nueva distribución de acciones y al conveirte a un arreglo numpy. Líneas 11 y 12.\n",
    "1. Selecciona aleatoriamente la siguiente acción con base en el modelo de probabilidad recibido. Línea 13.\n",
    "1. Entrega la acción al ambiente y éste le retorna la nueva observación, recompensa, la indicación de si el episodio terminó. Se ignora la el vector de posibles acciones, porque no se requiere en este caso. (14).\n",
    "1. Se acumula la recompensa del paso y se guarda la información del paso  en las tuplas namedtuple (15,16 y 17).\n",
    "1. Al finalizar el episodio, se guarda la información de todo el episodio en la tupla Episode y se sube a la lista del lote(batch). Se reinician los objetos para un nuevo episodio. Si el lote de episodios se ha completado se entrega el lote y se vacía el lote de episodios. Sino se iniciará un nuevo episodio. (18-27)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94638b9-fb65-445a-b045-a1e005468969",
   "metadata": {},
   "source": [
    "### Filtro de lotes de episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89135a1-fe79-463a-927b-10b7ab32f9f0",
   "metadata": {},
   "source": [
    "Como se dijo arriba, la recompensa acumulada de cada episodio se usa como insumo para determinar los episodios élite. El entrenamiento de la red se realiza únicamente con estos episodio elite.\n",
    "\n",
    "La función *filter_batch* recibe un lote de entrenamiento entregado por el generador de lotes *iterate_batches* y el porcentaje de episodios que se consideran como *élite*.\n",
    "\n",
    "La función hace lo siguiente:\n",
    "\n",
    "1. Extrae las recompensas totales de cada episodio de los (batch) de entrenamiento y las coloca en una lista (rewards). Línea 2.\n",
    "1. Calcula la frontera inferior, para definir los episodios *élite*. Se calcula el percentil 70 de las recompensas. (3).\n",
    "1. Se calcula la recompensa promedio. Esta recompensa promedio se usará para determinar cuando parar el entrenamiento.\n",
    "1. Se extrae la observación y la acción de cada episodio *élite* y se suben a listas. (6-12).\n",
    "1. Se suben las listas a tensores y se retornan, junto con la frontera inferior de recompensa y la recompsnsa promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c9d03e-05c4-49d3-b6f1-f5d741960843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a30e77-06df-4f56-bfce-4d0473b9d46d",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b040e8e-6e97-49da-a662-5b5b4d033492",
   "metadata": {},
   "source": [
    "Finalmente tenemos el ciclo completo de entrenamiento. Usaremos un objeto SummaryWriter para guardar algunos objetos y verlos en Tensorboard.\n",
    "\n",
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10b78e0-734e-4f4d-a645-a381d6ca9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.659, reward_mean=17.5, rw_bound=17.5\n",
      "1: loss=0.660, reward_mean=14.3, rw_bound=16.0\n",
      "2: loss=0.677, reward_mean=19.5, rw_bound=19.5\n",
      "3: loss=0.682, reward_mean=23.1, rw_bound=28.5\n",
      "4: loss=0.666, reward_mean=29.6, rw_bound=34.5\n",
      "5: loss=0.648, reward_mean=31.6, rw_bound=37.0\n",
      "6: loss=0.649, reward_mean=29.8, rw_bound=31.5\n",
      "7: loss=0.646, reward_mean=40.1, rw_bound=47.5\n",
      "8: loss=0.635, reward_mean=41.1, rw_bound=44.0\n",
      "9: loss=0.606, reward_mean=42.2, rw_bound=56.0\n",
      "10: loss=0.607, reward_mean=40.5, rw_bound=54.0\n",
      "11: loss=0.608, reward_mean=57.2, rw_bound=65.0\n",
      "12: loss=0.594, reward_mean=58.6, rw_bound=86.5\n",
      "13: loss=0.585, reward_mean=63.3, rw_bound=63.5\n",
      "14: loss=0.572, reward_mean=64.1, rw_bound=71.5\n",
      "15: loss=0.583, reward_mean=79.7, rw_bound=89.0\n",
      "16: loss=0.569, reward_mean=79.7, rw_bound=100.0\n",
      "17: loss=0.550, reward_mean=70.9, rw_bound=82.0\n",
      "18: loss=0.543, reward_mean=60.6, rw_bound=71.0\n",
      "19: loss=0.547, reward_mean=79.1, rw_bound=91.5\n",
      "20: loss=0.546, reward_mean=80.9, rw_bound=96.5\n",
      "21: loss=0.520, reward_mean=89.9, rw_bound=102.0\n",
      "22: loss=0.545, reward_mean=85.1, rw_bound=90.0\n",
      "23: loss=0.532, reward_mean=94.2, rw_bound=102.0\n",
      "24: loss=0.525, reward_mean=110.2, rw_bound=139.0\n",
      "25: loss=0.532, reward_mean=101.9, rw_bound=126.5\n",
      "26: loss=0.512, reward_mean=105.8, rw_bound=121.0\n",
      "27: loss=0.494, reward_mean=108.1, rw_bound=122.0\n",
      "28: loss=0.515, reward_mean=110.4, rw_bound=125.5\n",
      "29: loss=0.518, reward_mean=109.0, rw_bound=117.5\n",
      "30: loss=0.493, reward_mean=118.2, rw_bound=132.0\n",
      "31: loss=0.508, reward_mean=120.3, rw_bound=142.5\n",
      "32: loss=0.495, reward_mean=121.4, rw_bound=141.5\n",
      "33: loss=0.496, reward_mean=129.0, rw_bound=143.5\n",
      "34: loss=0.494, reward_mean=127.1, rw_bound=138.5\n",
      "35: loss=0.489, reward_mean=141.9, rw_bound=166.0\n",
      "36: loss=0.503, reward_mean=158.1, rw_bound=197.0\n",
      "37: loss=0.497, reward_mean=150.9, rw_bound=175.5\n",
      "38: loss=0.485, reward_mean=170.4, rw_bound=200.0\n",
      "39: loss=0.498, reward_mean=169.1, rw_bound=197.5\n",
      "40: loss=0.485, reward_mean=182.2, rw_bound=200.0\n",
      "41: loss=0.503, reward_mean=180.6, rw_bound=200.0\n",
      "42: loss=0.497, reward_mean=198.3, rw_bound=200.0\n",
      "43: loss=0.498, reward_mean=200.0, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = \\\n",
    "            filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Resuelto!\")\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee84771b-a961-45a0-975d-4b78c7e55806",
   "metadata": {},
   "source": [
    "### Abre Tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f09fa6-ec73-48ae-b060-25dab389b078",
   "metadata": {},
   "source": [
    "Para explorar como ha ido la pérdidad, la frontera inferior de recompensa y la recompensa promedio.\n",
    "\n",
    "Si desea ver al actividad del agente descomente la línea 3 para tener un monitor de actividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b178a6-5f6e-47f1-9c28-d0388c7f4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f78e08a-b693-4882-84ab-cddfa22598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cierra el ambiebte. LA venta X11 se cierra.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5011b-6e7f-4323-bfb9-11c692328d16",
   "metadata": {},
   "source": [
    "### Implementacion usando clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f103279-2e06-4573-bcf7-e1588fc9c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Perceptron with a hidden layer\n",
    "    '''\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss = None\n",
    "        self.iteration = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compil(self, optimizer, loss):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, epochs=1, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            predicts = self.forward(X)\n",
    "            loss = self.loss(predicts, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.iteration += 1\n",
    "            if verbose:\n",
    "                print(\"iteration = %d: loss=%.3f \" % (self.iteration, loss.item()))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa86fd8-7103-4dfd-9684-3f00b0f19297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,  batch_size, percentile):\n",
    "        self.percentile = percentile\n",
    "        self.batch_size = batch_size\n",
    "        self.env = None\n",
    "        self.net = None\n",
    "        self.batch = self.iterate_batches()#generator of batches\n",
    "\n",
    "    def set_net(self, net):\n",
    "        self.net = net\n",
    "    \n",
    "    def set_env(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    \n",
    "    def iterate_batches(self):\n",
    "        Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "        EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "        batch = []\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = []\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        obs = self.env.reset() # first observation\n",
    "        while True:\n",
    "            obs_v = torch.FloatTensor([obs])\n",
    "            act_probs_v = sm(self.net(obs_v)) # compute probabilities for next action\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            action = np.random.choice(len(act_probs), p=act_probs) # select action\n",
    "            next_obs, reward, is_done, _ = self.env.step(action) # send action to env\n",
    "            episode_reward += reward\n",
    "            step = EpisodeStep(observation=obs, action=action)\n",
    "            episode_steps.append(step)\n",
    "            if is_done:\n",
    "                e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "                batch.append(e)\n",
    "                episode_reward = 0.0\n",
    "                episode_steps = []\n",
    "                next_obs = self.env.reset()\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "            obs = next_obs\n",
    "        \n",
    "    \n",
    "    def filter_batch(self):\n",
    "        batch = next(self.batch) # generate next batch\n",
    "        rewards = list(map(lambda s: s.reward, batch)) # extract rewards from the batch\n",
    "        reward_bound = np.percentile(rewards, self.percentile) # extract top percentile reward\n",
    "        reward_mean = float(np.mean(rewards))\n",
    "\n",
    "        train_obs = []\n",
    "        train_act = []\n",
    "        for reward, steps in batch:\n",
    "            if reward < reward_bound:\n",
    "                continue\n",
    "            train_obs.extend(map(lambda step: step.observation, steps))\n",
    "            train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "        train_obs_v = torch.FloatTensor(train_obs)\n",
    "        train_act_v = torch.LongTensor(train_act)\n",
    "        return train_obs_v, train_act_v, reward_bound, reward_mean \n",
    "    \n",
    "    def get_batch(self):\n",
    "        return self.filter_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3fc8553-024f-4b4c-b4bc-fd1a611b94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, env=\"CartPole-v0\"):\n",
    "        self.env = gym.make(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self.env.observation_space\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b37a782a-362d-474c-99f5-614f6944e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self,  net, agent, env, reward_limit = 199):\n",
    "        self.net = net\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self. reward_limit = reward_limit\n",
    "        \n",
    "    def play(self):\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "               \n",
    "        X, y, reward_b, reward_m = self.agent.get_batch()\n",
    "        while reward_m <= self.reward_limit:\n",
    "            self.net.fit(X, y)\n",
    "            X, y, reward_b, reward_m = self.agent.get_batch()\n",
    "        \n",
    "        print('¡Resuelto!')\n",
    "        print(\"reward_mean=%.1f, rw_bound=%.1f\" % (reward_m, reward_b))    \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01088113-f096-41bb-8a95-68bf6a01e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "#\n",
    "# imports\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# env\n",
    "env = Env(env=\"CartPole-v0\")\n",
    "OBS_SIZE = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "# brain\n",
    "brain = Net(obs_size=OBS_SIZE , hidden_size=HIDDEN_SIZE, n_actions=N_ACTIONS)\n",
    "optimizer = optim.Adam(params=brain.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "brain.compil(optimizer, loss)\n",
    "\n",
    "\n",
    "\n",
    "# agent\n",
    "agent = Agent(batch_size=BATCH_SIZE, percentile=PERCENTILE)\n",
    "agent.set_net(brain)\n",
    "agent.set_env(env)\n",
    "\n",
    "u = CrossEntropy(net=brain, agent=agent, env=env, reward_limit=199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9321961f-2bf5-4aa4-98e2-37c21b5df8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 1: loss=0.680 \n",
      "iteration = 2: loss=0.668 \n",
      "iteration = 3: loss=0.666 \n",
      "iteration = 4: loss=0.658 \n",
      "iteration = 5: loss=0.647 \n",
      "iteration = 6: loss=0.642 \n",
      "iteration = 7: loss=0.626 \n",
      "iteration = 8: loss=0.647 \n",
      "iteration = 9: loss=0.625 \n",
      "iteration = 10: loss=0.624 \n",
      "iteration = 11: loss=0.629 \n",
      "iteration = 12: loss=0.597 \n",
      "iteration = 13: loss=0.598 \n",
      "iteration = 14: loss=0.593 \n",
      "iteration = 15: loss=0.587 \n",
      "iteration = 16: loss=0.591 \n",
      "iteration = 17: loss=0.571 \n",
      "iteration = 18: loss=0.586 \n",
      "iteration = 19: loss=0.559 \n",
      "iteration = 20: loss=0.564 \n",
      "iteration = 21: loss=0.557 \n",
      "iteration = 22: loss=0.549 \n",
      "iteration = 23: loss=0.552 \n",
      "iteration = 24: loss=0.542 \n",
      "iteration = 25: loss=0.547 \n",
      "iteration = 26: loss=0.550 \n",
      "iteration = 27: loss=0.552 \n",
      "iteration = 28: loss=0.544 \n",
      "iteration = 29: loss=0.540 \n",
      "iteration = 30: loss=0.538 \n",
      "iteration = 31: loss=0.540 \n",
      "iteration = 32: loss=0.533 \n",
      "iteration = 33: loss=0.540 \n",
      "¡Resuelto!\n",
      "reward_mean=200.0, rw_bound=200.0\n"
     ]
    }
   ],
   "source": [
    "u.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb70f90-c2aa-460b-84d3-725a7c56336a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
