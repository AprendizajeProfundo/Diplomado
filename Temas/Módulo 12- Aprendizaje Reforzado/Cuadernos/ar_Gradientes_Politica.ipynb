{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Gradientes de la Política</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Policy gradients</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Gradientes de política](#Gradientes-de-política)\n",
    "* [Algoritmo Refuerzo (Reinforce](#Algoritmo-Refuerzo-(Reinforce)\n",
    "* [Ejemplo CartPole](#Ejemplo-CartPole )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las lecciones previas hemos hablado de la política de un agente, pero no hemos desarrollado ningún procedimiento que directamente la incluya como parte central de los algortimos.\n",
    "\n",
    "Por otro lado, hasta el momento hemos trabajdo con modelos que tienen pocas acciones posibles, por lo que el método de entropia cruzada y el método Q-learning y el método DQN, los cuales están basados en redes neuronales de clasifición aplican bien al tipo de problemas abordadado. Pero, ¿Qué sucede si el número de posibles acciones es muy grande o incluso infinito o no contable?\n",
    "\n",
    "Piense por un momento en el problema de conducción automática. Un acción posible es girar el timon para cambiar ligéramiente la dirección para por ejemplo evitar un obstáculo. El ángulo de giro que corresponde a la respectiva acción es un número, posiblemente un número real.\n",
    "\n",
    "\n",
    "De otro lado, nos hemos enfocado en los valores de los estados o de las acciones dados los estados (Q-valores). Estos son números que aunque están vinculados con la política, no son probabilidades. \n",
    "\n",
    "En la pŕactica, cambios ligeros en el proceso de optimización de las funciones de pérdida pueden ocasioanr cambios importantes en estos valores. Por otro lado, si la salida de la red neuronal es un vector de probabilidades, tales cmabios por lo general resultan más suaves. Por todas razones, en esta lección pondremos nuestro objetivo en la política del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La hipótesis de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje reforzado s ebasa en que toda las metas y propósitos de una agente pueden ser pensados en términos de la maximizaciń del valor esperado de la suma acumulativa de una señal escalar recibida que llamamos recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesos de decisión de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente trabaja siguiendo un porceso de decisión de Markov (PDM). el cual consiste de una decisión (¿Cuál acción tomar?) que debe tomar en cada estado. Esto da origen a una sucesión de estados, acciones y recompensas llamada  `trayectoria` y que se puede visualizar como\n",
    "\n",
    "$$\n",
    "s_0, a_0, r_1, s_1, a_1,r_2,\\cdots\n",
    "$$\n",
    "\n",
    "y el objetivo es maximizar este conjunto de recompensas.\n",
    "\n",
    "Un `Proceso de decisión de Markov (PDM)` es una tupla $(S,A,R,p, \\gamma)$ tal que\n",
    "\n",
    "$$\n",
    "p(s',r| s,a) = Pr[s_{t+1}= s', R_{t+1}=r|S_t=s, A_t=a]\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\cdots\n",
    "$$\n",
    "\n",
    "en donde $S_t \\in S$ (espacio de estados), $A_t \\in A$(espacio de acciones), $R_t \\in R$ (spacio de recompensas). $p$ se denomina dinámica del ambiente. \n",
    "\n",
    "En palabras simples un PDM define la probabilidad de transición a un nuevo estado $s'$ recibir una recompensa $r$ partiendo del estdado actual $s$ y ejecutando una acción $a$.\n",
    "\n",
    "Técnicamente es un proceso de Markov de primer orden. Un elemento importante del modelo es el factor de descuento $\\gamma$, el cual es un valor entre 0 y 1. Sumando las recompensas futuras a lo largo del tiempo, descontadas con una potencia del factor de descuento se obtiene el concepto de retorno $G_t$. \n",
    "\n",
    "La idea central es que el agente encuentre trayectorias que maximizen el valor esperado del retorno.\n",
    "\n",
    "La dinámica del ambiente $p$ está fuera del alcance del agente. Recuerde el ejemlo de Frozen Lake. Un ejmeplo de la vida real puede ser el siguiente. Suponga que se encuentra en un lugar con demasiado viento y conduce un vehículo muy liviano. O puede imaginarse en un velero. Se puede intentar ir en una determinada dirección (la acción). Pero el viento extremo lo puede conducir en otra dirección. Si embargo puede ser que sea posible elegir una dirección diferente que permita ir en la dirección correcta. Esta es la política, que el agente si controla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un agente sigue una política $\\pi$, genera una sucesión de estados, acciones y recompensas que denominaremos trayectoria. Técnicamente la  política se define la probabilidad de acciones dado un estado:\n",
    "\n",
    "$$\n",
    "\\pi(A_t=a|S_t=s), \\hspace{3mm} \\forall A_t\\in A(s), S_t\\in S.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Gradientes de política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El `objetivo del aprendizaje reforzado` (AR) es maximizar la recompensa $r$ cuando sigue un política parametrizada $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta)= \\mathbb{E}_{\\pi}[r(\\tau)],\n",
    "$$\n",
    "\n",
    "en donde $r(\\tau)$ representa la recompensa total para una trayectoria $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Puede demostrarse que bajos ciertos supuestos que generalemnte se tienen en AR todo PDM finito tiene al menos una política optimal en el sentido de la recompensa obtneida y que entre todas las políticas optimales al menos una es estacionaria y deterministica.\n",
    "\n",
    "El procedimiento clásico para estimar el parametro $\\theta$ es el método del gradiente descendiente, el cual ( en términos muy simples) se basa en la regla de actualización\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\theta_t).\n",
    "$$\n",
    "\n",
    "$\\nabla J(\\theta_t)$ es el gradiente de la politica y $\\alpha$ una rata de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El reto que tenemos es encontrar el gradiente de la política. El primer  problema es que $J(\\theta)$ está definido como una esperanza (¡una integral!). \n",
    "\n",
    "Bajo algunas condiciones relacionadas con las derivadas de $\\pi$  es posible pasar el gradiente a través de la integral como sigue\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\mathbb{E}_{\\pi}[r(\\tau)] &= \\nabla \\int \\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int\\nabla \\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int \\pi(\\tau)\\nabla \\log\\pi(\\tau)r(\\tau)d\\tau\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que se tiene el siguiente resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Teorema del gradiente de la política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)\\nabla \\log \\pi_{\\theta}(\\tau)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se expande la definición de $\\pi_{\\theta}(\\tau)$ se obtiene\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta}(\\tau) = P(s_0)\\prod_{t=1}^{T}\\pi_{\\theta}(a_t|s_t)p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "$P$ respenta la distribución del estado inicial $s_0$ y se ha aplicado la regla del producto de la probabilidad y el hecho de ser un proceso de Markov que implica que cada nueva acción es independiente de la anterior. $T$ representa la longitud de la trayectoria. \n",
    "\n",
    "Tomando logaritmo se obtiene que\n",
    "\n",
    "$$\n",
    "\\log \\pi_{\\theta}(\\tau) = \\log P(s_0) + \\sum_{t=1}^{T}\\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\log p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "Por lo tanto se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[ r(\\tau) \\left(\\sum_{t=1}^{T}\\nabla \\log \\pi_{\\theta}(a_t|s_t)\\right) \\right]\n",
    "$$\n",
    "\n",
    "Este resultado dice que no es necesario conocer la distribución del estaod inicial $P$ ni la dinámica del ambiente $p$ para calcular el gradiente de la política.\n",
    "\n",
    "Los algortimos que usan este resultado son conocidos como `algoritmos libres del modelo`, debido a que no modelamos el ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la última ecuación se tiene que el cálculo del gradiente involucra una esperanza, es decir una integral, la cual por lo general es intratable. Aquí es ne donde entran en acción las técnicas Monte Carlo Markov Chain (MCMC). La idea central es que en cada paso de tiempo (cada iteración) se hace lo siguiente:\n",
    "\n",
    "1. Se obtiene una muestra aleatoria grande de acciones siguiendo la distribución (política) $\\pi(\\theta^*)$, en donde $\\theta^*$ es la estimación actual del parámetro $\\theta$.\n",
    "1. Se calcula el promedio de la expresion del gradiente para la muestra obtenida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensa de la trayectoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El termino $r(\\tau)$ de la recompensa total de la trayectoria $\\tau$ permanence inumutable en la expresión de $\\nabla J(\\theta)$. Este gradiente parametrizado no depende de  $r(\\tau)$. Sin embargo el termino agrega bastante varianza en el muestreo MCMC. En realidad hay $T$ fuentes de variación debidas a  cada $R_t$. En su lugar podemos hacer uso del retorno $G_t$ debido a que desde el punto de vista de la  optimización del objetivo de AR, la recompensa pasada no contribuye para nada.\n",
    "\n",
    "Cuando se reemplaza $r(\\tau)$ por el retorno descontado $G_t$ arribamos al algoritmo clásico del gradiente de la politica llamado reforzamiento (`reinforce`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Método Refuerzo (Reinforce) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo reinforce se basa en aproximar el gradiente de la política como \n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}G_t \\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, en la formula anterior no se ha resuelto el problema de la varianza en las trayectorias muestreadas. Desde el punto de vista de la inferencia bayesiana, se sabe que cuando el tamaño de muestra es grande, no importa la apriori seleccionada. Esto implica que cons muestras grandes de trayectorias la distribución del estado inicial no es importante, el algortimo MCMC converge al modelo de los parámetros verdaderso. El problema es que varianzas muy grandes el problema de estabilizar los parámetros del modelo es bastatente difícil. Resolveremos este problema en la lección Actor-crítico.\n",
    "\n",
    "De momento examinemos la implementación usual del método reinforce.\n",
    "\n",
    "Como hemos hecho antes a lo largo del curso, vamos a usar la aproximación del retorno con la función de Q-valores $Q(s,a)$. Así, para cada paso de la trayectoria tenemos que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\approx \\mathbb{E}_{\\pi_{\\theta}}\\left[  Q(s,a) \\nabla\\log \\pi_{\\theta}(a|s) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente de la política indica la dirección en la cual se deben cmabiar los parámetros de la red neuronal para mejorar la política en términos de la recompensa total acumulada. En esta aproximación el gradiente se escala proporcionalmente al valor de la acción tomada $Q(s,a)$ y el gradiente en sí mismo es igual al gradiente del logaritmo de la probabilidad de la acción tomada. \n",
    "\n",
    "Esto significa que estamos tratando de aumentar la probabilidad de acciones que nos han dado buena recompensa total y disminuyen la probabilidad de acciones con\n",
    "malos resultados finales. \n",
    "\n",
    "La esperanza es aproximada mediante el promedio del gradiente en varios pasos, de acuerdo con las técnicas MCMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida y máxima log-verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida que usualmente se utiliza en los métodos de gradiente de la política es\n",
    "\n",
    "$$\n",
    "\\mathfrak{L} = -Q(s,a) \\log\\pi(a|s)\n",
    "$$\n",
    "\n",
    "Es función de pérdida es menos la log-verosimilitud del modelo estadístico\n",
    "\n",
    "$$\n",
    "L = \\pi(a|s)^{Q(s,a)}.\n",
    "$$\n",
    "\n",
    "Por ejemplo para una trayectoria $\\tau$ con $T$ pasos se tiene que\n",
    "\n",
    "$$\n",
    "\\log L(\\tau) = \\sum_{t=1}^{T} Q(s_t,a_t)\\log\\pi(a_t|s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El algortimo Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialice la red neuronal con pesos aleatorios.\n",
    "1. Corra N episodios completos (una trayectoria), almacenando  las transiciones $(s,a,r,s')$.\n",
    "1. Para cada paso, $t$, de cada uno de los episodios,  $k$, calcule la recompensa total descontada para la sucesión de pasos: $Q_{t,k}= \\sum_{i=0} \\gamma^i r_i$.\n",
    "1. Calcule la función de pérdida para todas las transiciones: $\\mathfrak{L} = - \\sum_{tk} Q_{t,k} \\log(\\pi(s_{t,k},a_{t,k}))$\n",
    "1. Ejecute el paso de actualización de pesos del algoritmo SGD selecionado.\n",
    "1. Repita desde el paso 2 hasta convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias con Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El métod reinforce se diferencia del método Q-learning en varias cosas.\n",
    "\n",
    "1. No se requiere una exploración explícita de tipo epsilon-greedy, que se usa en Q-learning para evitar que el algoritmo quede estancado en un mínimo local. Ahora la red neuronal retorna probabilidades directamente. Al comienzo la red es inicializada con valores aleatorios, por lo que la primera salida corresponde a una distribución uniforme.\n",
    "1. No se usa la memoria de repetición (replay buffer). Los métodos gradiente de política son métodos basados en la política (Q-learning es libre de política). Es decir, los datos obtenidos para el entrenamiento so se basan en políticas viejas. Esto es bueno y malo. Lo  bueno es que estos métodos convergen más rápido por lo general. Lo malo es que  requieren por lo general mucha más interacción con el ambiente que por ejemplo DQN.\n",
    "1. No se requiere una red neuronal objetivo (target). Aquí se usan los Q-valores, pero ya no son aproximados como DQN, sino que se calculan completamente en cada trayectoria. En DQN se require la red target para romper la correlación entre valores $Q(s,a)$, porque en cada paso ellos son apxroximados. Aquí los Q-valores se calculan sin aproximación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Ejemplo CartPole </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vemos el método en acción. usaresmo el problema CarPole para ilustrar el método con código real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa  módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase PGN (Policy Gradient Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la red neuronal que usaremos. Ya familiar para todos.\n",
    "Observe que la red no regresa probanbilidades como hemos previsto. Esto se hace porque en la implemntación decidimos usar en la perdida la función *log_softmax* que calula de manera eficiente el logaritmo del softmax que ne realidad es lo que se requiere para la función de pérdida. Además este cálculo es bastante má estable que calcular inicialmente softmax y luego el logaritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de los Q-valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función se hace el cálculo de los Q-valores de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenaminto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
