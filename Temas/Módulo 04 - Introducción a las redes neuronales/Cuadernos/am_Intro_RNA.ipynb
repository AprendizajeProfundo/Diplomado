{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:#4361EE\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"><center>Diplomado en Inteligencia Artificial y Aprendizaje Profundo</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"><center>Introducción a Redes Neuronales</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/mind-544404__340.webp\" width=\"600\" height=\"400\" align=\"center\" /> \n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "\n",
    "Fuente: [imágenes libres en pixabay](https://pixabay.com/es/images/search/neurons/)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Camilo José Torres Jiménez, Msc, cjtorresj@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dextronomo@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Maria del Pilar Montenegro, pmontenegro88@gmail.com\n",
    "5. Jessica López Mejía, jelopezme@unal.edu.co\n",
    "6. Venus Puertas, vpuertasg@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Jefe Jurídica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Paula Andrea Guzmán, guzmancruz.paula@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Coordinador Jurídico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. David Fuentes, fuentesd065@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Desarrolladores Principales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Dairo Moreno, damoralesj@unal.edu.co\n",
    "10. Joan Castro, jocastroc@unal.edu.co\n",
    "11. Bryan Riveros, briveros@unal.edu.co\n",
    "12. Rosmer Vargas, rovargasc@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Expertos en Bases de Datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Giovvani Barrera, udgiovanni@gmail.com\n",
    "14. Camilo Chitivo, cchitivo@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [¿Qué es una Red Neuronal Artificial?](#¿Qué-es-una-Red-Neuronal-Artificial?)\n",
    "    * [Partes de una Neurona y sus Funciones](#Partes-de-una-Neurona-y-sus-Funciones)\n",
    "    * [Comparación entre Redes Neuronales Biológicas y Artificiales](#Comparación-entre-Redes-Neuronales-Biológicas-y-Artificiales)\n",
    "    * [¿Cómo funciona una Red Neuronal Artificial?](#¿Cómo-funciona-una-Red-Neuronal-Artificial?)\n",
    "    * [Tipos de Arquitecturas de Redes Neuronales](#Tipos-de-Arquitecturas-de-Redes-Neuronales)\n",
    "    * [Perceptrón multicapa: Aprendizaje Profundo de RNA](#Perceptrón-multicapa:-Aprendizaje-Profundo-de-RNA)\n",
    "* [Enfoque Matemático de una RNA](#Enfoque-Matemático-de-una-RNA)\n",
    "    * [Modelamiento matemático de una RNA con una capa oculta](#Modelamiento-matemático-de-una-RNA-con-una-capa-oculta)\n",
    "* [¿Por qué necesitamos funciones de activación no lineales?](#¿Por-qué-necesitamos-funciones-de-activación-no-lineales?)\n",
    "* [Una RNA es una función vectorial](#Una-RNA-es-una-función-vectorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">¿Qué es una Red Neuronal Artificial?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuronales Artificiales son modelos computacionales inspirados en el cerebro humano. Muchos de los avances recientes en ciencia y tecnología se han hecho en el campo de la Inteligencia Artificial, que van desde reconocimiento de voz hablada, reconocimiento de imágenes, y robótica, entre otros.\n",
    "\n",
    "Como se dijo anteriormente, las Redes Neuronales Artificiales son simulaciones inspiradas en el ámbito biológico hechas en un ordenador para realizar tareas como\n",
    "\n",
    "1. Clustering\n",
    "2. Clasificación\n",
    "3. Reconocimiento de Patrones\n",
    "\n",
    "Aunque en general, se usan para resolver objetivos específicos guiados por su creador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Somos Electricidad</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/brain-5885161_960_720.webp\" width=\"500\" height=\"400\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: [pixabay: imágenes libres](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Partes de una Neurona y sus Funciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/Structure-Of-Neurons-In-Brain.jpeg\" width=\"600\" height=\"300\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro, basado en una imagen de [pixabay](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Las células nerviosas típicas del cerebro humano se componen de cuatro partes:\n",
    "\n",
    "1. **Función de la Dendrita**. Recibe las señales de otras neuronas.\n",
    "2. **Soma (cuerpo celular)**. Suma todas las señales entrantes para generar una señal total de entrada(input).\n",
    "3. **Estructura del Axón**. Cuando la suma sobrepasa un cierto umbral numérico, la neurona se activa, dispara y la señal viaja a través del axón hacia otras neuronas.\n",
    "4. **Trabajo de la Sinapsis**. Es el punto donde se realiza la interconexión de una neurona con otras neuronas. La cantidad de la señal transmitida depende en la fuerza (peso sináptico) de las conexiones. Las conexiones pueden ser inhibidoras (disminuyendo la fuerza) o de excitación (aumentando la fuerza) en principio.\n",
    "\n",
    "Así pues, una **Red Neuronal** es, en general una **red altamente interconectada** de billones de neuronas con trillones de interconexiones entre ellas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/neurons-1773922__340.webp\" width=\"600\" height=\"300\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: [Pixabay: imágenes libres](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Comparación entre Redes Neuronales Biológicas y Artificiales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/electronics-3007664_960_720.jpg\" width=\"600\" height=\"300\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: [Pixabay: imágenes libres](https://pixabay.com/es/photos/electr%c3%b3nica-circuito-integrado-3007664/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/neurona_artificial.png\" width=\"600\" height=\"300\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/neural_network.png\" width=\"600\" height=\"300\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: [ClipArtMax](https://www.clipartmax.com/download/m2i8d3G6b1b1K9i8_%5B2%5D-image-on-quoracdn-machine-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Las **dendritas** en las Redes Neuronales Biológicas son un análogo a las entradas conteniendo un peso específico basada en la interconexión \"sináptica\" presente en la Red Neuronal Artificial.\n",
    "\n",
    "El **cuerpo celular** es comparable a la unidad artificial llamada \"neurona\" en una Red Neuronal Artificial, que también comprende la suma de señales y umbral de activación.\n",
    "\n",
    "La salida de los **Axones** (presentes en la sinapsis) son el análogo de los datos de salida en la Red Neuronal Artificial.\n",
    "\n",
    "Por lo tanto, **RNA** son modeladas usando el trabajo básico de las neuronas biológicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">¿Cómo funciona una Red Neuronal Artificial?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/ANN_Capa_Oculta.png\" width=\"800\" height=\"600\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "La red neuronal artificial recibe información del mundo externo en forma de patrón en forma de vector, digamos $x=(x_1,\\ldots,x_n)^t$. En este caso, la capa de entrada posee $n$ neuronas artificiales.\n",
    "\n",
    "\n",
    "Cada componente $x_i$ de la entrada se multiplica por el peso correspondiente $w_{i}$. Los pesos son la información utilizada por la red neuronal para resolver un problema específico. Estos pesos deben aprenderse (ajustarse, estimarse) en el **paso de entrenamiento**. Los **pesos representan el conocimiento** que tienen los ANN sobre el problema en cuestión. \n",
    "\n",
    "Usando la metáfora biológica, los pesos representan la fuerza de la interconexión entre las neuronas dentro de la red neuronal.\n",
    "\n",
    "Las entradas y los pesos se combinan y se resumen dentro de la unidad de computación (neurona artificial), y se agrega un **\"sesgo\"** (intercepto), como muestra la figura arriba.\n",
    "\n",
    "La suma es un número real: $z = \\sum_i x_iw_i + b$, $z \\in\\mathcal{R}$. Esta suma se transforma a través de una función de activación, digamos $g(\\cdot)$, para obtener la salida neta $x^* = g(z)$. La función de activación determina el comportamiento de la neurona. Para más detalles, consulte la siguiente sección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Tipos de Arquitecturas de redes neuronales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Popular](../Imagenes/Popular-Neural-Network-Architecture.jpg)\n",
    "\n",
    "Fuente: [Artificial Neural Networks Applications and Algorithms](https://www.xenonstack.com/blog/artificial-neural-network-applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Arquitecturas de redes neuronales modernas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Redes_Neuronales/Imagenes/RN_modernas.png\" width=\"800\" height=\"500\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. **Modelo Perceptrón de una sola capa**. Red Neuronal que tiene dos unidades de entrada (es decir, puntos en $\\mathbb{R}²$) y una unidad de salida sin capas ocultas. Fue usado para clasificación simple de **dos clases** en conjuntos de datos del plano.\n",
    "\n",
    "2. **RNA de base radial**. Estas redes son similares a la red neuronal de avance hacia adelante, excepto que una **función de base radial** se utiliza como la función de activación de estas neuronas. La salida de la red es una combinación lineal de funciones de base radial de las entradas y los parámetros neuronales. Las redes de base radial tienen muchos usos, incluida la aproximación de funciones, la predicción de series temporales, la clasificación y el control de sistemas.\n",
    "\n",
    "3. **Red neuronal de perceptrón multicapa**. Estas redes utilizan **más de una capa oculta** de neuronas, a diferencia del perceptrón de una sola capa. También se conocen como Redes neuronales de alimentación profunda.\n",
    "\n",
    "4. **Red neuronal recurrente**. Tipo de red neuronal en la que las neuronas de cada capa oculta tienen **auto-conexiones**. Las redes neuronales recurrentes **poseen memoria**. En cualquier caso, la neurona de capa oculta recibe la activación de la siguiente capa, así como su valor de activación anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "5. **Red neuronal de memoria a corto y largo plazo (LSTM)**. El tipo de red neuronal recurrente en la que la célula de memoria se incorpora a las neuronas de capa oculta. Esto genera un ciclo de retroalimentación, y por eso, a veces, este tipo de redes se llaman de retroalimentación.\n",
    "\n",
    "6. **Red de Hopfield**. Una red de neuronas **totalmente interconectadas** en la que cada neurona está conectada a cada otra neurona. La red se entrena con el patrón de entrada al establecer un valor de neuronas en el patrón deseado. Luego se calculan sus pesos. Los pesos no cambian. Una vez entrenada para uno o más patrones, la red convergerá a los patrones aprendidos. Es diferente de otras redes neuronales. Las unidades en las redes de Hopfield son unidades de umbral binarias, es decir, las unidades solo toman dos valores diferentes para sus estados y el valor se determina por si la entrada de las unidades excede o no su umbral. Las redes de Hopfield normalmente tienen unidades que toman valores de 1 o -1. Este modelo utiliza el concepto de **memoria asociativa**, y se ha utilizado para explicar el trabajo de la memoria humana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "7. **Máquinas de Boltzmann**. Estas redes son similares a la red Hopfield, excepto que algunas neuronas son de entrada, mientras que otras están ocultas, en naturaleza. Los pesos se inicializan al azar y se aprenden a través del algoritmo de **retropropagación**. Son teóricamente intrigantes debido a sla naturaleza local y Hebbiana de su algoritmo de entrenamiento (siendo entrenado por la regla de Hebb), y debido a su paralelismo y la semejanza de su dinámica con procesos físicos simples. Las máquinas Boltzmann con **conectividad sin restricciones** no han demostrado ser útiles para problemas prácticos en el aprendizaje automático o la inferencia, pero si la conectividad está restringida adecuadamente, el aprendizaje puede ser lo suficientemente eficiente como para ser útil para problemas prácticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "8. **Redes Neuronales Convolucionales**. Las RNCs son **versiones regularizadas** de perceptrones multicapa. Los perceptrones multicapa generalmente son redes completamente conectadas, es decir, cada neurona en una capa está conectada a todas las neuronas en la capa siguiente. La \"conexión total\" de estas redes las hace propensas a **sobreajustar datos**. Las redes convolucionales se inspiraron en procesos biológicos en donde, el patrón de conectividad entre las neuronas se asemeja a la organización de la corteza visual animal. Las neuronas corticales individuales responden a los estímulos solo en una **región restringida del campo visual conocida como campo receptivo**. Los campos receptivos de diferentes neuronas se superponen parcialmente de modo que cubren todo el campo visual.\n",
    "\n",
    "9. **Transformers**. Los transformers son redes neuronales artificiales inspiradas en procesos de codificación y decodificación de información. Junto con los conceptos de **autoatención**, se han convertido en pilares fundamentales del estado del arte. Ejemplos de su aplicación van desde modelos como **BERT**, **GPT-3** y **MUM (Multitask-Unified-Model)**, entre muchos otros, logrando una gran revolución en el campo de la inteligencia artificial.\n",
    "\n",
    "10. **Red autocodificadora** o auto-encoder. Es escencialmente un perceptron multicapa de múltiples usos como parte de otras arquitecturas. Por sí mismas son útiles para hacer reducción de dimensión de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "11. **Red neuronal modular**. Es la estructura combinada de diferentes tipos de redes neuronales como el perceptrón multicapa, Hopfield, la red neuronal recurrente, etc., que se incorporan como un solo módulo en la red para realizar una subtarea independiente de redes neuronales completas completas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Perceptrón multicapa: Aprendizaje Profundo de RNA</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "El perceptrón multicapa es actualmente **la arquitectura más utilizada**. Las redes de aprendizaje profundo (RAP) son preceptrónes multicapa con una o más capas ocultas. La profundidad está determinada por el número de capas ocultas.\n",
    "\n",
    "Las redes neuronales pueden verse como gráficos dirigidos ponderados en los que las neuronas artificiales son nodos, y los bordes dirigidos con pesos son conexiones entre las salidas de neuronas y las entradas de neuronas. La figura  muestra un perceptrón multicapa con dos capas ocultas. Este tipo de RNA también se conoce como Red neuronal de alimentación profunda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/Artificial-Neural-Network-Architecture.jpg\" style=\"width:500px;height:400px;\"/> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Artificial Neural Networks Applications and Algorithms](https://www.xenonstack.com/blog/artificial-neural-network-applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Enfoque Matemático de una RNA</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Supuestos básicos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "La primera capa oculta de un RNA podría ser una reducción de dimensión. Sin embargo, en general, es más práctico reducir los datos previamente. Entonces, a partir de este punto, suponemos que los datos de entrenamiento son datos reducidos (si es que tal reducción es necesaria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "En esta sección consideramos sólo una capa oculta.\n",
    "**Asumiremos** que:\n",
    "\n",
    "1. La capa de entrada tiene $n$ neuronas. Entonces los valores de entrada son $n$-vectores.\n",
    "\n",
    "2. La capa oculta tiene $q$ neuronas. Esto implica que existen conexiones $q$ desde cada neurona de entrada a la capa oculta. En total hay conexiones $n \\times q$ entre la capa de entrada y la capa oculta. Cada conexión tiene un peso $w^{1}_{ij}$, que representa la fuerza de la conexión entre la neurona $i$ en la capa de entrada y la neurona $j$ en la capa oculta.\n",
    "\n",
    "3. La capa de salida tiene neuronas $L$. Esto implica que existen conexiones $L$ de cada neurona oculta a la capa de salida. En total hay conexiones $q \\times L$ entre la capa oculta y la capa de salida. Cada conexión tiene un peso $w^{2}_{jk}$, que representa la fuerza de la conexión entre la neurona $j$ en la capa oculta y la neurona $k$ en la capa de salida.\n",
    "\n",
    "$\\leadsto$ **Notación Vectorial**. *Por facilidad, denotaremos los vectores en un formato de fila, como es habitual en matemáticas. En estadística es común la notación de columna*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Modelo matemático de una RNA con una capa oculta</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Datos de entrenamiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $X_{N\\times n}$ la matriz de los datos de entrenamiento de entrada $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Transformación afín de los datos.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $W^{1}$ una matriz $n\\times q$  cuyas filas son los vectores de peso $w^{1}_{ij}$, \n",
    "que conceptualmente conectan la capa de entrada con la capa oculta. Sea $b^{1}$ el $q$-vector de los respectivos bias. Supongamos que $b=(b^1_1,\\ldots, b^1_j,\\ldots,b^1_q)$. Entonces , $b^1_j$ es el bias en la neurona $j$ de la capa oculta. \n",
    "\n",
    "\n",
    "Dado un vector de entrada $x$, que es fila de matriz $X$, la entrada completa de la capa oculta se obtiene mediante\n",
    "\n",
    "\n",
    "$$\n",
    "z^{1} = xW^{1} + b^{1} \\quad (1)\n",
    "$$\n",
    "\n",
    "$\\leadsto$ Note que hemos asumido que hay $n$ neuronas en la capa de entrada. Si $q<n$, $W^{1}$ realiza una proyección sobre un subespacio de dimensión reducida. Si $q>n$, $W^{1}$ realiza un \"sumergimiento\" (embedding) sobre un espacio de dimensión mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "La ecuación (1) es una **transformación afín**, que puede ser expresada en coordenadas homogéneas de la siguiente manera:\n",
    "\n",
    "$\\tilde{x} = (x,1)$, $\\tilde{z}^1 = (z^1,1)$. Let $\\tilde{W}$ definida como:\n",
    "\n",
    "$$\n",
    "\\tilde{W}^1 = \\begin{pmatrix} W^1 & 0\\\\ b^1 & 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "por lo que obtenemos,\n",
    "\n",
    "$$\n",
    "\\tilde{z}^1 = \\tilde{x}\\tilde{W}^1.\n",
    "$$\n",
    "\n",
    "Esta ecuación significa que una transformación afín se puede expresar como una transformación lineal en coordenadas homogéneas. Para obtener más información, consulte la próxima lección, sobre transformaciones afines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Activación de Neuronas en la capa oculta</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f^1$ la función de activación en la capa oculta. Entonces, $f^1$ se aplica a cada elemento de $z^1$. Sea $x^1 = (x_1^1,\\ldots,x_j^1,\\ldots, x_q^1)$. El efecto de la función de activación se escribe como\n",
    "\n",
    "$$\n",
    "x^1 = f^1(z^1),\n",
    "$$\n",
    "donde $x^1_j = f^1(z^1_j)$, para $j=1,\\ldots,q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Desde la capa oculta a la capa de salida</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Transformación afín de los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que $W^{2}$ es una matriz $q\\times L$,  cuyas filas son los vectores de pesos $w^{2}_{jk}$,  que conceptualmente conecta la capa oculta con la capa de salida. Sea $b^{2}$ el $L$-vector de los correspondientes biases (interceptos). \n",
    "\n",
    "$\\leadsto$ En la aplicación de una Red Neuronal Profunda a un problema de clasificación, $L$ corresponde al número de clases.\n",
    "\n",
    "\n",
    "Dado $x^1$ el vector de salida de la capa oculta, la entrada completa a la capa de salida se obtiene como\n",
    "\n",
    "\n",
    "$$\n",
    "z^{1} = x^1W^{2} + b^{2}. \\quad (2)\n",
    "$$\n",
    "\n",
    "En coordenadas homogéneas, tenemos que\n",
    "\n",
    "$$\n",
    "\\tilde{z}^2 = \\tilde{x}^1\\tilde{W}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Activación de Neuronas en la capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f^2$ la función de activación en la capa de salida. Entonces, $f^2$ se aplica a cada elemento de $z^2$. Sea $y = (y_1,\\ldots,y_k,\\ldots, y_L)$. El efecto de la función de activación se escribe como\n",
    "\n",
    "$$\n",
    "y = f^2(z^2),\n",
    "$$\n",
    "donde $y_k = f^2(z^2_k)$, for $k=1,\\ldots,L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">¿Por qué necesitamos funciones de activación no lineales?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Una red neuronal sin funciones de activación lineal es esencialmente un modelo de regresión lineal. La función de activación realiza una transformación no lineal de la entrada, lo que la hace capaz de aprender y realizar tareas más complejas.\n",
    "\n",
    "Para ver este hecho, suponga que $\\tilde{G}^1$ y $\\tilde{G}^2$ representan las matrices asociadas a las funciones de activación lineal en coordenadas homogéneas. Por lo tanto, tenemos que\n",
    "\n",
    "$$\n",
    "\\tilde{y} =\\tilde{x}\\tilde{W}^1\\tilde{G}^1\\tilde{W}^2\\tilde{G}^2 = x\\tilde{W}\n",
    "$$\n",
    "donde \n",
    "$$\n",
    "\\tilde{W}=\\tilde{W}^1\\tilde{G}^1\\tilde{W}^2\\tilde{G}^2\n",
    "$$ .\n",
    "\n",
    "$\\leadsto$ Así, en este caso la RNP se reduce a un modelo lineal simple, que no es muy útil en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Una RNA es una función vectorial</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una RNA con una capa oculta es una función $f:\\mathcal{R}^n \\to \\mathcal{R}^L$, definida como\n",
    "\n",
    "$$\n",
    "y = f(x)  = f^2(f^1( x W^1 + b^1 ) W^2 + b^2).\n",
    "$$\n",
    "\n",
    "$\\leadsto$ Como puede verse, si la RNA tiene más de una capa oculta, la función $f$ puede ser extendida directamente de forma recursiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Unesco: educación e inteligencia artificial](https://es.unesco.org/themes/tic-educacion/inteligencia-artificial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "3620b340ed2ed479d71e72c732c6d6a6eac39d8adbece51414abdea5ed84aec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
